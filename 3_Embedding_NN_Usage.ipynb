{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a695c0",
   "metadata": {},
   "source": [
    "# Embedding Neural Network Router Usage\n",
    "\n",
    "This notebook demonstrates how to use the Embedding Neural Network for intelligent multimodal model routing. The router uses CLIP embeddings and a trained neural network to select the optimal model based on:\n",
    "- Query content and complexity\n",
    "- Multi-turn conversation context\n",
    "- Cost-based optimization with confidence thresholds\n",
    "\n",
    "\n",
    "> **Note**: The GitHub repository includes a pre-trained neural network and the weights are stored in `llm-router/src/nat_sfc_router/training/router_artifacts`.  The notebook `2_Embedding_NN_Training.ipynb` re-trains the neural network and over-writes those weights. You can run the usage notebook without running the training notebook to use the existing neural network OR you can run the training notebook and then use this notebook with your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c53a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e374a",
   "metadata": {},
   "source": [
    "Clone the repository, or skip this line if you are using the Brev launchable which already has the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA-AI-Blueprints/llm-router.git\n",
    "!cd llm-router && git checkout experimental # TODO: Remove once ces-dev is merged to main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get current working directory\n",
    "cwd = Path.cwd()\n",
    "\n",
    "# Check if we're already in llm-router directory\n",
    "if cwd.name == 'llm-router':\n",
    "    print(f\"✓ Already in llm-router directory: {cwd}\")\n",
    "else:\n",
    "    # Check if ../llm-router exists (we're in a subdirectory of parent)\n",
    "    parent_llm_router = cwd.parent / 'llm-router'\n",
    "    if parent_llm_router.exists() and parent_llm_router.is_dir():\n",
    "        os.chdir(parent_llm_router)\n",
    "        print(f\"✓ Changed to llm-router directory: {parent_llm_router}\")\n",
    "    # Check if ./llm-router exists (we're in the parent directory)\n",
    "    elif (cwd / 'llm-router').exists() and (cwd / 'llm-router').is_dir():\n",
    "        os.chdir(cwd / 'llm-router')\n",
    "        print(f\"✓ Changed to llm-router directory: {cwd / 'llm-router'}\")\n",
    "    else:\n",
    "        print(f\"⚠ Warning: Could not find llm-router directory. Current directory: {cwd}\")\n",
    "        print(\"  Continuing with current directory...\")\n",
    "\n",
    "# Verify we can find expected files\n",
    "if not Path('pyproject.toml').exists():\n",
    "    print(\"⚠ Warning: pyproject.toml not found in current directory\")\n",
    "    print(f\"  Current directory: {Path.cwd()}\")\n",
    "else:\n",
    "    print(f\"✓ Verified project files in: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c372d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f23f30",
   "metadata": {},
   "source": [
    "First, we set up the Python path and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f99148-abad-4f1f-b8d3-c75733755fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe858d9",
   "metadata": {},
   "source": [
    "The following sections walk through the Neural Network Objective Function implementation and demonstrate example usage of the multimodal router."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac518d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Neural Network Objective Function\n",
    "\n",
    "This module provides a production-ready objective function that uses a pre-trained neural network router to intelligently route requests to the best model based on:\n",
    "\n",
    "1. **Embedding generation** from text and multimodal content\n",
    "2. **Neural network predictions** with configurable confidence thresholds\n",
    "3. **Multi-turn conversation context** understanding\n",
    "\n",
    "The router loads on service startup (before request handling) for optimal performance.\n",
    "\n",
    "The following cells walk through each component of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Optional, Tuple, TYPE_CHECKING\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "import time\n",
    "\n",
    "from pydantic import Field\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "# Import ModelRouter from training package\n",
    "from nat_sfc_router.training.model_router import _resolve_router_path\n",
    "from nat_sfc_router.training import ModelRouter\n",
    "\n",
    "# Import OpenAI schema for type hints\n",
    "from nat_sfc_router.schema.openai_chat_request import OpenAIChatRequest\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db154156",
   "metadata": {},
   "source": [
    "### 2.1 Model Routing Configuration\n",
    "\n",
    "Define the mapping between router output model names and actual pipeline model names. Also configure confidence thresholds for model selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map router output model names to actual pipeline model names\n",
    "# The router predicts one of these model names based on embeddings\n",
    "MODEL_ROUTER_TO_TARGET = {\n",
    "    # GPT models -> openai/gpt-oss-120b\n",
    "    'gpt-5-chat': 'gpt-5-chat',\n",
    "    'gpt-5': 'gpt-5-chat',\n",
    "    \n",
    "    # Nemotron VL models -> nvidia/nemotron-nano-12b-v2-vl\n",
    "    'nemotron-vl': 'nvidia/nemotron-nano-12b-v2-vl',\n",
    "    'nemotron-nano-12b-v2-vl': 'nvidia/nemotron-nano-12b-v2-vl',\n",
    "    'nvidia/nemotron-nano-12b-v2-vl': 'nvidia/nemotron-nano-12b-v2-vl',\n",
    "\n",
    "    # Some versions of the router were trained with Qwen, so map Qwen to nano vl for example purposes,\n",
    "    'Qwen/Qwen3-VL-8B-Instruct': 'nvidia/nemotron-nano-12b-v2-vl',\n",
    "    \n",
    "    # Nemotron models -> nvidia/nvidia-nemotron-nano-9b-v2\n",
    "    'nemotron-nano-12b-v2-vl': 'nvidia/nvidia-nemotron-nano-9b-v2',\n",
    "    'nemotron-nano': 'nvidia/nvidia-nemotron-nano-9b-v2',\n",
    "    'nvidia/nvidia-nemotron-nano-9b-v2': 'nvidia/nvidia-nemotron-nano-9b-v2',\n",
    "}\n",
    "\n",
    "# Custom confidence thresholds for model selection\n",
    "# These thresholds ensure the router only selects a model if confidence is above the threshold\n",
    "# If the top choice doesn't meet its threshold, the router falls back to the next best model\n",
    "CUSTOM_THRESHOLDS = {\n",
    "    'gpt-5-chat': 0.70,\n",
    "    'nvidia/nemotron-nano-12b-v2-vl': 0.55,\n",
    "    'nvidia/nvidia-nemotron-nano-9b-v2': 0.50,\n",
    "}\n",
    "\n",
    "# Default fallback model (used on errors or unknown routing)\n",
    "DEFAULT_FALLBACK_MODEL = 'nvidia/nvidia-nemotron-nano-9b-v2'\n",
    "\n",
    "print(f\"Configured {len(MODEL_ROUTER_TO_TARGET)} model mappings\")\n",
    "print(f\"Configured thresholds for {len(CUSTOM_THRESHOLDS)} models\")\n",
    "print(f\"Default fallback: {DEFAULT_FALLBACK_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ac33f",
   "metadata": {},
   "source": [
    "### 2.2 Router Loading\n",
    "\n",
    "The router is loaded once on service startup for optimal performance. This function initializes the ModelRouter with the trained neural network router model, custom confidence thresholds, and device selection (CUDA if available).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global router instance (loaded on startup)\n",
    "_router = None\n",
    "\n",
    "\n",
    "def _load_router():\n",
    "    \"\"\"\n",
    "    Load the ModelRouter on service startup (before _response_fn).\n",
    "    This ensures the router is initialized once when the service starts,\n",
    "    not on every request.\n",
    "    \"\"\"\n",
    "    global _router\n",
    "    if _router is None:\n",
    "        logger.info(\"Loading ModelRouter on service startup...\")\n",
    "        try:\n",
    "            _router = ModelRouter(\n",
    "                model_thresholds=CUSTOM_THRESHOLDS,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                verbose=True\n",
    "            )\n",
    "            logger.info(\"✓ ModelRouter loaded successfully on startup\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load ModelRouter: {e}\", exc_info=True)\n",
    "            raise\n",
    "    return _router\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b89ba6",
   "metadata": {},
   "source": [
    "### 2.3 Message Parsing Utilities\n",
    "\n",
    "This function extracts and formats text and images from multi-turn chat messages. It handles OpenAI API format messages with support for:\n",
    "- Multiple turns (user, assistant, system)\n",
    "- Text content (strings and lists)\n",
    "- Images (data URIs and base64 encoded)\n",
    "- Both dict and Pydantic object types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_images_from_messages(\n",
    "    messages: List[Dict[str, Any]]\n",
    ") -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract and format text and images from multi-turn chat messages.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts or objects with 'role' and 'content'\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (combined_text, images_list)\n",
    "        - combined_text: Full conversation context as a single string\n",
    "        - images_list: List of image URIs/base64 strings suitable for embedding model\n",
    "    \"\"\"\n",
    "    full_text_parts = []\n",
    "    images = []\n",
    "    \n",
    "    # Convert messages to list in case it's a ValidatorIterator or other iterable\n",
    "    messages_list = list(messages) if not isinstance(messages, list) else messages\n",
    "    \n",
    "    for msg_idx, message in enumerate(messages_list):\n",
    "        # Handle both dict and object types\n",
    "        if isinstance(message, dict):\n",
    "            role = message.get(\"role\", \"\").upper()\n",
    "            content = message.get(\"content\", \"\")\n",
    "        else:\n",
    "            # Try to access as object attributes (Pydantic models, etc.)\n",
    "            role = getattr(message, \"role\", \"\").upper()\n",
    "            content = getattr(message, \"content\", \"\")\n",
    "        \n",
    "        # Add role prefix for conversation context\n",
    "        if role:\n",
    "            full_text_parts.append(f\"[{role}]:\")\n",
    "        \n",
    "        # Handle string content\n",
    "        if isinstance(content, str):\n",
    "            if content.strip():\n",
    "                full_text_parts.append(content)\n",
    "        \n",
    "        # Handle list content (multimodal)\n",
    "        elif isinstance(content, list):\n",
    "            for part_idx, part in enumerate(content):\n",
    "                if isinstance(part, dict):\n",
    "                    # Text items (dict)\n",
    "                    if part.get(\"type\") == \"text\":\n",
    "                        text = part.get(\"text\", \"\").strip()\n",
    "                        if text:\n",
    "                            full_text_parts.append(text)\n",
    "                    \n",
    "                    # Image items (dict)\n",
    "                    elif part.get(\"type\") == \"image_url\":\n",
    "                        img_url_obj = part.get(\"image_url\", {})\n",
    "                        \n",
    "                        # Handle both dict and string formats\n",
    "                        if isinstance(img_url_obj, dict):\n",
    "                            url = img_url_obj.get(\"url\", \"\").strip()\n",
    "                        else:\n",
    "                            url = str(img_url_obj).strip()\n",
    "                        \n",
    "                        # Only include valid image URLs\n",
    "                        if url and isinstance(url, str) and len(url) > 0:\n",
    "                            images.append(url)\n",
    "                else:\n",
    "                    # Handle object types (from Pydantic models)\n",
    "                    part_type = getattr(part, \"type\", None)\n",
    "                    \n",
    "                    if part_type == \"text\":\n",
    "                        text_content = getattr(part, \"text\", \"\")\n",
    "                        if text_content:\n",
    "                            full_text_parts.append(text_content)\n",
    "                    \n",
    "                    elif part_type == \"image_url\":\n",
    "                        img_url_obj = getattr(part, \"image_url\", None)\n",
    "                        if img_url_obj:\n",
    "                            # Handle as dict or object\n",
    "                            if isinstance(img_url_obj, dict):\n",
    "                                url = img_url_obj.get(\"url\", \"\").strip()\n",
    "                            else:\n",
    "                                url = getattr(img_url_obj, \"url\", \"\")\n",
    "                                if url:\n",
    "                                    url = url.strip()\n",
    "                            \n",
    "                            # Only include valid image URLs\n",
    "                            if url and isinstance(url, str) and len(url) > 0:\n",
    "                                images.append(url)\n",
    "        else:\n",
    "            # Try to iterate over content in case it's a ValidatorIterator\n",
    "            try:\n",
    "                content_list = list(content)\n",
    "                for part_idx, part in enumerate(content_list):\n",
    "                    if isinstance(part, dict):\n",
    "                        # Text items (dict)\n",
    "                        if part.get(\"type\") == \"text\":\n",
    "                            text = part.get(\"text\", \"\").strip()\n",
    "                            if text:\n",
    "                                full_text_parts.append(text)\n",
    "                        # Image items (dict)\n",
    "                        elif part.get(\"type\") == \"image_url\":\n",
    "                            img_url_obj = part.get(\"image_url\", {})\n",
    "                            if isinstance(img_url_obj, dict):\n",
    "                                url = img_url_obj.get(\"url\", \"\").strip()\n",
    "                            else:\n",
    "                                url = str(img_url_obj).strip()\n",
    "                            if url and isinstance(url, str) and len(url) > 0:\n",
    "                                images.append(url)\n",
    "                    else:\n",
    "                        # Handle object types\n",
    "                        part_type = getattr(part, \"type\", None)\n",
    "                        if part_type == \"text\":\n",
    "                            text_content = getattr(part, \"text\", \"\")\n",
    "                            if text_content:\n",
    "                                full_text_parts.append(text_content)\n",
    "                        elif part_type == \"image_url\":\n",
    "                            img_url_obj = getattr(part, \"image_url\", None)\n",
    "                            if img_url_obj:\n",
    "                                if isinstance(img_url_obj, dict):\n",
    "                                    url = img_url_obj.get(\"url\", \"\").strip()\n",
    "                                else:\n",
    "                                    url = getattr(img_url_obj, \"url\", \"\")\n",
    "                                    if url:\n",
    "                                        url = url.strip()\n",
    "                                if url and isinstance(url, str) and len(url) > 0:\n",
    "                                    images.append(url)\n",
    "            except TypeError:\n",
    "                # Not iterable, log and skip\n",
    "                logger.debug(\n",
    "                    f\"extract_text_and_images_from_messages - Message {msg_idx}: \"\n",
    "                    f\"content is not iterable, skipping\"\n",
    "                )\n",
    "    \n",
    "    # Combine all text parts\n",
    "    full_text = \" \".join(full_text_parts).strip()\n",
    "    \n",
    "    # Fallback text if empty\n",
    "    if not full_text:\n",
    "        logger.warning(\"No text extracted from messages, using default prompt\")\n",
    "        full_text = \"routing query\"\n",
    "    \n",
    "    logger.info(f\"extract_text_and_images_from_messages - Final: text_len={len(full_text)}, images={len(images)}\")\n",
    "    \n",
    "    return full_text, images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752b008",
   "metadata": {},
   "source": [
    "### 2.4 Model Name Mapping\n",
    "\n",
    "Maps router output model names to target pipeline model names. Includes direct lookup and case-insensitive matching for flexibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4085887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_router_model_to_target(router_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Map router output model name to target pipeline model name.\n",
    "    \n",
    "    Args:\n",
    "        router_model: Model name predicted by router (e.g., 'gpt-5-chat')\n",
    "    \n",
    "    Returns:\n",
    "        Target model name for routing (e.g., 'openai/gpt-oss-120b')\n",
    "    \"\"\"\n",
    "    # Direct lookup\n",
    "    if router_model in MODEL_ROUTER_TO_TARGET:\n",
    "        return MODEL_ROUTER_TO_TARGET[router_model]\n",
    "    \n",
    "    # Case-insensitive lookup\n",
    "    lower_model = router_model.lower()\n",
    "    for key, value in MODEL_ROUTER_TO_TARGET.items():\n",
    "        if key.lower() == lower_model:\n",
    "            return value\n",
    "            \n",
    "    # Default fallback\n",
    "    logger.warning(\n",
    "        f\"Unknown router model '{router_model}', using default fallback: {DEFAULT_FALLBACK_MODEL}\"\n",
    "    )\n",
    "    return DEFAULT_FALLBACK_MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d34ed5",
   "metadata": {},
   "source": [
    "### 2.5 Cost-Based Model Selection\n",
    "\n",
    "Selects the best model using cost-based routing strategy:\n",
    "1. Filter models that meet their confidence threshold\n",
    "2. Among qualified models, select the one with lowest cost factor\n",
    "3. Falls back to highest probability if no thresholds specified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model_by_cost(\n",
    "    probabilities: Dict[str, float],\n",
    "    model_thresholds: Optional[Dict[str, float]] = None,\n",
    "    model_costs: Optional[Dict[str, float]] = None\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Select the best model using cost-based routing.\n",
    "    \n",
    "    Args:\n",
    "        probabilities: Dict of model_name -> confidence score\n",
    "        model_thresholds: Dict of model_name -> minimum confidence threshold\n",
    "        model_costs: Dict of model_name -> cost factor (lower is better)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (selected_model, selection_reason)\n",
    "    \"\"\"\n",
    "    if not probabilities:\n",
    "        raise ValueError(\"No probabilities provided\")\n",
    "    \n",
    "    # Filter models that meet their thresholds\n",
    "    qualified_models = []\n",
    "    \n",
    "    if model_thresholds:\n",
    "        for model_name, prob in probabilities.items():\n",
    "            threshold = model_thresholds.get(model_name, 0.0)\n",
    "            if prob >= threshold:\n",
    "                qualified_models.append((model_name, prob, threshold))\n",
    "        \n",
    "        if qualified_models:\n",
    "            logger.debug(\n",
    "                f\"Models meeting thresholds: \"\n",
    "                f\"{[(m, f'{p:.3f}') for m, p, t in qualified_models]}\"\n",
    "            )\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"No models met their thresholds, falling back to highest probability\"\n",
    "            )\n",
    "            # Fallback: use highest probability if nothing meets threshold\n",
    "            best_model = max(probabilities.items(), key=lambda x: x[1])\n",
    "            return best_model[0], \"threshold_fallback\"\n",
    "    else:\n",
    "        # No thresholds - all models are qualified\n",
    "        qualified_models = [(m, p, 0.0) for m, p in probabilities.items()]\n",
    "    \n",
    "    # If cost factors provided, select by lowest cost among qualified models\n",
    "    if model_costs:\n",
    "        # Find the qualified model with lowest cost\n",
    "        best_model_name = None\n",
    "        best_cost = float('inf')\n",
    "        best_prob = 0.0\n",
    "        \n",
    "        for model_name, prob, threshold in qualified_models:\n",
    "            cost = model_costs.get(model_name, float('inf'))\n",
    "            \n",
    "            # Prefer lower cost, but use probability as tiebreaker\n",
    "            if cost < best_cost or (cost == best_cost and prob > best_prob):\n",
    "                best_model_name = model_name\n",
    "                best_cost = cost\n",
    "                best_prob = prob\n",
    "        \n",
    "        if best_model_name:\n",
    "            logger.debug(\n",
    "                f\"Cost-based selection: {best_model_name} \"\n",
    "                f\"(cost={best_cost:.3f}, prob={best_prob:.3f})\"\n",
    "            )\n",
    "            return best_model_name, \"cost_optimized\"\n",
    "    \n",
    "    # Default: select highest probability among qualified models\n",
    "    best_model = max(qualified_models, key=lambda x: x[1])\n",
    "    return best_model[0], \"highest_probability\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b36ea6",
   "metadata": {},
   "source": [
    "### 2.6 Objective Function Registration\n",
    "\n",
    "The main objective function that ties everything together. This function:\n",
    "1. Configures thresholds and costs from the config\n",
    "2. Loads the router on startup\n",
    "3. Provides an async response function for routing requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de16ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNObjectiveConfig(FunctionBaseConfig, name=\"nn_objective_fn\"):\n",
    "    \"\"\"Neural network objective function configuration for model routing.\n",
    "    \n",
    "    Attributes:\n",
    "        model_thresholds: Dict of model_name -> minimum confidence threshold\n",
    "                         Only routes to models meeting their threshold\n",
    "        model_costs: Dict of model_name -> cost factor for cost-based selection\n",
    "                    Among models meeting thresholds, selects lowest cost model\n",
    "    \"\"\"\n",
    "    model_thresholds: Optional[Dict[str, float]] = None\n",
    "    model_costs: Optional[Dict[str, float]] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4820981",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_function(config_type=NNObjectiveConfig)\n",
    "async def nn_objective_fn(config: NNObjectiveConfig, _builder: Builder):\n",
    "    \"\"\"\n",
    "    Neural network objective function for intelligent cost-optimized model routing.\n",
    "    \n",
    "    Uses a pre-trained neural network router with:\n",
    "    - HuggingFace embedding model (Nemotron VL) for multimodal content encoding\n",
    "    - Trained router network to predict best model\n",
    "    - Confidence thresholds (from config) for quality gates\n",
    "    - Cost-based selection among qualified models\n",
    "    - Multi-turn conversation context support\n",
    "    \n",
    "    Routing Strategy:\n",
    "    1. Generate embedding for the query\n",
    "    2. Get confidence scores from neural router\n",
    "    3. Filter models meeting their confidence threshold\n",
    "    4. Select lowest-cost model among qualified options\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract configuration\n",
    "    model_thresholds = config.model_thresholds or {}\n",
    "    model_costs = config.model_costs or {}\n",
    "    \n",
    "    logger.info(\"nn_objective_fn: Configuration loaded\")\n",
    "    logger.info(f\"  Thresholds: {model_thresholds}\")\n",
    "    logger.info(f\"  Costs: {model_costs}\")\n",
    "    \n",
    "    # Load router on startup - this happens BEFORE _response_fn\n",
    "    logger.info(\"nn_objective_fn: Initializing router...\")\n",
    "    router = _load_router()\n",
    "    logger.info(\"nn_objective_fn: Router ready\")\n",
    "    \n",
    "    async def _response_fn(chat_request: OpenAIChatRequest) -> Tuple[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Route a chat request to the best model using the neural network router.\n",
    "        \"\"\"\n",
    "        response_start = time.perf_counter()\n",
    "        \n",
    "        try:\n",
    "            # ===== EXTRACT MESSAGES =====\n",
    "            extract_start = time.perf_counter()\n",
    "            messages = chat_request.messages\n",
    "\n",
    "            logger.info(f\"Routing with messages: {messages}\")\n",
    "            \n",
    "            if not messages:\n",
    "                logger.warning(\"No messages received, using default fallback model\")\n",
    "                return DEFAULT_FALLBACK_MODEL\n",
    "            \n",
    "            # Convert messages to dicts if needed (handle Pydantic models)\n",
    "            messages_dict = []\n",
    "            for msg in messages:\n",
    "                if hasattr(msg, 'model_dump'):\n",
    "                    messages_dict.append(msg.model_dump())\n",
    "                elif hasattr(msg, '__dict__'):\n",
    "                    messages_dict.append(vars(msg))\n",
    "                elif isinstance(msg, dict):\n",
    "                    messages_dict.append(msg)\n",
    "                else:\n",
    "                    messages_dict.append(dict(msg))\n",
    "            \n",
    "            extract_time = time.perf_counter() - extract_start\n",
    "            logger.debug(f\"Extracted {len(messages_dict)} messages in {extract_time*1000:.2f}ms\")\n",
    "            \n",
    "            # ===== PARSE TEXT AND IMAGES =====\n",
    "            parse_start = time.perf_counter()\n",
    "            full_text, images = extract_text_and_images_from_messages(messages_dict)\n",
    "            parse_time = time.perf_counter() - parse_start\n",
    "            \n",
    "            logger.debug(\n",
    "                f\"Parsed {len(images)} images, \"\n",
    "                f\"text: {len(full_text)} chars, \"\n",
    "                f\"time: {parse_time*1000:.2f}ms\"\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Routing with full text: {full_text} and number of images: {len(images)}\")\n",
    "            \n",
    "            # ===== ROUTE USING NEURAL NETWORK =====\n",
    "            route_start = time.perf_counter()\n",
    "            \n",
    "            # Get probabilities from router (without thresholds - raw scores)\n",
    "            # Use async version to avoid event loop conflicts with CLIP client\n",
    "            embedding = await router.generate_embedding_async(full_text, images)\n",
    "            embedding_2d = embedding.reshape(1, -1)\n",
    "            \n",
    "            # Get raw probabilities from router\n",
    "            router.router_model.eval()\n",
    "            with torch.no_grad():\n",
    "                proba = router.router_model(torch.FloatTensor(embedding_2d).to(router.device))\n",
    "                proba = proba.cpu().numpy()\n",
    "            \n",
    "            # Build probability dict\n",
    "            probabilities = {\n",
    "                model: float(proba[0, i])\n",
    "                for i, model in enumerate(router.model_names)\n",
    "            }\n",
    "            \n",
    "            route_time = time.perf_counter() - route_start\n",
    "            \n",
    "            # ===== COST-BASED SELECTION =====\n",
    "            cost_select_start = time.perf_counter()\n",
    "            \n",
    "            router_model, selection_reason = select_best_model_by_cost(\n",
    "                probabilities=probabilities,\n",
    "                model_thresholds=model_thresholds,\n",
    "                model_costs=model_costs\n",
    "            )\n",
    "            \n",
    "            cost_select_time = time.perf_counter() - cost_select_start\n",
    "            confidence = probabilities.get(router_model, 0.0)\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Routing decision | \"\n",
    "                f\"Model: {router_model} | \"\n",
    "                f\"Confidence: {confidence:.3f} | \"\n",
    "                f\"Selection: {selection_reason} | \"\n",
    "                f\"Probabilities: {{{', '.join(f'{m}: {p:.3f}' for m, p in probabilities.items())}}} | \"\n",
    "                f\"Route+Select time: {route_time*1000 + cost_select_time*1000:.2f}ms\"\n",
    "            )\n",
    "            \n",
    "            # ===== MAP TO TARGET MODEL =====\n",
    "            map_start = time.perf_counter()\n",
    "            target_model = map_router_model_to_target(router_model)\n",
    "            map_time = time.perf_counter() - map_start\n",
    "            \n",
    "            total_time = time.perf_counter() - response_start\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Final routing | \"\n",
    "                f\"Router model: {router_model} -> Target: {target_model} | \"\n",
    "                f\"Total time: {total_time*1000:.2f}ms\"\n",
    "            )\n",
    "            \n",
    "            return target_model, probabilities\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in nn_objective_fn routing: {e}\", exc_info=True)\n",
    "            logger.warning(f\"Using default fallback model: {DEFAULT_FALLBACK_MODEL}\")\n",
    "            return DEFAULT_FALLBACK_MODEL\n",
    "    \n",
    "    yield FunctionInfo.from_fn(\n",
    "        _response_fn,\n",
    "        description=\"Neural network objective function for intelligent model routing using embeddings and trained router.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8bd7e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Example Usage\n",
    "\n",
    "The following cells demonstrate how to use the ModelRouter directly for multimodal routing. This shows the core functionality without the full objective function integration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5784b9",
   "metadata": {},
   "source": [
    "### 3.1 Load the Router\n",
    "\n",
    "Initialize the ModelRouter with the trained neural network. This requires a running CLIP server for embedding generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af751143-38da-4f8b-8bcf-66729507d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PREREQUISITE: Deploy CLIP server with Docker (Linux)\n",
    "# =====================================================\n",
    "# Run this command in your terminal BEFORE proceeding with the notebook.\n",
    "\n",
    "!docker run -d --rm \\\n",
    "    --name clip_server \\\n",
    "    --gpus all \\\n",
    "    -p 51000:51000 \\\n",
    "    jinaai/clip-as-service:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the router (requires CLIP server to be running)\n",
    "# To start a CLIP server: docker run -p 51000:51000 jinaai/clip-as-service:latest\n",
    "router = _load_router()\n",
    "print(f\"Router loaded with models: {router.model_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9efc8",
   "metadata": {},
   "source": [
    "### 3.2 Text-Only Routing Example\n",
    "\n",
    "Route a simple text query to determine the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b174be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Text-only routing\n",
    "text_query = \"What is the capital of France?\"\n",
    "selected_model = router.route(text_query)\n",
    "print(f\"Query: {text_query}\")\n",
    "print(f\"Selected model: {selected_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452c330",
   "metadata": {},
   "source": [
    "### 3.3 Multi-Turn Conversation Example\n",
    "\n",
    "Demonstrate routing with a multi-turn conversation using the message parsing utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70700741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multi-turn conversation routing\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I'm working on a machine learning project.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"That sounds interesting! What kind of ML project are you working on?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm building a neural network for image classification. Can you help me understand backpropagation?\"}\n",
    "]\n",
    "\n",
    "# Extract text from messages\n",
    "full_text, images = extract_text_and_images_from_messages(messages)\n",
    "print(f\"Extracted text: {full_text[:200]}...\")\n",
    "print(f\"Number of images: {len(images)}\")\n",
    "\n",
    "# Route the conversation\n",
    "selected_model = router.route(full_text, images=images if images else None)\n",
    "print(f\"Selected model: {selected_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95306d",
   "metadata": {},
   "source": [
    "### 3.4 Cost-Based Selection Example\n",
    "\n",
    "Demonstrate how cost-based selection works with custom thresholds and cost factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Cost-based selection with simulated probabilities\n",
    "simulated_probabilities = {\n",
    "    'gpt-5-chat': 0.75,\n",
    "    'nvidia/nemotron-nano-12b-v2-vl': 0.60,\n",
    "    'nvidia/nvidia-nemotron-nano-9b-v2': 0.55,\n",
    "}\n",
    "\n",
    "# Define cost factors (lower = cheaper)\n",
    "model_costs = {\n",
    "    'gpt-5-chat': 1.0,  # Most expensive\n",
    "    'nvidia/nemotron-nano-12b-v2-vl': 0.3,  # Mid-tier\n",
    "    'nvidia/nvidia-nemotron-nano-9b-v2': 0.1,  # Cheapest\n",
    "}\n",
    "\n",
    "# Select best model with cost optimization\n",
    "selected_model, reason = select_best_model_by_cost(\n",
    "    probabilities=simulated_probabilities,\n",
    "    model_thresholds=CUSTOM_THRESHOLDS,\n",
    "    model_costs=model_costs\n",
    ")\n",
    "\n",
    "print(f\"Probabilities: {simulated_probabilities}\")\n",
    "print(f\"Thresholds: {CUSTOM_THRESHOLDS}\")\n",
    "print(f\"Cost factors: {model_costs}\")\n",
    "print(f\"Selected model: {selected_model}\")\n",
    "print(f\"Selection reason: {reason}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df882e34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Async Usage with `nn_objective_fn`\n",
    "\n",
    "The `nn_objective_fn` is designed for **async environments** with the NAT framework. This section demonstrates how to use it directly for testing and development.\n",
    "\n",
    "Key differences from the direct `ModelRouter` usage:\n",
    "- **Async interface**: Uses `async/await` for non-blocking operation\n",
    "- **OpenAI-compatible**: Accepts `OpenAIChatRequest` objects (OpenAI chat completion format)\n",
    "- **Framework integration**: Designed to work with NAT's `Builder` and `FunctionInfo` system\n",
    "- **Configuration-driven**: Uses `NNObjectiveConfig` for thresholds and costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d6096",
   "metadata": {},
   "source": [
    "### 4.1 Create a Helper to Run the Objective Function\n",
    "\n",
    "Since `nn_objective_fn` is an async generator designed for the NAT framework, we need a helper to extract and use the response function directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf92270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def get_objective_response_fn(config: NNObjectiveConfig):\n",
    "    \"\"\"\n",
    "    Helper to extract the response function from nn_objective_fn.\n",
    "    \n",
    "    In production, the NAT framework handles this automatically.\n",
    "    This helper allows us to test the objective function directly.\n",
    "    \"\"\"\n",
    "    # The @register_function decorator uses asynccontextmanager\n",
    "    # which yields the FunctionInfo directly when entering the context\n",
    "    async with nn_objective_fn(config, _builder=None) as function_info:\n",
    "        # FunctionInfo has single_fn (regular async) and stream_fn (async generator)\n",
    "        # Our _response_fn is a regular async function, so use single_fn\n",
    "        return function_info.single_fn\n",
    "\n",
    "print(\"Helper function created for testing nn_objective_fn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5882cc89",
   "metadata": {},
   "source": [
    "### 4.2 Route Using OpenAI Chat Request Format\n",
    "\n",
    "Create an `OpenAIChatRequest` and route it using the objective function. This demonstrates the production API format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demo_async_routing():\n",
    "    \"\"\"Demonstrate async routing with nn_objective_fn\"\"\"\n",
    "    \n",
    "    # 1. Create configuration with custom thresholds and costs\n",
    "    config = NNObjectiveConfig(\n",
    "        model_thresholds={\n",
    "            'gpt-5-chat': 0.70,\n",
    "            'nvidia/nemotron-nano-12b-v2-vl': 0.55,\n",
    "            'nvidia/nvidia-nemotron-nano-9b-v2': 0.50,\n",
    "        },\n",
    "        model_costs={\n",
    "            'gpt-5-chat': 1.0,              # Most expensive\n",
    "            'nvidia/nemotron-nano-12b-v2-vl': 0.3,  # Mid-tier\n",
    "            'nvidia/nvidia-nemotron-nano-9b-v2': 0.1,  # Cheapest\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"Configuration:\")\n",
    "    print(f\"  Thresholds: {config.model_thresholds}\")\n",
    "    print(f\"  Costs: {config.model_costs}\")\n",
    "    print()\n",
    "    \n",
    "    # 2. Get the response function from the objective\n",
    "    print(\"Initializing objective function...\")\n",
    "    response_fn = await get_objective_response_fn(config)\n",
    "    print(\"✓ Objective function ready\\n\")\n",
    "    \n",
    "    # 3. Create OpenAI-format chat requests\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Simple question\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Multi-turn conversation\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"I need help with machine learning.\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"I'd be happy to help! What specific aspect of ML?\"},\n",
    "                {\"role\": \"user\", \"content\": \"Explain backpropagation in neural networks.\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Complex reasoning\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Analyze the philosophical implications of artificial general intelligence on human society, considering economic, ethical, and existential perspectives.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 4. Route each request\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Routing Results\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for test in test_cases:\n",
    "        # Create OpenAIChatRequest\n",
    "        chat_request = OpenAIChatRequest(\n",
    "            model=\"router\",  # Placeholder - router will select actual model\n",
    "            messages=test[\"messages\"]\n",
    "        )\n",
    "        \n",
    "        # Route the request\n",
    "        result = await response_fn(chat_request)\n",
    "        \n",
    "        # Handle result (can be tuple of (model, probs) or just model string)\n",
    "        if isinstance(result, tuple):\n",
    "            target_model, probabilities = result\n",
    "            print(f\"\\n{test['name']}:\")\n",
    "            print(f\"  → Selected model: {target_model}\")\n",
    "            print(f\"  → Probabilities: {', '.join(f'{m}: {p:.3f}' for m, p in probabilities.items())}\")\n",
    "        else:\n",
    "            print(f\"\\n{test['name']}:\")\n",
    "            print(f\"  → Selected model: {result}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Async routing demo complete!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Run the async demo\n",
    "await demo_async_routing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe82beb",
   "metadata": {},
   "source": [
    "### 4.3 Multimodal Routing with Images\n",
    "\n",
    "The objective function also supports multimodal requests with images using the OpenAI vision format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cde2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demo_multimodal_routing():\n",
    "    \"\"\"Demonstrate multimodal routing with images\"\"\"\n",
    "    \n",
    "    # Create configuration\n",
    "    config = NNObjectiveConfig(\n",
    "        model_thresholds=CUSTOM_THRESHOLDS\n",
    "    )\n",
    "    \n",
    "    # Get response function (reuses loaded router)\n",
    "    response_fn = await get_objective_response_fn(config)\n",
    "    \n",
    "    # Create a multimodal request with image (OpenAI vision format)\n",
    "    # In production, this would be a real base64 image\n",
    "    multimodal_request = OpenAIChatRequest(\n",
    "        model=\"router\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"What objects do you see in this image? Describe them in detail.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg==\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"Multimodal Request (with image):\")\n",
    "    print(\"  Text: 'What objects do you see in this image?'\")\n",
    "    print(\"  Image: [base64 encoded image]\")\n",
    "    print()\n",
    "    \n",
    "    # Route the multimodal request\n",
    "    result = await response_fn(multimodal_request)\n",
    "    \n",
    "    if isinstance(result, tuple):\n",
    "        target_model, probabilities = result\n",
    "        print(f\"Selected model: {target_model}\")\n",
    "        print(f\"Probabilities:\")\n",
    "        for model, prob in sorted(probabilities.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {model}: {prob:.3f}\")\n",
    "    else:\n",
    "        print(f\"Selected model: {result}\")\n",
    "\n",
    "# Run multimodal demo\n",
    "await demo_multimodal_routing()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
