{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f98ccf",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7df3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb607b54",
   "metadata": {},
   "source": [
    "## ``prepare_hf_data.py``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bd453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "\n",
    "MODEL_IDS = {\n",
    "    \"gpt-5-chat\": 0,\n",
    "    \"nemotron-nano-12b-v2-vl\": 1,\n",
    "    \"meta/llama-3.2-1b-instruct\": 2\n",
    "}\n",
    "\n",
    "def retry_with_exponential_backoff(\n",
    "    max_retries=50,\n",
    "    initial_delay=1,\n",
    "    exponential_base=2,\n",
    "    max_delay=3000,\n",
    "    jitter=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Decorator to retry a function with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        initial_delay: Initial delay in seconds\n",
    "        exponential_base: Base for exponential backoff\n",
    "        max_delay: Maximum delay between retries\n",
    "        jitter: Add random jitter to prevent thundering herd\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            delay = initial_delay\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    \n",
    "                    if attempt == max_retries:\n",
    "                        print(f\"Max retries ({max_retries}) reached for {func.__name__}. Last error: {e}\")\n",
    "                        raise\n",
    "                    \n",
    "                    # Calculate delay with exponential backoff\n",
    "                    current_delay = min(delay * (exponential_base ** attempt), max_delay)\n",
    "                    \n",
    "                    # Add jitter if enabled\n",
    "                    if jitter:\n",
    "                        current_delay = current_delay * (0.5 + random.random())\n",
    "                    \n",
    "                    print(f\"Error in {func.__name__} (attempt {attempt + 1}/{max_retries + 1}): {e}\")\n",
    "                    print(f\"Retrying in {current_delay:.2f} seconds...\")\n",
    "                    time.sleep(current_delay)\n",
    "            \n",
    "            raise last_exception\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_from_disk(\"finevision_combined_images_text\")\n",
    "\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = \"https://jcull-mgh2hy0n-eastus2.cognitiveservices.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY = \"\"\n",
    "\n",
    "\n",
    "def prepare_prompt_with_images(sample):\n",
    "    \"\"\"\n",
    "    Prepare the prompt by replacing <imageN> tags with actual image references\n",
    "    and collecting the images\n",
    "    \"\"\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": sample[\"text\"][0][\"content\"],\n",
    "        \"images\": sample[\"images\"],\n",
    "        \"answer\": sample[\"answer\"]\n",
    "    }\n",
    "\n",
    "\n",
    "@retry_with_exponential_backoff(max_retries=5, initial_delay=1)\n",
    "def generate_response_with_openai(prompt_data):\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=\"2025-01-01-preview\"\n",
    "    )\n",
    "    # Prepare content with text and images\n",
    "    content = [{\"type\": \"text\", \"text\": prompt_data['prompt']}]\n",
    "    \n",
    "    # Add images if available\n",
    "    for image in prompt_data.get('images', []):\n",
    "        if image is not None:\n",
    "            # Convert PIL Image to base64\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "            \n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-chat\",\n",
    "        messages=[{\"role\": \"user\", \"content\": content}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# call nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16 to generate responses\n",
    "@retry_with_exponential_backoff(max_retries=5, initial_delay=1)\n",
    "def generate_response_with_nemotron_vlm(prompt_data):\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://10.185.119.147:8001/v1\",\n",
    "        api_key=\"not-needed\"\n",
    "    )\n",
    "    content = [{\"type\": \"text\", \"text\": prompt_data['prompt']}]\n",
    "    \n",
    "    # Add images if available\n",
    "    for image in prompt_data.get('images', []):\n",
    "        if image is not None:\n",
    "            # Convert PIL Image to base64\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "            \n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"nvidia/nemotron-nano-12b-v2-vl\",\n",
    "        messages=[{\"role\": \"user\", \"content\": content}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def generate_response_with_llama(prompt_data):\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://10.185.119.221:8007/v1\",\n",
    "        api_key=\"not-needed\"\n",
    "    )\n",
    "    # Build messages from conversation\n",
    "    messages = []\n",
    "    conversation = prompt_data['prompt']\n",
    "    first_user_turn = True\n",
    "    \n",
    "    # Process each turn in the conversation\n",
    "    for turn in conversation:\n",
    "        role = turn.get(\"role\", \"user\")\n",
    "        content_text = turn.get(\"content\", \"\")\n",
    "        \n",
    "        # For the first user message, include images\n",
    "        if role == \"user\" and first_user_turn:\n",
    "            first_user_turn = False\n",
    "            content = [{\"type\": \"text\", \"text\": content_text}]\n",
    "            \n",
    "            # Add images if available\n",
    "            for image_dict in prompt_data.get('images', []):\n",
    "                if image_dict is not None and 'bytes' in image_dict:\n",
    "                    # Get image bytes and convert to base64\n",
    "                    img_bytes = image_dict['bytes']\n",
    "                    if img_bytes is not None:\n",
    "                        # Convert bytes to PIL Image then to base64\n",
    "                        image = Image.open(io.BytesIO(img_bytes))\n",
    "                        buffered = io.BytesIO()\n",
    "                        image.save(buffered, format=\"PNG\")\n",
    "                        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "                        \n",
    "                        content.append({\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                            }\n",
    "                        })\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "        else:\n",
    "            messages.append({\"role\": role, \"content\": content_text})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta/llama-3.2-1b-instruct\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def generate_model_responses(prompt_data, model_ids):\n",
    "    \"\"\"\n",
    "    Generate responses from different models for the given prompt\n",
    "    \n",
    "    Args:\n",
    "        prompt_data: Dict with 'prompt', 'images', etc.\n",
    "        model_ids: List of model IDs to generate responses from\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping model_id to response\n",
    "    \"\"\"\n",
    "    responses = {}\n",
    "    for model_id in model_ids:\n",
    "        try:\n",
    "            if model_id == \"gpt-5-chat\":\n",
    "                responses[model_id] = generate_response_with_openai(prompt_data)\n",
    "            elif model_id == \"nemotron-nano-12b-v2-vl\":\n",
    "                responses[model_id] = generate_response_with_nemotron_vlm(prompt_data)\n",
    "            elif model_id == \"meta/llama-3.2-1b-instruct\":\n",
    "                responses[model_id] = \"I cannot answer that question because I am a model that can only answer questions without images.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate response for {model_id} after all retries. Skipping. Error: {e}\")\n",
    "            continue\n",
    "    return responses\n",
    "\n",
    "\n",
    "@retry_with_exponential_backoff(max_retries=5, initial_delay=1)\n",
    "def judge_single_response(prompt_data, model_response, ground_truth_label):\n",
    "    \"\"\"\n",
    "    Use Azure OpenAI as a judge to evaluate if a single model response correctly answers the question.\n",
    "    Returns True if the model response is correct, False otherwise.\n",
    "    \"\"\"\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=\"2025-01-01-preview\"\n",
    "    )\n",
    "\n",
    "    # Compose the system prompt for the judge\n",
    "    sys_prompt = (\n",
    "        \"You are an expert AI tasked with evaluating whether a model's response correctly answers a question. \"\n",
    "        \"You will be given the original prompt (with images), the ground truth answer, and the model's response. \"\n",
    "        \"Your task is to determine whether the model's response correctly answers the question and matches \"\n",
    "        \"or logically aligns with the ground truth answer.\\n\\n\"\n",
    "        \"Reply strictly with 'yes' if the response is correct, or 'no' if it is incorrect.\"\n",
    "    )\n",
    "\n",
    "    content = [{\"type\": \"text\", \"text\": \"PROMPT: \" + prompt_data['prompt']}]\n",
    "    \n",
    "    # Add images if available\n",
    "    for image in prompt_data.get('images', []):\n",
    "        if image is not None:\n",
    "            # Convert PIL Image to base64\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "            \n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            })\n",
    "\n",
    "    content.append({\"type\": \"text\", \"text\": f\"\\n\\nGROUND TRUTH ANSWER: {ground_truth_label}\"})\n",
    "    content.append({\"type\": \"text\", \"text\": f\"\\n\\nMODEL RESPONSE: {model_response}\"})\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5-chat\",\n",
    "            messages=messages\n",
    "        )\n",
    "        judge_answer = response.choices[0].message.content.strip().lower()\n",
    "        return judge_answer.startswith(\"yes\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Azure judge evaluation: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_evaluation_data(dataset_split, num_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate all models on the dataset and record success (1) or failure (0) for each model.\n",
    "    \n",
    "    Args:\n",
    "        dataset_split: The dataset split to process\n",
    "        num_samples: Number of samples to process (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        List of evaluation samples with model success indicators\n",
    "    \"\"\"\n",
    "    evaluation_data = []\n",
    "    model_list = list(MODEL_IDS.keys())\n",
    "    \n",
    "    # Limit number of samples if specified\n",
    "    num_to_process = len(dataset_split) if num_samples is None else min(num_samples, len(dataset_split))\n",
    "    \n",
    "    print(f\"\\nProcessing {num_to_process} samples...\")\n",
    "    \n",
    "    skipped_samples = 0\n",
    "    \n",
    "    for idx in tqdm(range(num_to_process)):\n",
    "        sample = dataset_split[idx]\n",
    "        \n",
    "        # Prepare prompt with images\n",
    "        prompt_data = prepare_prompt_with_images(sample)\n",
    "        \n",
    "        # Skip if prompt is empty\n",
    "        if not prompt_data[\"prompt\"] or len(prompt_data[\"prompt\"]) == 0:\n",
    "            print(f\"Skipping sample {idx}: Empty prompt\")\n",
    "            skipped_samples += 1\n",
    "            continue\n",
    "        \n",
    "        responses = generate_model_responses(prompt_data, model_list)\n",
    "        \n",
    "        # Judge each model's response independently\n",
    "        model_scores = {}\n",
    "        all_failed = True\n",
    "        \n",
    "        for model_id in model_list:\n",
    "            if model_id not in responses:\n",
    "                print(f\"Skipping model {model_id} for sample {idx}: No response generated\")\n",
    "                model_scores[model_id] = 0\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Judge if the response is correct\n",
    "                is_correct = judge_single_response(prompt_data, responses[model_id], prompt_data[\"answer\"])\n",
    "                model_scores[model_id] = 1 if is_correct else 0\n",
    "                if is_correct:\n",
    "                    all_failed = False\n",
    "                print(f\"Sample {idx} - {model_id}: {'✓' if is_correct else '✗'}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to judge {model_id} for sample {idx}: {e}\")\n",
    "                model_scores[model_id] = 0\n",
    "        \n",
    "        # Skip if all models failed (optional - remove this if you want to keep all samples)\n",
    "        # if all_failed:\n",
    "        #     print(f\"Skipping sample {idx}: All models failed\")\n",
    "        #     skipped_samples += 1\n",
    "        #     continue\n",
    "        \n",
    "        # Convert images to base64 for serialization\n",
    "        images_base64 = []\n",
    "        for image in prompt_data[\"images\"]:\n",
    "            if image is not None:\n",
    "                buffered = io.BytesIO()\n",
    "                image.save(buffered, format=\"PNG\")\n",
    "                img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "                images_base64.append(img_base64)\n",
    "        \n",
    "        evaluation_data.append({\n",
    "            \"idx\": idx,\n",
    "            \"prompt\": prompt_data[\"prompt\"],\n",
    "            \"answer\": prompt_data[\"answer\"],\n",
    "            \"images\": images_base64,\n",
    "            \"model_scores\": model_scores\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nSkipped {skipped_samples} samples due to failures\")\n",
    "    \n",
    "    return evaluation_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Evaluate all models on the dataset\n",
    "    # For demonstration, process only a small subset\n",
    "    # Set num_samples=None to process all data\n",
    "    all_evaluations = create_evaluation_data(dataset, num_samples=2000)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    train_evaluations = all_evaluations[:1600]  # 80% for training\n",
    "    test_evaluations = all_evaluations[1600:]   # 20% for testing\n",
    "    \n",
    "    print(f\"\\nGenerated {len(train_evaluations)} train samples\")\n",
    "    print(f\"Generated {len(test_evaluations)} test samples\")\n",
    "    \n",
    "    # Calculate and print success rates for each model\n",
    "    print(\"\\nModel Success Rates (Train):\")\n",
    "    for model_id in MODEL_IDS.keys():\n",
    "        success_count = sum(1 for item in train_evaluations if item[\"model_scores\"].get(model_id, 0) == 1)\n",
    "        total_count = len(train_evaluations)\n",
    "        print(f\"  {model_id}: {success_count}/{total_count} ({100*success_count/total_count:.1f}%)\")\n",
    "    \n",
    "    # Save the evaluation data\n",
    "    output_train_path = \"hf_evaluations_train.json\"\n",
    "    output_test_path = \"hf_evaluations_test.json\"\n",
    "    \n",
    "    with open(output_train_path, \"w\") as f:\n",
    "        json.dump(train_evaluations, f, indent=2)\n",
    "    \n",
    "    with open(output_test_path, \"w\") as f:\n",
    "        json.dump(test_evaluations, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nSaved to {output_train_path} and {output_test_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693b5b1",
   "metadata": {},
   "source": [
    "## ``generate_embeddings.py``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from clip_client import Client\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CLIP embedding dimensions\n",
    "CLIP_TEXT_DIM = 512\n",
    "CLIP_IMAGE_DIM = 512\n",
    "COMBINED_DIM = CLIP_TEXT_DIM + CLIP_IMAGE_DIM  # 1024\n",
    "\n",
    "\n",
    "def load_embedding_model():\n",
    "    \"\"\"Load the CLIP client\"\"\"\n",
    "    print(\"Connecting to CLIP server...\")\n",
    "    client = Client('grpc://10.185.119.147:51000')\n",
    "    print(\"Connected to CLIP server\")\n",
    "    return client\n",
    "\n",
    "\n",
    "def load_json_data(json_path):\n",
    "    \"\"\"\n",
    "    Load data from JSON file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_prompt_with_images(sample):\n",
    "    \"\"\"\n",
    "    Prepare the prompt by collecting images from JSON format\n",
    "    \"\"\"\n",
    "    prompt = sample[\"prompt\"]\n",
    "    images = sample.get(\"images\", [])\n",
    "    \n",
    "    # Filter out None values if any\n",
    "    images = [img for img in images if img is not None]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": images,\n",
    "        \"task\": sample.get(\"task\", \"\"),\n",
    "        \"label\": sample.get(\"label\", \"\")\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_embeddings_for_dataset(clip_client, dataset_split, max_samples=None):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all prompts in the dataset using CLIP\n",
    "    \n",
    "    Args:\n",
    "        clip_client: The CLIP client\n",
    "        dataset_split: Dataset split to process\n",
    "        max_samples: Maximum number of samples to process (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings [num_samples, 1024]\n",
    "        - If text + image: [text_embedding(512) | image_embedding(512)]\n",
    "        - If text only: [text_embedding(512) | zeros(512)]\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    \n",
    "    num_to_process = len(dataset_split) if max_samples is None else min(max_samples, len(dataset_split))\n",
    "    \n",
    "    print(f\"Generating embeddings for {num_to_process} samples...\")\n",
    "    print(f\"Output embedding dimension: {COMBINED_DIM}\")\n",
    "    \n",
    "    for idx in tqdm(range(num_to_process)):\n",
    "        sample = dataset_split[idx]\n",
    "        prompt_data = prepare_prompt_with_images(sample)\n",
    "        \n",
    "        text = prompt_data['prompt']\n",
    "        images = prompt_data['images']\n",
    "        \n",
    "        # Generate embedding based on whether images are present\n",
    "        if len(images) > 0:\n",
    "            # Has both text and image - concatenate text and image embeddings\n",
    "            # Encode text and image separately\n",
    "            text_embedding = clip_client.encode([text])  # Shape: (1, 512)\n",
    "            image_data_uri = f\"data:image/png;base64,{images[0]}\"\n",
    "            image_embedding = clip_client.encode([image_data_uri])  # Shape: (1, 512)\n",
    "            \n",
    "            # Concatenate text and image embeddings\n",
    "            combined_embedding = np.concatenate([\n",
    "                text_embedding[0],  # (512,)\n",
    "                image_embedding[0]  # (512,)\n",
    "            ])  # Result: (1024,)\n",
    "            \n",
    "            embeddings_list.append(combined_embedding)\n",
    "        else:\n",
    "            # Text only - pad with zeros\n",
    "            text_embedding = clip_client.encode([text])  # Shape: (1, 512)\n",
    "            \n",
    "            # Pad with 512 zeros\n",
    "            padding = np.zeros(CLIP_IMAGE_DIM)\n",
    "            combined_embedding = np.concatenate([\n",
    "                text_embedding[0],  # (512,)\n",
    "                padding  # (512,)\n",
    "            ])  # Result: (1024,)\n",
    "            \n",
    "            embeddings_list.append(combined_embedding)\n",
    "    \n",
    "    # Stack all embeddings\n",
    "    embeddings = np.stack(embeddings_list, axis=0)\n",
    "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the HF pairwise data from JSON files\n",
    "    print(\"Loading HF pairwise data from JSON files...\")\n",
    "    train_data = load_json_data(\"hf_evaluations_train.json\")\n",
    "    test_data = load_json_data(\"hf_evaluations_test.json\")\n",
    "    \n",
    "    print(f\"Loaded {len(train_data)} train samples\")\n",
    "    print(f\"Loaded {len(test_data)} test samples\")\n",
    "    \n",
    "    # Load CLIP client\n",
    "    clip_client = load_embedding_model()\n",
    "    \n",
    "    # Generate embeddings for train set\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Generating TRAIN embeddings...\")\n",
    "    print(\"=\"*80)\n",
    "    train_embeddings = generate_embeddings_for_dataset(clip_client, train_data, max_samples=None)\n",
    "    \n",
    "    # Generate embeddings for test set\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Generating TEST embeddings...\")\n",
    "    print(\"=\"*80)\n",
    "    test_embeddings = generate_embeddings_for_dataset(clip_client, test_data, max_samples=None)\n",
    "    \n",
    "    # Save train and test embeddings separately\n",
    "    train_output_path = \"hf_train_embeddings.npy\"\n",
    "    test_output_path = \"hf_test_embeddings.npy\"\n",
    "    \n",
    "    np.save(train_output_path, train_embeddings)\n",
    "    np.save(test_output_path, test_embeddings)\n",
    "    \n",
    "    print(f\"\\n✓ Saved train embeddings to: {train_output_path}\")\n",
    "    print(f\"  Shape: {train_embeddings.shape}\")\n",
    "    print(f\"✓ Saved test embeddings to: {test_output_path}\")\n",
    "    print(f\"  Shape: {test_embeddings.shape}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"train_samples\": len(train_embeddings),\n",
    "        \"test_samples\": len(test_embeddings),\n",
    "        \"embedding_dim\": train_embeddings.shape[1],\n",
    "        \"model\": \"CLIP (grpc://10.185.119.147:51000)\",\n",
    "        \"text_dim\": CLIP_TEXT_DIM,\n",
    "        \"image_dim\": CLIP_IMAGE_DIM,\n",
    "        \"combined_dim\": COMBINED_DIM,\n",
    "        \"embedding_structure\": \"text_embedding(512) | image_embedding_or_zeros(512)\"\n",
    "    }\n",
    "    \n",
    "    with open(\"hf_embeddings_metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved metadata to: hf_embeddings_metadata.json\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Embeddings generated successfully!\")\n",
    "    print(\"You can now use these embeddings for training the matrix factorization model.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b853176",
   "metadata": {},
   "source": [
    "## ``nn_router.py``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2289abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn_router.py\n",
    "\n",
    "Uses pre-computed embeddings from HuggingFace evaluations to train a neural network router.\n",
    "\n",
    "Expects:\n",
    "- hf_evaluations_train.json / hf_evaluations_test.json with model_scores\n",
    "- hf_train_embeddings.npy / hf_test_embeddings.npy with pre-computed embeddings\n",
    "\n",
    "Features:\n",
    "- Multi-layer neural network with batch normalization and dropout\n",
    "- Hyperparameter tuning with random search\n",
    "- Class weighting to handle data imbalance (e.g., few positive examples for Llama model)\n",
    "- Early stopping to prevent overfitting\n",
    "- GPU support\n",
    "\n",
    "Installs:\n",
    "pip install torch scikit-learn joblib\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "TRAIN_JSON = \"hf_evaluations_train2.json\"\n",
    "TEST_JSON = \"hf_evaluations_test2.json\"\n",
    "TRAIN_EMBEDDINGS = \"hf_train_embeddings.npy\"\n",
    "TEST_EMBEDDINGS = \"hf_test_embeddings.npy\"\n",
    "RANDOM_STATE = 42\n",
    "TUNE_HYPERPARAMETERS = True  # Set to False to skip tuning and use default params\n",
    "USE_CLASS_WEIGHTS = True  # Set to False to disable class weighting for imbalanced data\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "# Check for GPU availability\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Default hyperparameters\n",
    "default_config = {\n",
    "    'hidden_dims': [512, 256, 128],  # Hidden layer dimensions\n",
    "    'dropout': 0.3,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 50,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 10,  # Early stopping patience\n",
    "}\n",
    "\n",
    "# Hyperparameter search space for tuning\n",
    "param_grid = {\n",
    "    'hidden_dims': [\n",
    "        [256, 128],\n",
    "        [512, 256],\n",
    "        [512, 256, 128],\n",
    "        [1024, 512, 256],\n",
    "        [256, 128, 64],\n",
    "    ],\n",
    "    'dropout': [0.2, 0.3, 0.4, 0.5],\n",
    "    'learning_rate': [0.0001, 0.0005, 0.001, 0.002],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'weight_decay': [0, 1e-6, 1e-5, 1e-4],\n",
    "}\n",
    "\n",
    "# ---------- Neural Network Architecture ----------\n",
    "class RouterNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-output neural network for routing.\n",
    "    Predicts probability that each model will be correct for a given input.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[512, 256, 128], dropout=0.3):\n",
    "        super(RouterNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (one sigmoid output per model)\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        # Apply sigmoid to get probabilities for each model independently\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def load_data(json_path: str, embeddings_path: str, selected_models=None):\n",
    "    \"\"\"\n",
    "    Load data from JSON file and corresponding embeddings.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON file with model_scores\n",
    "        embeddings_path: Path to embeddings .npy file\n",
    "        selected_models: Optional list of model names to keep. If None, keeps all models.\n",
    "    \n",
    "    Returns:\n",
    "        embeddings (X), labels (Y), model_names, and prompts.\n",
    "    \"\"\"\n",
    "    # Load JSON\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Load embeddings\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    \n",
    "    # Verify alignment\n",
    "    if len(data) != len(embeddings):\n",
    "        raise ValueError(f\"Mismatch: {len(data)} JSON records vs {len(embeddings)} embeddings\")\n",
    "    \n",
    "    # Extract labels from model_scores\n",
    "    if not data:\n",
    "        raise ValueError(\"Empty JSON file\")\n",
    "    \n",
    "    all_model_names = list(data[0]['model_scores'].keys())\n",
    "    print(f\"Found models in data: {all_model_names}\")\n",
    "    \n",
    "    # Filter to selected models if specified\n",
    "    if selected_models is not None:\n",
    "        model_names = [m for m in all_model_names if m in selected_models]\n",
    "        if len(model_names) != len(selected_models):\n",
    "            missing = set(selected_models) - set(model_names)\n",
    "            print(f\"Warning: The following selected models were not found in data: {missing}\")\n",
    "        print(f\"Using selected models: {model_names}\")\n",
    "    else:\n",
    "        model_names = all_model_names\n",
    "    \n",
    "    # Build label matrix\n",
    "    labels = []\n",
    "    prompts = []\n",
    "    for record in data:\n",
    "        prompts.append(record['prompt'])\n",
    "        label_row = [record['model_scores'].get(model, 0) for model in model_names]\n",
    "        labels.append(label_row)\n",
    "    \n",
    "    Y = np.array(labels, dtype=np.float32)\n",
    "    X = embeddings.astype(np.float32)\n",
    "    \n",
    "    return X, Y, model_names, prompts\n",
    "\n",
    "def compute_class_weights(y_train, model_names, method='inverse'):\n",
    "    \"\"\"\n",
    "    Compute class weights to handle imbalanced data.\n",
    "    \n",
    "    Args:\n",
    "        y_train: Training labels (n_samples, n_models)\n",
    "        model_names: List of model names\n",
    "        method: 'inverse' or 'balanced'\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_models,) with weights for each model\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    pos_rates = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Computing class weights to handle data imbalance:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        pos_rate = y_train[:, i].mean()\n",
    "        pos_rates.append(pos_rate)\n",
    "        \n",
    "        if method == 'inverse':\n",
    "            # Inverse frequency: weight = 1 / pos_rate\n",
    "            # Higher weight for rarer classes\n",
    "            if pos_rate > 0:\n",
    "                weight = 1.0 / pos_rate\n",
    "            else:\n",
    "                weight = 1.0\n",
    "        elif method == 'balanced':\n",
    "            # Balanced: weight = n_samples / (n_classes * n_samples_for_class)\n",
    "            n_pos = y_train[:, i].sum()\n",
    "            if n_pos > 0:\n",
    "                weight = len(y_train) / (2 * n_pos)\n",
    "            else:\n",
    "                weight = 1.0\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        \n",
    "        weights.append(weight)\n",
    "        print(f\"  {model_name:40s}: pos_rate={pos_rate:.4f}, weight={weight:.4f}\")\n",
    "    \n",
    "    # Normalize weights so they sum to number of models (keeps loss scale similar)\n",
    "    weights = np.array(weights)\n",
    "    weights = weights * len(weights) / weights.sum()\n",
    "    \n",
    "    print(f\"\\nNormalized weights (sum={weights.sum():.2f}):\")\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        print(f\"  {model_name:40s}: {weights[i]:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config, model_names, class_weights=None):\n",
    "    \"\"\"\n",
    "    Train the neural network with early stopping.\n",
    "    Supports class weights to handle data imbalance.\n",
    "    \"\"\"\n",
    "    # Use weighted BCE loss if class weights are provided\n",
    "    if class_weights is not None:\n",
    "        # BCELoss doesn't support weights directly, so we'll use a weighted version\n",
    "        def weighted_bce_loss(outputs, targets):\n",
    "            # Compute BCE loss element-wise\n",
    "            bce = -(targets * torch.log(outputs + 1e-8) + (1 - targets) * torch.log(1 - outputs + 1e-8))\n",
    "            # Apply class weights\n",
    "            weighted = bce * class_weights.to(outputs.device)\n",
    "            return weighted.mean()\n",
    "        criterion = weighted_bce_loss\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Print progress every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"    Epoch {epoch+1}/{config['epochs']}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config['patience']:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, best_val_loss\n",
    "\n",
    "def evaluate_router(model, X_test, y_test, model_names, model_thresholds=None):\n",
    "    \"\"\"\n",
    "    Evaluate the router on test set.\n",
    "    \n",
    "    Args:\n",
    "        model: trained RouterNetwork\n",
    "        X_test: test embeddings\n",
    "        y_test: test labels\n",
    "        model_names: list of model names\n",
    "        model_thresholds: optional dict of model name -> threshold for selection\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test).to(DEVICE)\n",
    "        proba_test = model(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "    # Evaluate per-model metrics\n",
    "    metrics = {}\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        y_true = y_test[:, i]\n",
    "        y_score = proba_test[:, i]\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "        except ValueError:\n",
    "            auc = float(\"nan\")\n",
    "        preds = (y_score >= 0.5).astype(int)\n",
    "        acc = accuracy_score(y_true, preds)\n",
    "        f1 = f1_score(y_true, preds, zero_division=0)\n",
    "        metrics[model_name] = {\n",
    "            \"auc\": auc, \n",
    "            \"accuracy\": acc, \n",
    "            \"f1\": f1, \n",
    "            \"positive_rate\": y_true.mean()\n",
    "        }\n",
    "    \n",
    "    # Router evaluation (system accuracy)\n",
    "    chosen_idx = np.argmax(proba_test, axis=1)\n",
    "    \n",
    "    # Apply model thresholds if specified\n",
    "    if model_thresholds is not None and len(model_thresholds) > 0:\n",
    "        # Get indices for models with thresholds\n",
    "        threshold_map = {}\n",
    "        for model_name, threshold in model_thresholds.items():\n",
    "            if model_name in model_names:\n",
    "                model_idx = model_names.index(model_name)\n",
    "                threshold_map[model_idx] = threshold\n",
    "        \n",
    "        # Override choices that don't meet thresholds\n",
    "        for i in range(len(chosen_idx)):\n",
    "            chosen_model_idx = chosen_idx[i]\n",
    "            \n",
    "            # Check if chosen model has a threshold and doesn't meet it\n",
    "            if chosen_model_idx in threshold_map:\n",
    "                threshold = threshold_map[chosen_model_idx]\n",
    "                if proba_test[i, chosen_model_idx] < threshold:\n",
    "                    # Find next best model that either has no threshold or meets its threshold\n",
    "                    proba_copy = proba_test[i].copy()\n",
    "                    \n",
    "                    # Try models in order of probability until we find one that meets threshold\n",
    "                    sorted_indices = np.argsort(proba_copy)[::-1]  # Descending order\n",
    "                    \n",
    "                    for candidate_idx in sorted_indices:\n",
    "                        # Skip the originally chosen model (already failed threshold)\n",
    "                        if candidate_idx == chosen_model_idx:\n",
    "                            continue\n",
    "                        \n",
    "                        # Check if candidate has a threshold\n",
    "                        if candidate_idx in threshold_map:\n",
    "                            candidate_threshold = threshold_map[candidate_idx]\n",
    "                            if proba_copy[candidate_idx] >= candidate_threshold:\n",
    "                                chosen_idx[i] = candidate_idx\n",
    "                                break\n",
    "                        else:\n",
    "                            # No threshold for this model, so it's acceptable\n",
    "                            chosen_idx[i] = candidate_idx\n",
    "                            break\n",
    "    \n",
    "    chosen_label = np.array([y_test[i, chosen_idx[i]] for i in range(len(chosen_idx))])\n",
    "    system_accuracy = chosen_label.mean()\n",
    "    \n",
    "    # Model selection counts\n",
    "    model_selection_counts = {}\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        count = np.sum(chosen_idx == i)\n",
    "        model_selection_counts[model_name] = {\n",
    "            'count': int(count),\n",
    "            'percentage': count / len(chosen_idx) * 100,\n",
    "            'accuracy_when_chosen': chosen_label[chosen_idx == i].mean() if count > 0 else 0.0\n",
    "        }\n",
    "    \n",
    "    # Oracle accuracy\n",
    "    any_correct = (y_test.sum(axis=1) >= 1).mean()\n",
    "    \n",
    "    # Baseline accuracies\n",
    "    always_acc = {}\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        always_acc[model_name] = y_test[:, i].mean()\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'system_accuracy': system_accuracy,\n",
    "        'model_selection_counts': model_selection_counts,\n",
    "        'any_correct': any_correct,\n",
    "        'always_acc': always_acc,\n",
    "        'proba_test': proba_test\n",
    "    }\n",
    "\n",
    "def test_model_thresholds(model, X_test, y_test, model_names, \n",
    "                          threshold_configs=None):\n",
    "    \"\"\"\n",
    "    Test different threshold configurations and show their impact on model selection and accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: trained RouterNetwork\n",
    "        X_test: test embeddings\n",
    "        y_test: test labels\n",
    "        model_names: list of model names\n",
    "        threshold_configs: list of dicts with threshold configurations to test\n",
    "                          Example: [\n",
    "                              {},  # No threshold\n",
    "                              {'gpt-5-chat': 0.65},\n",
    "                              {'gpt-5-chat': 0.65, 'nemotron-nano-12b-v2-vl': 0.55},\n",
    "                          ]\n",
    "                          If None, uses default configurations\n",
    "    \"\"\"\n",
    "    if threshold_configs is None:\n",
    "        # Default configurations: test GPT-5 thresholds alone, then with Qwen-VL\n",
    "        threshold_configs = [\n",
    "            {},  # No threshold\n",
    "            {'gpt-5-chat': 0.60},\n",
    "            {'gpt-5-chat': 0.65},\n",
    "            {'gpt-5-chat': 0.70},\n",
    "            {'gpt-5-chat': 0.65, 'Qwen/Qwen3-VL-8B-Instruct': 0.50},\n",
    "            {'gpt-5-chat': 0.65, 'Qwen/Qwen3-VL-8B-Instruct': 0.55},\n",
    "            {'gpt-5-chat': 0.70, 'Qwen/Qwen3-VL-8B-Instruct': 0.55},\n",
    "        ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"TESTING MODEL THRESHOLD CONFIGURATIONS\")\n",
    "    print(\"=\"*120)\n",
    "    print(\"Thresholds control which models are used based on confidence levels\")\n",
    "    print(\"Higher thresholds = more cost savings (redirect to cheaper models)\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in threshold_configs:\n",
    "        # Evaluate with this threshold configuration\n",
    "        eval_results = evaluate_router(model, X_test, y_test, model_names, model_thresholds=config if config else None)\n",
    "        \n",
    "        # Extract key metrics\n",
    "        gpt5_selection = eval_results['model_selection_counts'].get('gpt-5-chat', {})\n",
    "        qwen_selection = eval_results['model_selection_counts'].get('Qwen/Qwen3-VL-8B-Instruct', {})\n",
    "        nano_text_selection = eval_results['model_selection_counts'].get('nvidia/nvidia-nemotron-nano-9b-v2', {})\n",
    "        \n",
    "        # Format threshold description\n",
    "        if not config:\n",
    "            threshold_desc = \"No thresholds\"\n",
    "        else:\n",
    "            parts = []\n",
    "            if 'gpt-5-chat' in config:\n",
    "                parts.append(f\"GPT5={config['gpt-5-chat']:.2f}\")\n",
    "            if 'Qwen/Qwen3-VL-8B-Instruct' in config:\n",
    "                parts.append(f\"Qwen={config['Qwen/Qwen3-VL-8B-Instruct']:.2f}\")\n",
    "            threshold_desc = \", \".join(parts)\n",
    "        \n",
    "        results.append({\n",
    "            'config': config,\n",
    "            'desc': threshold_desc,\n",
    "            'accuracy': eval_results['system_accuracy'],\n",
    "            'gpt5_pct': gpt5_selection.get('percentage', 0),\n",
    "            'qwen_pct': qwen_selection.get('percentage', 0),\n",
    "            'nano_text_pct': nano_text_selection.get('percentage', 0),\n",
    "        })\n",
    "    \n",
    "    # Print results table\n",
    "    print(f\"\\n{'Configuration':>30} {'Accuracy':>10} {'GPT-5 %':>10} {'Qwen-VL %':>12} {'Nano-Text %':>12} {'Cost Savings':>15}\")\n",
    "    print(\"-\" * 125)\n",
    "    \n",
    "    baseline_gpt5_pct = results[0]['gpt5_pct']\n",
    "    baseline_qwen_pct = results[0]['qwen_pct']\n",
    "    \n",
    "    for r in results:\n",
    "        # Calculate cost savings (assuming GPT-5 is most expensive, then Qwen-VL, then Nano-Text)\n",
    "        # Use relative costs: GPT-5 = 1.0, Qwen-VL = 0.4, Nano-Text = 0.1 (example ratios)\n",
    "        baseline_cost = baseline_gpt5_pct * 1.0 + baseline_qwen_pct * 0.4 + (100 - baseline_gpt5_pct - baseline_qwen_pct) * 0.1\n",
    "        current_cost = r['gpt5_pct'] * 1.0 + r['qwen_pct'] * 0.4 + r['nano_text_pct'] * 0.1\n",
    "        cost_savings = ((baseline_cost - current_cost) / baseline_cost * 100) if baseline_cost > 0 else 0\n",
    "        \n",
    "        savings_str = f\"-{cost_savings:.1f}%\" if cost_savings > 0 else \"baseline\"\n",
    "        \n",
    "        print(f\"{r['desc']:>30} {r['accuracy']:>10.3f} {r['gpt5_pct']:>9.1f}% \"\n",
    "              f\"{r['qwen_pct']:>11.1f}% {r['nano_text_pct']:>11.1f}% {savings_str:>15}\")\n",
    "    \n",
    "    print(\"=\"*125)\n",
    "    print(\"\\nRecommended Configurations:\")\n",
    "    print(\"  1. GPT-5 only (0.65): Moderate savings, redirects expensive model when unsure\")\n",
    "    print(\"  2. GPT-5 (0.65) + Qwen-VL (0.55): Balanced savings, redirects both premium models ← RECOMMENDED\")\n",
    "    print(\"  3. GPT-5 (0.70) + Qwen-VL (0.55): Aggressive savings, maximum cost reduction\")\n",
    "    print(\"\\nNote: Cost savings assume relative costs of GPT-5:Qwen-VL:Nano-Text = 10:4:1\")\n",
    "    print(\"=\"*125)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def tune_hyperparameters(X_train, y_train, X_val, y_val, model_names, class_weights=None, n_trials=10):\n",
    "    \"\"\"\n",
    "    Perform random search over hyperparameters.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPERPARAMETER TUNING (Neural Network)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Running {n_trials} random search trials...\")\n",
    "    \n",
    "    if class_weights is not None:\n",
    "        print(\"Using class weights to handle data imbalance\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_config = None\n",
    "    best_model = None\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = y_train.shape[1]\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Sample random hyperparameters\n",
    "        config = {\n",
    "            'hidden_dims': random.choice(param_grid['hidden_dims']),  # Use random.choice for lists\n",
    "            'dropout': float(np.random.choice(param_grid['dropout'])),\n",
    "            'learning_rate': float(np.random.choice(param_grid['learning_rate'])),\n",
    "            'batch_size': int(np.random.choice(param_grid['batch_size'])),\n",
    "            'weight_decay': float(np.random.choice(param_grid['weight_decay'])),\n",
    "            'epochs': default_config['epochs'],\n",
    "            'patience': default_config['patience'],\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTrial {trial+1}/{n_trials}\")\n",
    "        print(f\"  Config: hidden={config['hidden_dims']}, dropout={config['dropout']:.2f}, \"\n",
    "              f\"lr={config['learning_rate']:.4f}, bs={config['batch_size']}, wd={config['weight_decay']:.6f}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = RouterNetwork(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            hidden_dims=config['hidden_dims'],\n",
    "            dropout=config['dropout']\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_train),\n",
    "            torch.FloatTensor(y_train)\n",
    "        )\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_val),\n",
    "            torch.FloatTensor(y_val)\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model, val_loss = train_model(model, train_loader, val_loader, config, model_names, class_weights)\n",
    "        \n",
    "        print(f\"  Final validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Update best config\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_config = config.copy()\n",
    "            best_model = model\n",
    "            print(f\"  *** New best model! ***\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(\"=\"*80)\n",
    "    for key, value in best_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return best_model, best_config\n",
    "\n",
    "# ---------- Main training pipeline ----------\n",
    "def train_router():\n",
    "    \"\"\"Train router using pre-computed embeddings and labels.\"\"\"\n",
    "    # Define models to train on\n",
    "    selected_models = [\n",
    "        'gpt-5-chat',\n",
    "        'nvidia/nvidia-nemotron-nano-9b-v2',\n",
    "        'Qwen/Qwen3-VL-8B-Instruct'\n",
    "    ]\n",
    "    \n",
    "    # Load training data (filtered to selected models only)\n",
    "    X_train_full, y_train_full, model_names, train_prompts = load_data(\n",
    "        TRAIN_JSON, TRAIN_EMBEDDINGS, selected_models=selected_models\n",
    "    )\n",
    "    \n",
    "    # Load test data (filtered to selected models only)\n",
    "    X_test, y_test, _, test_prompts = load_data(\n",
    "        TEST_JSON, TEST_EMBEDDINGS, selected_models=selected_models\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train_full.shape[0]} samples, {X_train_full.shape[1]} features\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"Models: {model_names}\")\n",
    "    print(f\"Label distribution (train):\")\n",
    "    for i, model in enumerate(model_names):\n",
    "        pos_rate = y_train_full[:, i].mean()\n",
    "        print(f\"  {model}: {pos_rate:.3f} positive rate\")\n",
    "    \n",
    "    # Split training data into train/val for hyperparameter tuning\n",
    "    val_size = int(0.15 * len(X_train_full))\n",
    "    indices = np.random.permutation(len(X_train_full))\n",
    "    val_indices = indices[:val_size]\n",
    "    train_indices = indices[val_size:]\n",
    "    \n",
    "    X_train = X_train_full[train_indices]\n",
    "    y_train = y_train_full[train_indices]\n",
    "    X_val = X_train_full[val_indices]\n",
    "    y_val = y_train_full[val_indices]\n",
    "    \n",
    "    print(f\"\\nSplit: {len(X_train)} train, {len(X_val)} validation\")\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = y_train.shape[1]\n",
    "    \n",
    "    # Compute class weights to handle data imbalance (if enabled)\n",
    "    class_weights = None\n",
    "    if USE_CLASS_WEIGHTS:\n",
    "        class_weights = compute_class_weights(y_train, model_names, method='balanced')\n",
    "    else:\n",
    "        print(\"\\nClass weighting disabled (USE_CLASS_WEIGHTS=False)\")\n",
    "    \n",
    "    # Train model with or without hyperparameter tuning\n",
    "    if TUNE_HYPERPARAMETERS:\n",
    "        model, best_config = tune_hyperparameters(\n",
    "            X_train, y_train, X_val, y_val, model_names, class_weights, n_trials=10\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nTraining neural network with default parameters...\")\n",
    "        print(\"(Set TUNE_HYPERPARAMETERS=True to optimize hyperparameters)\")\n",
    "        \n",
    "        config = default_config.copy()\n",
    "        \n",
    "        model = RouterNetwork(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            hidden_dims=config['hidden_dims'],\n",
    "            dropout=config['dropout']\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_train),\n",
    "            torch.FloatTensor(y_train)\n",
    "        )\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_val),\n",
    "            torch.FloatTensor(y_val)\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\nTraining...\")\n",
    "        model, val_loss = train_model(model, train_loader, val_loader, config, model_names, class_weights)\n",
    "        best_config = config\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION ON TEST SET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = evaluate_router(model, X_test, y_test, model_names)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nPer-model metrics (on test set):\")\n",
    "    print(\"=\"*80)\n",
    "    for model_name, m in results['metrics'].items():\n",
    "        print(f\"  {model_name:40s}: AUC={m['auc']:.3f}, acc={m['accuracy']:.3f}, \"\n",
    "              f\"f1={m['f1']:.3f}, pos_rate={m['positive_rate']:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Baseline (always choose single model) accuracies (fraction correct):\")\n",
    "    print(\"=\"*80)\n",
    "    for model_name, acc in results['always_acc'].items():\n",
    "        print(f\"  {model_name:40s}: {acc:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Router model selection distribution on test set:\")\n",
    "    print(\"=\"*80)\n",
    "    for model_name, stats in results['model_selection_counts'].items():\n",
    "        print(f\"  {model_name:40s}: selected {stats['count']:4d} times \"\n",
    "              f\"({stats['percentage']:5.1f}%) - accuracy when chosen: {stats['accuracy_when_chosen']:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Router system accuracy (choose model with highest predicted prob): \"\n",
    "          f\"{results['system_accuracy']:.3f}\")\n",
    "    print(f\"Oracle (if you always picked any correct model when available): \"\n",
    "          f\"{results['any_correct']:.3f}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nNote: Oracle is an upper bound; a perfect router achieves that when it \"\n",
    "          \"picks a correct model whenever one exists.\")\n",
    "    \n",
    "    # Save model and config\n",
    "    out_dir = Path(\"router_artifacts\")\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': best_config,\n",
    "        'model_names': model_names,\n",
    "        'input_dim': input_dim,\n",
    "        'output_dim': output_dim,\n",
    "    }, out_dir / \"nn_router.pth\")\n",
    "    \n",
    "    print(f\"\\nSaved model to {out_dir.resolve()}/nn_router.pth\")\n",
    "    \n",
    "    return model, model_names, best_config, (X_test, y_test, results['proba_test'], test_prompts)\n",
    "\n",
    "# ---------- Inference utility ----------\n",
    "def load_router(model_path=\"router_artifacts/nn_router.pth\"):\n",
    "    \"\"\"Load the trained router model.\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=DEVICE)\n",
    "    \n",
    "    model = RouterNetwork(\n",
    "        input_dim=checkpoint['input_dim'],\n",
    "        output_dim=checkpoint['output_dim'],\n",
    "        hidden_dims=checkpoint['config']['hidden_dims'],\n",
    "        dropout=checkpoint['config']['dropout']\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint['model_names'], checkpoint['config']\n",
    "\n",
    "def route_embeddings(embeddings, model, model_names, return_probs=False, model_thresholds=None):\n",
    "    \"\"\"\n",
    "    Given embeddings, returns chosen model names and optionally probabilities.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of shape (n_samples, embedding_dim)\n",
    "        model: trained RouterNetwork\n",
    "        model_names: list of model names\n",
    "        return_probs: whether to return probability scores\n",
    "        model_thresholds: optional dict mapping model names to minimum confidence thresholds.\n",
    "                         If a model is chosen but prob < threshold, route to next best model.\n",
    "                         Example: {'gpt-5-chat': 0.65, 'Qwen/Qwen3-VL-8B-Instruct': 0.55}\n",
    "                         Suggested values: 0.60 (moderate), 0.65 (significant), 0.70+ (aggressive)\n",
    "    \n",
    "    Returns:\n",
    "        chosen_models: list of chosen model names\n",
    "        proba (optional): probability scores for each model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(embeddings).to(DEVICE)\n",
    "        proba = model(X_tensor).cpu().numpy()\n",
    "    \n",
    "    # Choose model with highest probability\n",
    "    chosen_idx = np.argmax(proba, axis=1)\n",
    "    \n",
    "    # Apply model thresholds if specified\n",
    "    if model_thresholds is not None and len(model_thresholds) > 0:\n",
    "        # Get indices for models with thresholds\n",
    "        threshold_map = {}\n",
    "        for model_name, threshold in model_thresholds.items():\n",
    "            if model_name in model_names:\n",
    "                model_idx = model_names.index(model_name)\n",
    "                threshold_map[model_idx] = threshold\n",
    "        \n",
    "        # Override choices that don't meet thresholds\n",
    "        for i in range(len(chosen_idx)):\n",
    "            chosen_model_idx = chosen_idx[i]\n",
    "            \n",
    "            # Check if chosen model has a threshold and doesn't meet it\n",
    "            if chosen_model_idx in threshold_map:\n",
    "                threshold = threshold_map[chosen_model_idx]\n",
    "                if proba[i, chosen_model_idx] < threshold:\n",
    "                    # Find next best model that either has no threshold or meets its threshold\n",
    "                    proba_copy = proba[i].copy()\n",
    "                    \n",
    "                    # Try models in order of probability until we find one that meets threshold\n",
    "                    sorted_indices = np.argsort(proba_copy)[::-1]  # Descending order\n",
    "                    \n",
    "                    for candidate_idx in sorted_indices:\n",
    "                        # Skip the originally chosen model (already failed threshold)\n",
    "                        if candidate_idx == chosen_model_idx:\n",
    "                            continue\n",
    "                        \n",
    "                        # Check if candidate has a threshold\n",
    "                        if candidate_idx in threshold_map:\n",
    "                            candidate_threshold = threshold_map[candidate_idx]\n",
    "                            if proba_copy[candidate_idx] >= candidate_threshold:\n",
    "                                chosen_idx[i] = candidate_idx\n",
    "                                break\n",
    "                        else:\n",
    "                            # No threshold for this model, so it's acceptable\n",
    "                            chosen_idx[i] = candidate_idx\n",
    "                            break\n",
    "    \n",
    "    chosen_models = [model_names[i] for i in chosen_idx]\n",
    "    if return_probs:\n",
    "        return chosen_models, proba\n",
    "    return chosen_models\n",
    "\n",
    "# ---------- Example usage ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Train\n",
    "    model, model_names, config, test_info = train_router()\n",
    "    X_test, y_test, proba_test, test_prompts = test_info\n",
    "    \n",
    "    # Test different threshold configurations (GPT-5 and Nemotron)\n",
    "    test_model_thresholds(model, X_test, y_test, model_names)\n",
    "    \n",
    "    # Show some example routing decisions on test set (with different threshold configs)\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Sample routing decisions on test set (NO THRESHOLD):\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    chosen, probs = route_embeddings(X_test[:10], model, model_names, return_probs=True)\n",
    "    for i, (prompt, choice, prob) in enumerate(zip(test_prompts[:10], chosen, probs)):\n",
    "        print(f\"\\n{i+1}. PROMPT: {prompt[:80]}...\")\n",
    "        print(f\"   ROUTED TO: {choice}\")\n",
    "        prob_dict = {model_names[j]: f\"{float(prob[j]):.3f}\" for j in range(len(model_names))}\n",
    "        print(f\"   PROBS: {prob_dict}\")\n",
    "        # Show which models were actually correct\n",
    "        correct_models = [model_names[j] for j in range(len(model_names)) if y_test[i, j] == 1]\n",
    "        print(f\"   CORRECT: {correct_models}\")\n",
    "    \n",
    "    # Show routing with GPT-5 threshold only\n",
    "    print(\"\\n\" + \"=\"*105)\n",
    "    print(\"Sample routing decisions (WITH GPT-5 THRESHOLD=0.65):\")\n",
    "    print(\"=\"*105)\n",
    "    \n",
    "    chosen_gpt5, probs_gpt5 = route_embeddings(\n",
    "        X_test[:10], model, model_names, return_probs=True, \n",
    "        model_thresholds={'gpt-5-chat': 0.65}\n",
    "    )\n",
    "    for i, (prompt, choice, prob) in enumerate(zip(test_prompts[:10], chosen_gpt5, probs_gpt5)):\n",
    "        print(f\"\\n{i+1}. PROMPT: {prompt[:80]}...\")\n",
    "        print(f\"   ROUTED TO: {choice}\")\n",
    "        prob_dict = {model_names[j]: f\"{float(prob[j]):.3f}\" for j in range(len(model_names))}\n",
    "        print(f\"   PROBS: {prob_dict}\")\n",
    "        correct_models = [model_names[j] for j in range(len(model_names)) if y_test[i, j] == 1]\n",
    "        print(f\"   CORRECT: {correct_models}\")\n",
    "        if chosen[i] != choice:\n",
    "            print(f\"   ⚠️  ROUTING CHANGED: {chosen[i]} → {choice} (due to GPT-5 threshold)\")\n",
    "    \n",
    "    # Show routing with both GPT-5 and Qwen-VL thresholds\n",
    "    print(\"\\n\" + \"=\"*105)\n",
    "    print(\"Sample routing decisions (WITH GPT-5=0.65 & QWEN-VL=0.55 THRESHOLDS):\")\n",
    "    print(\"=\"*105)\n",
    "    \n",
    "    chosen_both, probs_both = route_embeddings(\n",
    "        X_test[:10], model, model_names, return_probs=True,\n",
    "        model_thresholds={'gpt-5-chat': 0.65, 'Qwen/Qwen3-VL-8B-Instruct': 0.55}\n",
    "    )\n",
    "    for i, (prompt, choice, prob) in enumerate(zip(test_prompts[:10], chosen_both, probs_both)):\n",
    "        print(f\"\\n{i+1}. PROMPT: {prompt[:80]}...\")\n",
    "        print(f\"   ROUTED TO: {choice}\")\n",
    "        prob_dict = {model_names[j]: f\"{float(prob[j]):.3f}\" for j in range(len(model_names))}\n",
    "        print(f\"   PROBS: {prob_dict}\")\n",
    "        correct_models = [model_names[j] for j in range(len(model_names)) if y_test[i, j] == 1]\n",
    "        print(f\"   CORRECT: {correct_models}\")\n",
    "        if chosen[i] != choice:\n",
    "            print(f\"   ⚠️  ROUTING CHANGED: {chosen[i]} → {choice} (due to thresholds)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
