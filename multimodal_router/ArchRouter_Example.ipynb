{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f98ccf",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bd453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d80ea7",
   "metadata": {},
   "source": [
    "## ``run_router.sh``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy with docker on Linux:\n",
    "!docker run -d  --rm --runtime nvidia --gpus \"device=1\" \\\n",
    "\t--name arch_router \\\n",
    "\t-v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "\t-p 8011:8000 \\\n",
    "\t--ipc=host \\\n",
    "\tvllm/vllm-openai:latest \\\n",
    "\t--model katanemo/Arch-Router-1.5B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63013cf",
   "metadata": {},
   "source": [
    "## ``hf_intent_objective_fn.py``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3916d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List\n",
    "from transformers import AutoTokenizer\n",
    "import logging\n",
    "from typing import Tuple, List, Optional\n",
    "import asyncio\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from pydantic import Field\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Remote model configuration\n",
    "REMOTE_MODEL_URL = \"http://10.185.119.221:8011\"\n",
    "MODEL_NAME = \"katanemo/Arch-Router-1.5B\"\n",
    "\n",
    "# Only need tokenizer for prompt encoding (model is remote)\n",
    "tokenizer = None\n",
    "\n",
    "def _load_tokenizer():\n",
    "    \"\"\"Lazy load the tokenizer on first use.\"\"\"\n",
    "    global tokenizer\n",
    "    if tokenizer is None:\n",
    "        logger.info(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "            logger.info(\"Tokenizer loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load tokenizer: {e}\")\n",
    "            raise\n",
    "    return tokenizer\n",
    "\n",
    "def _check_remote_model():\n",
    "    \"\"\"Check if remote model is available.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{REMOTE_MODEL_URL}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            logger.info(f\"Remote model at {REMOTE_MODEL_URL} is available\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Remote model at {REMOTE_MODEL_URL} is not available: {e}\")\n",
    "        return False\n",
    "\n",
    "# Please use our provided prompt for best performance\n",
    "TASK_INSTRUCTION = \"\"\"\n",
    "You are a helpful assistant designed to find the best suited route.\n",
    "You are provided with route description within <routes></routes> XML tags:\n",
    "<routes>\n",
    "\n",
    "{routes}\n",
    "\n",
    "</routes>\n",
    "\n",
    "<conversation>\n",
    "\n",
    "{conversation}\n",
    "\n",
    "</conversation>\n",
    "\"\"\"\n",
    "\n",
    "FORMAT_PROMPT = \"\"\"\n",
    "Your task is to decide which route is best suit with user intent on the conversation in <conversation></conversation> XML tags.  Follow the instruction:\n",
    "1. If the latest intent from user is irrelevant or user intent is full filled, response with other route {\"route\": \"other\"}.\n",
    "2. You must analyze the route descriptions and find the best match route for user latest intent. \n",
    "3. You only response the name of the route that best matches the user's request, use the exact name in the <routes></routes>.\n",
    "\n",
    "Based on your analysis, provide your response in the following JSON formats if you decide to match any route:\n",
    "{\"route\": \"route_name\"} \n",
    "\"\"\"\n",
    "\n",
    "# Custom JSON encoder for Pydantic models and non-serializable objects\n",
    "class PydanticEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        # Handle Pydantic models\n",
    "        if hasattr(obj, 'model_dump'):\n",
    "            return obj.model_dump()\n",
    "        # Handle dict-like objects\n",
    "        if hasattr(obj, '__dict__'):\n",
    "            return obj.__dict__\n",
    "        # Handle iterables (except strings)\n",
    "        if hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes)):\n",
    "            try:\n",
    "                return list(obj)\n",
    "            except TypeError:\n",
    "                pass\n",
    "        return super().default(obj)\n",
    "\n",
    "# Define route config\n",
    "route_config = [\n",
    "    {\n",
    "        \"name\": \"hard_question\",\n",
    "        \"description\": \"A question that requires deep reasoning, or complex problem solving, or if the user asks for careful thinking or careful consideration\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"chit_chat\",\n",
    "        \"description\": \"Any social chit chat, small talk, or casual conversation.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"try_again\",\n",
    "        \"description\": \"Only if the user explicitly says the previous answer was incorrect or incomplete.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"image_understanding\",\n",
    "        \"description\": \"A question that requires understanding an image.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"image_question\",\n",
    "        \"description\": \"A question that requires the assistant to see the user eg a question about their appearance, environment, scene or surroundings.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Pre-compute routes JSON once to avoid repeated serialization\n",
    "_ROUTES_JSON_CACHED = json.dumps(route_config, cls=PydanticEncoder)\n",
    "\n",
    "MAP_INTENT_TO_PIPELINE = {\n",
    "    \"other\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
    "    \"chit_chat\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
    "    \"hard_question\": \"gpt-5-chat\",\n",
    "    \"image_understanding\": \"Qwen/Qwen3-VL-8B-Instruct\",\n",
    "    \"image_question\": \"Qwen/Qwen3-VL-8B-Instruct\",\n",
    "    \"try_again\": \"gpt-5-chat\",\n",
    "}\n",
    "\n",
    "# Helper function to redact images while preserving text context\n",
    "def redact_images_from_conversation(conversation: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Remove image data for router.\"\"\"\n",
    "    redacted = []\n",
    "    for i, msg in enumerate(conversation):\n",
    "        msg_copy = msg.copy()\n",
    "        content = msg_copy.get(\"content\")\n",
    "    \n",
    "        \n",
    "        # If content is a list (multimodal), process it\n",
    "        if isinstance(content, list):\n",
    "            text_parts = []\n",
    "            \n",
    "            for item in content:\n",
    "                logger.info(f\"  Item: {type(item)}, {item if not isinstance(item, dict) else list(item.keys())}\")\n",
    "                if isinstance(item, dict):\n",
    "                    if item.get(\"type\") == \"text\":\n",
    "                        item_text = item.get(\"text\", \"\")\n",
    "                        text = f\"<new msg>{item_text} </msg>\"\n",
    "                        text_parts.append(text)\n",
    "                    elif item.get(\"type\") == \"image_url\":\n",
    "                        continue\n",
    "                        \n",
    "            \n",
    "            # Combine text parts and add image indicator if present\n",
    "            combined_text = \" \".join(text_parts)\n",
    "            \n",
    "            msg_copy[\"content\"] = combined_text\n",
    "        \n",
    "        redacted.append(msg_copy)\n",
    "    \n",
    "    return redacted\n",
    "\n",
    "# Helper function to create the system prompt for our model\n",
    "def format_prompt(conversation: List[Dict[str, Any]]):\n",
    "    \"\"\"Create the system prompt - uses pre-computed routes JSON for efficiency.\"\"\"\n",
    "    return (\n",
    "        TASK_INSTRUCTION.format(\n",
    "            routes=_ROUTES_JSON_CACHED,  # Use pre-computed JSON\n",
    "            conversation=json.dumps(conversation, cls=PydanticEncoder)\n",
    "        )\n",
    "        + FORMAT_PROMPT\n",
    "    )\n",
    "\n",
    "# Cached JSON response parsing\n",
    "@lru_cache(maxsize=128)\n",
    "def _parse_route_response(response: str) -> str:\n",
    "    \"\"\"Parse and cache route responses to avoid repeated JSON parsing.\"\"\"\n",
    "    try:\n",
    "        return json.loads(response)[\"route\"]\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle single quote format\n",
    "        import ast\n",
    "        return ast.literal_eval(response)[\"route\"]\n",
    "\n",
    "\n",
    "\n",
    "class HFIntentObjectiveConfig(FunctionBaseConfig, name=\"hf_intent_objective_fn\"):\n",
    "    \"\"\"HF intent objective function for best route.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def materialize_iterator(obj):\n",
    "    \"\"\"Recursively convert ValidatorIterator and other iterables to lists.\"\"\"\n",
    "    if hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, dict)):\n",
    "        try:\n",
    "            return [materialize_iterator(item) for item in obj]\n",
    "        except TypeError:\n",
    "            pass\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: materialize_iterator(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "@register_function(config_type=HFIntentObjectiveConfig)\n",
    "async def hf_intent_objective_fn(config: HFIntentObjectiveConfig,\n",
    "                                 _builder: Builder):\n",
    "    \"\"\"HF intent objective function for best route.\"\"\"\n",
    "\n",
    "    from nat_sfc_router.schema.openai_chat_request import OpenAIChatRequest\n",
    "\n",
    "    # Check if remote model is available\n",
    "    _check_remote_model()\n",
    "    \n",
    "    # Load tokenizer (model is remote)\n",
    "    loaded_tokenizer = _load_tokenizer()\n",
    "\n",
    "    def get_route_from_conversation(conversation: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Determine the best route for the conversation (using remote model).\"\"\"\n",
    "        inference_start = time.perf_counter()\n",
    "        \n",
    "        # Redact images from messages because the router does not support them\n",
    "        # But it can still determine if the text intent requires image understanding\n",
    "        redacted_conversation = redact_images_from_conversation(conversation)\n",
    "        \n",
    "        # ===== FORMAT PROMPT =====\n",
    "        prompt_start = time.perf_counter()\n",
    "        route_prompt = format_prompt(redacted_conversation)\n",
    "        prompt_time = time.perf_counter() - prompt_start\n",
    "        \n",
    "        # ===== CONSTRUCT MESSAGES =====\n",
    "        construct_start = time.perf_counter()\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": route_prompt},\n",
    "        ]\n",
    "        construct_time = time.perf_counter() - construct_start\n",
    "\n",
    "        # ===== ENCODE (TOKENIZE) =====\n",
    "        # Not needed for remote API, but keeping for timing consistency\n",
    "        encode_start = time.perf_counter()\n",
    "        encode_time = time.perf_counter() - encode_start\n",
    "\n",
    "        # ===== GENERATION (REMOTE API CALL) =====\n",
    "        generation_start = time.perf_counter()\n",
    "        try:\n",
    "            # Call remote vLLM OpenAI-compatible API\n",
    "            response = requests.post(\n",
    "                f\"{REMOTE_MODEL_URL}/v1/chat/completions\",\n",
    "                json={\n",
    "                    \"model\": MODEL_NAME,\n",
    "                    \"messages\": messages,\n",
    "                    \"max_tokens\": 32,\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"top_p\": 0.9,\n",
    "                },\n",
    "                timeout=30,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            response_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to call remote model: {e}\")\n",
    "            raise\n",
    "        \n",
    "        generation_time = time.perf_counter() - generation_start\n",
    "\n",
    "        # ===== DECODING =====\n",
    "        decode_start = time.perf_counter()\n",
    "        # Response is already decoded text from remote API\n",
    "        decode_time = time.perf_counter() - decode_start\n",
    "        \n",
    "        # Use cached parser\n",
    "        route = _parse_route_response(response_text)\n",
    "        \n",
    "        total_time = time.perf_counter() - inference_start\n",
    "        \n",
    "        # Log timing breakdown\n",
    "        logger.info(\n",
    "            f\"Route inference timing breakdown | \"\n",
    "            f\"Format: {prompt_time*1000:.2f}ms | \"\n",
    "            f\"Construct: {construct_time*1000:.2f}ms | \"\n",
    "            f\"Encode: {encode_time*1000:.2f}ms | \"\n",
    "            f\"Generate: {generation_time*1000:.2f}ms | \"\n",
    "            f\"Decode: {decode_time*1000:.2f}ms | \"\n",
    "            f\"Total: {total_time*1000:.2f}ms\"\n",
    "        )\n",
    "        logger.debug(f\"Route: {route}, Response: {response_text[:100]}\")\n",
    "        \n",
    "        return route\n",
    "\n",
    "    async def _response_fn(chat_request: OpenAIChatRequest) -> Tuple[str, str]:  # pyright: ignore[reportUnusedParameter]\n",
    "        \"\"\"HF intent objective function for best route.\"\"\"\n",
    "        response_start = time.perf_counter()\n",
    "\n",
    "        # ===== EXTRACT MESSAGES =====\n",
    "        extract_start = time.perf_counter()\n",
    "        messages = chat_request.messages\n",
    "        extract_time = time.perf_counter() - extract_start\n",
    "\n",
    "        if messages:\n",
    "            # ===== CONVERT TO DICT =====\n",
    "            dict_convert_start = time.perf_counter()\n",
    "            last_msg = messages[-1]\n",
    "            last_msg_dict = last_msg.model_dump() if hasattr(last_msg, 'model_dump') else dict(last_msg)\n",
    "            dict_convert_time = time.perf_counter() - dict_convert_start\n",
    "\n",
    "            # ===== MATERIALIZE ITERATORS =====\n",
    "            materialize_start = time.perf_counter()\n",
    "            last_msg_dict = materialize_iterator(last_msg_dict)\n",
    "            materialize_time = time.perf_counter() - materialize_start\n",
    "\n",
    "            # Assign a list containing only the last message's dictionary\n",
    "            messages_dict = [last_msg_dict]\n",
    "            \n",
    "            logger.debug(\n",
    "                f\"Message preparation timing | \"\n",
    "                f\"Extract: {extract_time*1000:.2f}ms | \"\n",
    "                f\"Dict convert: {dict_convert_time*1000:.2f}ms | \"\n",
    "                f\"Materialize: {materialize_time*1000:.2f}ms\"\n",
    "            )\n",
    "        else:\n",
    "            # Handle the case where the list of messages is empty\n",
    "            messages_dict = []\n",
    "            logger.warning(\"No messages received in chat request\")\n",
    "\n",
    "        # Run model inference (blocking call in event loop)\n",
    "        user_intent = get_route_from_conversation(messages_dict)\n",
    "        \n",
    "        total_response_time = time.perf_counter() - response_start\n",
    "\n",
    "        logger.info(f\"User intent: {user_intent} (total response time: {total_response_time*1000:.2f}ms)\")\n",
    "        return MAP_INTENT_TO_PIPELINE[user_intent], \"\"\n",
    "    \n",
    "\n",
    "    yield FunctionInfo.from_fn(\n",
    "        _response_fn,\n",
    "        description=\"Demonstrative objective function for best model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
