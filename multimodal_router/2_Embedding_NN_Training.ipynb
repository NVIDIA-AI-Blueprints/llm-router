{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f98ccf",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ee220",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "python = sys.executable\n",
    "\n",
    "!{python} -m ensurepip --upgrade\n",
    "!{python} -m pip install --upgrade pip setuptools wheel\n",
    "!{python} -m pip install --upgrade --force-reinstall python-dotenv\n",
    "%pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f3b203",
   "metadata": {},
   "source": [
    "Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac99709",
   "metadata": {},
   "source": [
    "Clone the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c432d52-3ddb-475f-ad4d-faaaf1848ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA-AI-Blueprints/llm-router.git\n",
    "!cd llm-router && git checkout ces-dev # TODO: Remove once ces-dev is merged to main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup the repository script\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "\n",
    "# 2. Install Git LFS\n",
    "!sudo apt-get install git-lfs\n",
    "\n",
    "# 3. Initialize Git LFS\n",
    "!git lfs install\n",
    "\n",
    "!git lfs fetch\n",
    "!git lfs checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc844571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('llm-router')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a446d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d800a31",
   "metadata": {},
   "source": [
    "First, we set up the Python path and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7676818",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a4e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install -r requirements.txt -r multimodal_router/src/nat_sfc_router/training/requirements.txt\n",
    "!cd multimodal_router && uv pip install .\n",
    "%pip install clip-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bd453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "import time\n",
    "from functools import wraps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb607b54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## 2. [OPTIONAL] Data Preparation (`prepare_hf_data.py`)\n",
    "\n",
    "This section prepares training data for the multimodal router by:\n",
    "\n",
    "1. **Loading a multimodal dataset** from HuggingFace (images + text)\n",
    "2. **Generating responses** from multiple models (GPT-5, Qwen-VL, Nemotron)\n",
    "3. **Judging responses** using LLM-as-judge to determine correctness\n",
    "4. **Creating evaluation data** with model success/failure labels for training\n",
    "\n",
    "The output is JSON files containing prompts, images (base64), and per-model success indicators.\n",
    "\n",
    "**This section is OPTIONAL because the train and test datasets have already been pre-generated for you for your convenience. You can verify with the cell below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf7dab",
   "metadata": {},
   "source": [
    "### 2.1 Load Dataset\n",
    "\n",
    "Load the multimodal dataset containing images and text prompts with ground truth answers.\n",
    "\n",
    "> **⚠️ Important:** This section requires the `finevision_combined_images_text` dataset to be available locally. \n",
    "> \n",
    "> **If you don't have this dataset**, you can **skip Section 2 entirely** and proceed to Section 3 (Embedding Generation), as pre-computed evaluation files (`hf_evaluations_train.json` and `hf_evaluations_test.json`) may already exist in `src/nat_sfc_router/training/`.\n",
    ">\n",
    "> To check if you can skip this section, run:\n",
    "> ```python\n",
    "> import os\n",
    "> print(\"Can skip Section 2:\", os.path.exists(\"src/nat_sfc_router/training/hf_evaluations_train.json\"))\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can skip this section (pre-computed evaluations exist)\n",
    "eval_files_exist = (\n",
    "    os.path.exists(\"multimodal_router/src/nat_sfc_router/training/hf_evaluations_train.json\") and\n",
    "    os.path.exists(\"multimodal_router/src/nat_sfc_router/training/hf_evaluations_test.json\")\n",
    ")\n",
    "\n",
    "if eval_files_exist:\n",
    "    print(\"✓ Pre-computed evaluation files found!\")\n",
    "    print(\"  - src/nat_sfc_router/training/hf_evaluations_train.json\")\n",
    "    print(\"  - src/nat_sfc_router/training/hf_evaluations_test.json\")\n",
    "    print(\"\\n⚠️ You can SKIP Section 2 (cells 17-36) and proceed directly to Section 3 (Embedding Generation).\")\n",
    "    print(\"   Set SKIP_DATA_PREP = False above and re-run, or simply skip to Section 3.\")\n",
    "    \n",
    "    SKIP_DATA_PREP = True  # Set to False if you want to regenerate evaluations\n",
    "else:\n",
    "    SKIP_DATA_PREP = False\n",
    "\n",
    "if not SKIP_DATA_PREP:\n",
    "    # Dataset path - update this to point to your dataset location\n",
    "    DATASET_PATH = \"finevision_combined_images_text\"  # or full path like \"/path/to/dataset\"\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_from_disk(DATASET_PATH)\n",
    "    \n",
    "    print(f\"Dataset loaded with {len(dataset)} samples\")\n",
    "    print(f\"Sample fields: {dataset[0].keys() if len(dataset) > 0 else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5521dc",
   "metadata": {},
   "source": [
    "### 2.2 Configuration\n",
    "\n",
    "Define the models to route between and a utility decorator for handling API retries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac038ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IDs mapping for the router\n",
    "MODEL_IDS = {\n",
    "    \"gpt-5-chat\": 0,\n",
    "    \"Qwen/Qwen3-VL-8B-Instruct\": 1,\n",
    "    \"nvidia/nvidia-nemotron-nano-9b-v2\": 2\n",
    "}\n",
    "\n",
    "def retry_with_exponential_backoff(\n",
    "    max_retries=50,\n",
    "    initial_delay=1,\n",
    "    exponential_base=2,\n",
    "    max_delay=3000,\n",
    "    jitter=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Decorator to retry a function with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        initial_delay: Initial delay in seconds\n",
    "        exponential_base: Base for exponential backoff\n",
    "        max_delay: Maximum delay between retries\n",
    "        jitter: Add random jitter to prevent thundering herd\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            delay = initial_delay\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    \n",
    "                    if attempt == max_retries:\n",
    "                        print(f\"Max retries ({max_retries}) reached for {func.__name__}. Last error: {e}\")\n",
    "                        raise\n",
    "                    \n",
    "                    # Calculate delay with exponential backoff\n",
    "                    current_delay = min(delay * (exponential_base ** attempt), max_delay)\n",
    "                    \n",
    "                    # Add jitter if enabled\n",
    "                    if jitter:\n",
    "                        current_delay = current_delay * (0.5 + random.random())\n",
    "                    \n",
    "                    print(f\"Error in {func.__name__} (attempt {attempt + 1}/{max_retries + 1}): {e}\")\n",
    "                    print(f\"Retrying in {current_delay:.2f} seconds...\")\n",
    "                    time.sleep(current_delay)\n",
    "            \n",
    "            raise last_exception\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "print(f\"Configured {len(MODEL_IDS)} models for routing: {list(MODEL_IDS.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4278cf75",
   "metadata": {},
   "source": [
    "### 2.3 API Configuration\n",
    "\n",
    "Configure API endpoints for the different models. Set these environment variables:\n",
    "- `AZURE_OPENAI_ENDPOINT`: Azure OpenAI endpoint URL\n",
    "- `OPENAI_API_KEY`: Azure OpenAI API key\n",
    "- `VLM_MODEL_BASE_URL`: Base URL for the vision-language model (Qwen)\n",
    "- `NANO_TEXT_MODEL_BASE_URL`: Base URL for the text-only model (Nemotron Nano)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43028e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API configuration from environment variables\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verify configuration\n",
    "print(\"API Configuration Status:\")\n",
    "print(f\"  AZURE_OPENAI_ENDPOINT: {'✓ Set' if AZURE_OPENAI_ENDPOINT else '✗ Not set'}\")\n",
    "print(f\"  OPENAI_API_KEY: {'✓ Set' if AZURE_OPENAI_API_KEY else '✗ Not set'}\")\n",
    "print(f\"  VLM_MODEL_BASE_URL: {'✓ Set' if os.getenv('VLM_MODEL_BASE_URL') else '✗ Not set'}\")\n",
    "print(f\"  NANO_TEXT_MODEL_BASE_URL: {'✓ Set' if os.getenv('NANO_TEXT_MODEL_BASE_URL') else '✗ Not set'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f11fc",
   "metadata": {},
   "source": [
    "### 2.4 Helper Functions\n",
    "\n",
    "Utility function to extract prompts and images from dataset samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_with_images(sample):\n",
    "    \"\"\"\n",
    "    Prepare the prompt by extracting text content and collecting images from the sample.\n",
    "    \n",
    "    Args:\n",
    "        sample: Dataset sample with 'text', 'images', and 'answer' fields\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'prompt', 'images', and 'answer' keys\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"prompt\": sample[\"text\"][0][\"content\"],\n",
    "        \"images\": sample[\"images\"],\n",
    "        \"answer\": sample[\"answer\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ec75a",
   "metadata": {},
   "source": [
    "### 2.5 Model Response Generators\n",
    "\n",
    "Functions to generate responses from each model. These handle the different API formats and image encoding requirements for each model type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b85716",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry_with_exponential_backoff(max_retries=5, initial_delay=1)\n",
    "def generate_response_with_openai(prompt_data):\n",
    "    \"\"\"Generate response using Azure OpenAI (GPT-5-chat) with vision support.\"\"\"\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=\"2025-01-01-preview\"\n",
    "    )\n",
    "    \n",
    "    # Prepare content with text and images\n",
    "    content = [{\"type\": \"text\", \"text\": prompt_data['prompt']}]\n",
    "    \n",
    "    # Add images if available\n",
    "    for image in prompt_data.get('images', []):\n",
    "        if image is not None:\n",
    "            # Convert PIL Image to base64\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "            \n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-chat\",\n",
    "        messages=[{\"role\": \"user\", \"content\": content}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce202b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry_with_exponential_backoff(max_retries=5, initial_delay=1)\n",
    "def generate_response_with_nemotron_vlm(prompt_data):\n",
    "    \"\"\"Generate response using Qwen VLM (vision-language model).\"\"\"\n",
    "    client = OpenAI(\n",
    "        base_url=os.getenv(\"VLM_MODEL_BASE_URL\"),\n",
    "        api_key=\"not-needed\"\n",
    "    )\n",
    "    \n",
    "    content = [{\"type\": \"text\", \"text\": prompt_data['prompt']}]\n",
    "    \n",
    "    # Add images if available\n",
    "    for image in prompt_data.get('images', []):\n",
    "        if image is not None:\n",
    "            # Convert PIL Image to base64\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "            \n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": content}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_text(prompt_data):\n",
    "    \"\"\"Generate response using Nemotron Nano (text-only model).\"\"\"\n",
    "    client = OpenAI(\n",
    "        base_url=os.getenv(\"NANO_TEXT_MODEL_BASE_URL\"),\n",
    "        api_key=\"not-needed\"\n",
    "    )\n",
    "    \n",
    "    # Build messages from conversation\n",
    "    messages = []\n",
    "    conversation = prompt_data['prompt']\n",
    "    first_user_turn = True\n",
    "    \n",
    "    # Process each turn in the conversation\n",
    "    for turn in conversation:\n",
    "        role = turn.get(\"role\", \"user\")\n",
    "        content_text = turn.get(\"content\", \"\")\n",
    "        \n",
    "        # For the first user message, include images\n",
    "        if role == \"user\" and first_user_turn:\n",
    "            first_user_turn = False\n",
    "            content = [{\"type\": \"text\", \"text\": content_text}]\n",
    "            \n",
    "            # Add images if available\n",
    "            for image_dict in prompt_data.get('images', []):\n",
    "                if image_dict is not None and 'bytes' in image_dict:\n",
    "                    # Get image bytes and convert to base64\n",
    "                    img_bytes = image_dict['bytes']\n",
    "                    if img_bytes is not None:\n",
    "                        # Convert bytes to PIL Image then to base64\n",
    "                        image = Image.open(io.BytesIO(img_bytes))\n",
    "                        buffered = io.BytesIO()\n",
    "                        image.save(buffered, format=\"PNG\")\n",
    "                        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "                        \n",
    "                        content.append({\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                            }\n",
    "                        })\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "        else:\n",
    "            messages.append({\"role\": role, \"content\": content_text})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f898970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_responses(prompt_data, model_ids):\n",
    "    \"\"\"\n",
    "    Generate responses from different models for the given prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt_data: Dict with 'prompt', 'images', etc.\n",
    "        model_ids: List of model IDs to generate responses from\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping model_id to response\n",
    "    \"\"\"\n",
    "    responses = {}\n",
    "    for model_id in model_ids:\n",
    "        try:\n",
    "            if model_id == \"gpt-5-chat\":\n",
    "                responses[model_id] = generate_response_with_openai(prompt_data)\n",
    "            elif model_id == \"Qwen/Qwen3-VL-8B-Instruct\":\n",
    "                responses[model_id] = generate_response_with_nemotron_vlm(prompt_data)\n",
    "            elif model_id == \"nvidia/nvidia-nemotron-nano-9b-v2\":\n",
    "                # Text-only model cannot process images, return fixed response\n",
    "                responses[model_id] = \"I cannot answer that question because I am a model that can only answer questions without images.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate response for {model_id} after all retries. Skipping. Error: {e}\")\n",
    "            continue\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147119c",
   "metadata": {},
   "source": [
    "### 2.6 LLM-as-Judge\n",
    "\n",
    "Use a powerful LLM (GPT-5) to evaluate whether each model's response correctly answers the question. This creates the ground truth labels for router training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry_with_exponential_backoff(max_retries=5, initial_delay=1)\n",
    "def judge_single_response(prompt_data, model_response, ground_truth_label):\n",
    "    \"\"\"\n",
    "    Use Azure OpenAI as a judge to evaluate if a single model response correctly answers the question.\n",
    "    \n",
    "    Args:\n",
    "        prompt_data: Dict with prompt and images\n",
    "        model_response: The model's generated response\n",
    "        ground_truth_label: The correct answer from the dataset\n",
    "    \n",
    "    Returns:\n",
    "        True if the model response is correct, False otherwise\n",
    "    \"\"\"\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=\"2025-01-01-preview\"\n",
    "    )\n",
    "\n",
    "    # Compose the system prompt for the judge\n",
    "    sys_prompt = (\n",
    "        \"You are an expert AI tasked with evaluating whether a model's response correctly answers a question. \"\n",
    "        \"You will be given the original prompt (with images), the ground truth answer, and the model's response. \"\n",
    "        \"Your task is to determine whether the model's response correctly answers the question and matches \"\n",
    "        \"or logically aligns with the ground truth answer.\\n\\n\"\n",
    "        \"Reply strictly with 'yes' if the response is correct, or 'no' if it is incorrect.\"\n",
    "    )\n",
    "\n",
    "    content = [{\"type\": \"text\", \"text\": \"PROMPT: \" + prompt_data['prompt']}]\n",
    "    \n",
    "    # Add images if available\n",
    "    for image in prompt_data.get('images', []):\n",
    "        if image is not None:\n",
    "            # Convert PIL Image to base64\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "            \n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                }\n",
    "            })\n",
    "\n",
    "    content.append({\"type\": \"text\", \"text\": f\"\\n\\nGROUND TRUTH ANSWER: {ground_truth_label}\"})\n",
    "    content.append({\"type\": \"text\", \"text\": f\"\\n\\nMODEL RESPONSE: {model_response}\"})\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5-chat\",\n",
    "            messages=messages\n",
    "        )\n",
    "        judge_answer = response.choices[0].message.content.strip().lower()\n",
    "        return judge_answer.startswith(\"yes\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Azure judge evaluation: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a9e9d",
   "metadata": {},
   "source": [
    "### 2.7 Evaluation Pipeline\n",
    "\n",
    "Main function that orchestrates the entire evaluation process: generating responses from all models and judging each response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_data(dataset_split, num_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate all models on the dataset and record success (1) or failure (0) for each model.\n",
    "    \n",
    "    Args:\n",
    "        dataset_split: The dataset split to process\n",
    "        num_samples: Number of samples to process (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        List of evaluation samples with model success indicators\n",
    "    \"\"\"\n",
    "    evaluation_data = []\n",
    "    model_list = list(MODEL_IDS.keys())\n",
    "    \n",
    "    # Limit number of samples if specified\n",
    "    num_to_process = len(dataset_split) if num_samples is None else min(num_samples, len(dataset_split))\n",
    "    \n",
    "    print(f\"\\nProcessing {num_to_process} samples...\")\n",
    "    \n",
    "    skipped_samples = 0\n",
    "    \n",
    "    for idx in tqdm(range(num_to_process)):\n",
    "        sample = dataset_split[idx]\n",
    "        \n",
    "        # Prepare prompt with images\n",
    "        prompt_data = prepare_prompt_with_images(sample)\n",
    "        \n",
    "        # Skip if prompt is empty\n",
    "        if not prompt_data[\"prompt\"] or len(prompt_data[\"prompt\"]) == 0:\n",
    "            print(f\"Skipping sample {idx}: Empty prompt\")\n",
    "            skipped_samples += 1\n",
    "            continue\n",
    "        \n",
    "        responses = generate_model_responses(prompt_data, model_list)\n",
    "        \n",
    "        # Judge each model's response independently\n",
    "        model_scores = {}\n",
    "        all_failed = True\n",
    "        \n",
    "        for model_id in model_list:\n",
    "            if model_id not in responses:\n",
    "                print(f\"Skipping model {model_id} for sample {idx}: No response generated\")\n",
    "                model_scores[model_id] = 0\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Judge if the response is correct\n",
    "                is_correct = judge_single_response(prompt_data, responses[model_id], prompt_data[\"answer\"])\n",
    "                model_scores[model_id] = 1 if is_correct else 0\n",
    "                if is_correct:\n",
    "                    all_failed = False\n",
    "                print(f\"Sample {idx} - {model_id}: {'✓' if is_correct else '✗'}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to judge {model_id} for sample {idx}: {e}\")\n",
    "                model_scores[model_id] = 0\n",
    "        \n",
    "        # Convert images to base64 for serialization\n",
    "        images_base64 = []\n",
    "        for image in prompt_data[\"images\"]:\n",
    "            if image is not None:\n",
    "                buffered = io.BytesIO()\n",
    "                image.save(buffered, format=\"PNG\")\n",
    "                img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "                images_base64.append(img_base64)\n",
    "        \n",
    "        evaluation_data.append({\n",
    "            \"idx\": idx,\n",
    "            \"prompt\": prompt_data[\"prompt\"],\n",
    "            \"answer\": prompt_data[\"answer\"],\n",
    "            \"images\": images_base64,\n",
    "            \"model_scores\": model_scores\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nSkipped {skipped_samples} samples due to failures\")\n",
    "    \n",
    "    return evaluation_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f9281",
   "metadata": {},
   "source": [
    "### 2.8 Run Evaluation\n",
    "\n",
    "Execute the evaluation pipeline. Adjust `NUM_SAMPLES` to control how many samples to process.\n",
    "\n",
    "> **Note:** This step can take a long time depending on the number of samples. For a quick test, use a small value like 10-50. For training, use 1000-2000+ samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Number of samples to evaluate\n",
    "# Set to None to process all samples, or a specific number for faster testing\n",
    "NUM_SAMPLES = 2000  # Adjust as needed\n",
    "\n",
    "# Run the evaluation\n",
    "print(\"=\"*80)\n",
    "print(\"Starting model evaluation...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_evaluations = create_evaluation_data(dataset, num_samples=NUM_SAMPLES)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(all_evaluations)} evaluation samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1857436",
   "metadata": {},
   "source": [
    "### 2.9 Train/Test Split and Statistics\n",
    "\n",
    "Split the evaluation data into training and testing sets (80/20 split) and display model success rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ccb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets (80/20 split)\n",
    "split_idx = int(len(all_evaluations) * 0.8)\n",
    "train_evaluations = all_evaluations[:split_idx]\n",
    "test_evaluations = all_evaluations[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(train_evaluations)}\")\n",
    "print(f\"Test samples:  {len(test_evaluations)}\")\n",
    "\n",
    "# Calculate and print success rates for each model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Model Success Rates (Training Set):\")\n",
    "print(\"=\"*80)\n",
    "for model_id in MODEL_IDS.keys():\n",
    "    success_count = sum(1 for item in train_evaluations if item[\"model_scores\"].get(model_id, 0) == 1)\n",
    "    total_count = len(train_evaluations)\n",
    "    rate = 100 * success_count / total_count if total_count > 0 else 0\n",
    "    print(f\"  {model_id:40s}: {success_count:4d}/{total_count:4d} ({rate:5.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa780ec",
   "metadata": {},
   "source": [
    "### 2.10 Save Evaluation Data\n",
    "\n",
    "Save the evaluation data to JSON files for use in the embedding generation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfccff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths for the evaluation data\n",
    "# These files will be used by the embedding generation step\n",
    "output_train_path = \"multimodal_router/src/nat_sfc_router/training/hf_evaluations_train.json\"\n",
    "output_test_path = \"multimodal_router/src/nat_sfc_router/training/hf_evaluations_test.json\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(output_train_path), exist_ok=True)\n",
    "\n",
    "# Save training data\n",
    "with open(output_train_path, \"w\") as f:\n",
    "    json.dump(train_evaluations, f, indent=2)\n",
    "print(f\"✓ Saved training data to: {output_train_path}\")\n",
    "\n",
    "# Save test data\n",
    "with open(output_test_path, \"w\") as f:\n",
    "    json.dump(test_evaluations, f, indent=2)\n",
    "print(f\"✓ Saved test data to: {output_test_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693b5b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Embedding Generation (`generate_embeddings.py`)\n",
    "\n",
    "This section generates CLIP embeddings for the evaluation data:\n",
    "\n",
    "1. **Load evaluation JSON files** from the previous step\n",
    "2. **Connect to CLIP server** for embedding generation\n",
    "3. **Generate multimodal embeddings** (text + image → 1024-dim vector)\n",
    "4. **Save embeddings as NumPy arrays** for neural network training\n",
    "\n",
    "The embeddings combine:\n",
    "- **Text embedding** (512 dimensions from CLIP text encoder)\n",
    "- **Image embedding** (512 dimensions from CLIP image encoder, or zeros if no image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91747b06-ab71-40c5-b867-3d76aca65d9e",
   "metadata": {},
   "source": [
    "First, let's spin up an instance of the CLIP model to use for embeddings generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7fe55-ea57-48d6-8211-e7b9ba1275d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PREREQUISITE: Deploy CLIP server with Docker (Linux)\n",
    "# =====================================================\n",
    "# Run this command in your terminal BEFORE proceeding with the notebook.\n",
    "\n",
    "!docker run -d --rm \\\n",
    "    --name clip_server \\\n",
    "    --gpus all \\\n",
    "    -p 51000:51000 \\\n",
    "    jinaai/clip-as-service:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36ce40-5031-4b80-9117-b1578590a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CLIP server is responding\n",
    "c = Client('grpc://0.0.0.0:51000')\n",
    "c.profile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72fcd4-c1da-430a-9dcb-8b04004022b1",
   "metadata": {},
   "source": [
    "Now, let's proceed with configuring the CLIP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for embedding generation\n",
    "# Note: json, numpy, os, tqdm are already imported from the previous section\n",
    "from clip_client import Client\n",
    "\n",
    "# CLIP embedding dimensions\n",
    "CLIP_TEXT_DIM = 512\n",
    "CLIP_IMAGE_DIM = 512\n",
    "COMBINED_DIM = CLIP_TEXT_DIM + CLIP_IMAGE_DIM  # 1024\n",
    "\n",
    "print(f\"CLIP embedding configuration:\")\n",
    "print(f\"  Text dimension:     {CLIP_TEXT_DIM}\")\n",
    "print(f\"  Image dimension:    {CLIP_IMAGE_DIM}\")\n",
    "print(f\"  Combined dimension: {COMBINED_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171f6ca",
   "metadata": {},
   "source": [
    "### 3.1 CLIP Client Setup\n",
    "\n",
    "Connect to the CLIP embedding server. Set the `CLIP_SERVER` environment variable to your server address (e.g., `10.185.119.147:51000`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fead670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\"\n",
    "    Load and connect to the CLIP server for embedding generation.\n",
    "    \n",
    "    Returns:\n",
    "        CLIP client connected to the server\n",
    "    \"\"\"\n",
    "    print(\"Connecting to CLIP server...\")\n",
    "    clip_server = os.getenv(\"CLIP_SERVER\", \"0.0.0.0:51000\")\n",
    "    if not clip_server:\n",
    "        raise ValueError(\"CLIP_SERVER environment variable not set. Please set it to your CLIP server address.\")\n",
    "    \n",
    "    client = Client(f'grpc://{clip_server}')\n",
    "    print(f\"✓ Connected to CLIP server at: {clip_server}\")\n",
    "    return client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28deeb8",
   "metadata": {},
   "source": [
    "### 3.2 Data Loading Functions\n",
    "\n",
    "Functions to load the evaluation JSON files and prepare samples for embedding generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(json_path):\n",
    "    \"\"\"\n",
    "    Load evaluation data from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        List of evaluation samples\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_prompt_for_embedding(sample):\n",
    "    \"\"\"\n",
    "    Prepare a sample from the evaluation JSON for embedding generation.\n",
    "    Extracts prompt text and base64-encoded images.\n",
    "    \n",
    "    Args:\n",
    "        sample: Evaluation sample with 'prompt' and 'images' fields\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'prompt' (text) and 'images' (list of base64 strings)\n",
    "    \"\"\"\n",
    "    prompt = sample[\"prompt\"]\n",
    "    images = sample.get(\"images\", [])\n",
    "    \n",
    "    # Filter out None values if any\n",
    "    images = [img for img in images if img is not None]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": images,\n",
    "        \"task\": sample.get(\"task\", \"\"),\n",
    "        \"label\": sample.get(\"label\", \"\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea2b51",
   "metadata": {},
   "source": [
    "### 3.3 Embedding Generation Function\n",
    "\n",
    "Generate CLIP embeddings for text+image pairs. The function concatenates:\n",
    "- Text embedding (512 dim) from CLIP text encoder\n",
    "- Image embedding (512 dim) from CLIP image encoder, or zeros if no image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7df15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_for_dataset(clip_client, dataset_split, max_samples=None):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all prompts in the dataset using CLIP.\n",
    "    \n",
    "    Args:\n",
    "        clip_client: The CLIP client\n",
    "        dataset_split: List of evaluation samples to process\n",
    "        max_samples: Maximum number of samples to process (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings [num_samples, 1024]\n",
    "        - If text + image: [text_embedding(512) | image_embedding(512)]\n",
    "        - If text only: [text_embedding(512) | zeros(512)]\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    \n",
    "    num_to_process = len(dataset_split) if max_samples is None else min(max_samples, len(dataset_split))\n",
    "    \n",
    "    print(f\"Generating embeddings for {num_to_process} samples...\")\n",
    "    print(f\"Output embedding dimension: {COMBINED_DIM}\")\n",
    "    \n",
    "    for idx in tqdm(range(num_to_process)):\n",
    "        sample = dataset_split[idx]\n",
    "        prompt_data = prepare_prompt_for_embedding(sample)\n",
    "        \n",
    "        text = prompt_data['prompt']\n",
    "        images = prompt_data['images']\n",
    "        \n",
    "        # Generate embedding based on whether images are present\n",
    "        if len(images) > 0:\n",
    "            # Has both text and image - concatenate text and image embeddings\n",
    "            text_embedding = clip_client.encode([text])  # Shape: (1, 512)\n",
    "            image_data_uri = f\"data:image/png;base64,{images[0]}\"\n",
    "            image_embedding = clip_client.encode([image_data_uri])  # Shape: (1, 512)\n",
    "            \n",
    "            # Concatenate text and image embeddings\n",
    "            combined_embedding = np.concatenate([\n",
    "                text_embedding[0],  # (512,)\n",
    "                image_embedding[0]  # (512,)\n",
    "            ])  # Result: (1024,)\n",
    "            \n",
    "            embeddings_list.append(combined_embedding)\n",
    "        else:\n",
    "            # Text only - pad with zeros\n",
    "            text_embedding = clip_client.encode([text])  # Shape: (1, 512)\n",
    "            \n",
    "            # Pad with 512 zeros\n",
    "            padding = np.zeros(CLIP_IMAGE_DIM)\n",
    "            combined_embedding = np.concatenate([\n",
    "                text_embedding[0],  # (512,)\n",
    "                padding  # (512,)\n",
    "            ])  # Result: (1024,)\n",
    "            \n",
    "            embeddings_list.append(combined_embedding)\n",
    "    \n",
    "    # Stack all embeddings\n",
    "    embeddings = np.stack(embeddings_list, axis=0)\n",
    "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502f2a0a",
   "metadata": {},
   "source": [
    "### 3.4 Load Evaluation Data\n",
    "\n",
    "Load the evaluation JSON files created in the previous section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to evaluation data (created in the previous section)\n",
    "train_json_path = \"multimodal_router/src/nat_sfc_router/training/hf_evaluations_train.json\"\n",
    "test_json_path = \"multimodal_router/src/nat_sfc_router/training/hf_evaluations_test.json\"\n",
    "\n",
    "# Load the evaluation data\n",
    "print(\"Loading evaluation data from JSON files...\")\n",
    "train_data = load_json_data(train_json_path)\n",
    "test_data = load_json_data(test_json_path)\n",
    "\n",
    "print(f\"✓ Loaded {len(train_data)} train samples from: {train_json_path}\")\n",
    "print(f\"✓ Loaded {len(test_data)} test samples from: {test_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db0a46",
   "metadata": {},
   "source": [
    "### 3.5 Connect to CLIP Server\n",
    "\n",
    "Establish connection to the CLIP embedding server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to CLIP server\n",
    "clip_client = load_embedding_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafe935",
   "metadata": {},
   "source": [
    "### 3.6 Generate Embeddings\n",
    "\n",
    "Generate CLIP embeddings for both training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for training set\n",
    "print(\"=\"*80)\n",
    "print(\"Generating TRAIN embeddings...\")\n",
    "print(\"=\"*80)\n",
    "train_embeddings = generate_embeddings_for_dataset(clip_client, train_data, max_samples=None)\n",
    "print(f\"\\n✓ Train embeddings shape: {train_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for test set\n",
    "print(\"=\"*80)\n",
    "print(\"Generating TEST embeddings...\")\n",
    "print(\"=\"*80)\n",
    "test_embeddings = generate_embeddings_for_dataset(clip_client, test_data, max_samples=None)\n",
    "print(f\"\\n✓ Test embeddings shape: {test_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51319246",
   "metadata": {},
   "source": [
    "### 3.7 Save Embeddings\n",
    "\n",
    "Save the generated embeddings as NumPy arrays for use in neural network training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths for embeddings\n",
    "train_embeddings_path = \"multimodal_router/src/nat_sfc_router/training/hf_train_embeddings.npy\"\n",
    "test_embeddings_path = \"multimodal_router/src/nat_sfc_router/training/hf_test_embeddings.npy\"\n",
    "metadata_path = \"multimodal_router/src/nat_sfc_router/training/hf_embeddings_metadata.json\"\n",
    "\n",
    "# Save embeddings\n",
    "np.save(train_embeddings_path, train_embeddings)\n",
    "print(f\"✓ Saved train embeddings to: {train_embeddings_path}\")\n",
    "print(f\"  Shape: {train_embeddings.shape}\")\n",
    "\n",
    "np.save(test_embeddings_path, test_embeddings)\n",
    "print(f\"✓ Saved test embeddings to: {test_embeddings_path}\")\n",
    "print(f\"  Shape: {test_embeddings.shape}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"train_samples\": len(train_embeddings),\n",
    "    \"test_samples\": len(test_embeddings),\n",
    "    \"embedding_dim\": train_embeddings.shape[1],\n",
    "    \"model\": f\"CLIP (grpc://{os.getenv('CLIP_SERVER', '0.0.0.0:51000')})\",\n",
    "    \"text_dim\": CLIP_TEXT_DIM,\n",
    "    \"image_dim\": CLIP_IMAGE_DIM,\n",
    "    \"combined_dim\": COMBINED_DIM,\n",
    "    \"embedding_structure\": \"text_embedding(512) | image_embedding_or_zeros(512)\"\n",
    "}\n",
    "\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"✓ Saved metadata to: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Embeddings generated successfully!\")\n",
    "print(\"You can now use these embeddings for training the neural network router.\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b853176",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Neural Network Router Training (`nn_router.py`)\n",
    "\n",
    "This section trains a neural network router using the pre-computed embeddings:\n",
    "\n",
    "1. **Load embeddings and labels** from the previous steps\n",
    "2. **Define neural network architecture** (multi-layer with batch normalization and dropout)\n",
    "3. **Train with class weighting** to handle imbalanced data\n",
    "4. **Hyperparameter tuning** via random search\n",
    "5. **Evaluate router performance** on test set\n",
    "6. **Test threshold configurations** for cost-quality tradeoffs\n",
    "7. **Save trained model** for inference\n",
    "\n",
    "The router predicts which model will correctly answer a given query, enabling intelligent routing to optimize for both accuracy and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2289abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for neural network router\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d55417",
   "metadata": {},
   "source": [
    "### 4.1 Configuration\n",
    "\n",
    "Set up file paths, random seeds, device selection, and hyperparameter configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d84a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for training data (from previous sections)\n",
    "TRAIN_JSON = \"multimodal_router/src/nat_sfc_router/training/hf_evaluations_train.json\"\n",
    "TEST_JSON = \"multimodal_router/src/nat_sfc_router/training/hf_evaluations_test.json\"\n",
    "TRAIN_EMBEDDINGS = \"multimodal_router/src/nat_sfc_router/training/hf_train_embeddings.npy\"\n",
    "TEST_EMBEDDINGS = \"multimodal_router/src/nat_sfc_router/training/hf_test_embeddings.npy\"\n",
    "\n",
    "# Training settings\n",
    "RANDOM_STATE = 42\n",
    "TUNE_HYPERPARAMETERS = True  # Set to False to skip tuning and use default params\n",
    "USE_CLASS_WEIGHTS = True  # Set to False to disable class weighting for imbalanced data\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "# Check for GPU availability\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Default hyperparameters\n",
    "default_config = {\n",
    "    'hidden_dims': [512, 256, 128],  # Hidden layer dimensions\n",
    "    'dropout': 0.3,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 50,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 10,  # Early stopping patience\n",
    "}\n",
    "\n",
    "# Hyperparameter search space for tuning\n",
    "param_grid = {\n",
    "    'hidden_dims': [\n",
    "        [256, 128],\n",
    "        [512, 256],\n",
    "        [512, 256, 128],\n",
    "        [1024, 512, 256],\n",
    "        [256, 128, 64],\n",
    "    ],\n",
    "    'dropout': [0.2, 0.3, 0.4, 0.5],\n",
    "    'learning_rate': [0.0001, 0.0005, 0.001, 0.002],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'weight_decay': [0, 1e-6, 1e-5, 1e-4],\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  TUNE_HYPERPARAMETERS: {TUNE_HYPERPARAMETERS}\")\n",
    "print(f\"  USE_CLASS_WEIGHTS: {USE_CLASS_WEIGHTS}\")\n",
    "print(f\"  Default hidden dims: {default_config['hidden_dims']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c919b",
   "metadata": {},
   "source": [
    "### 4.2 Neural Network Architecture\n",
    "\n",
    "Define the `RouterNetwork` class - a multi-output neural network that predicts which model will correctly answer a given query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b83f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouterNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-output neural network for routing.\n",
    "    Predicts probability that each model will be correct for a given input.\n",
    "    \n",
    "    Architecture:\n",
    "    - Multiple hidden layers with batch normalization and dropout\n",
    "    - Sigmoid output for independent probability predictions per model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[512, 256, 128], dropout=0.3):\n",
    "        super(RouterNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (one sigmoid output per model)\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        # Apply sigmoid to get probabilities for each model independently\n",
    "        return torch.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a166b16",
   "metadata": {},
   "source": [
    "### 4.3 Data Loading\n",
    "\n",
    "Function to load embeddings and labels from the files created in previous sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_path: str, embeddings_path: str, selected_models=None):\n",
    "    \"\"\"\n",
    "    Load data from JSON file and corresponding embeddings.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON file with model_scores\n",
    "        embeddings_path: Path to embeddings .npy file\n",
    "        selected_models: Optional list of model names to keep. If None, keeps all models.\n",
    "    \n",
    "    Returns:\n",
    "        embeddings (X), labels (Y), model_names, and prompts.\n",
    "    \"\"\"\n",
    "    # Load JSON\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Load embeddings\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    \n",
    "    # Verify alignment\n",
    "    if len(data) != len(embeddings):\n",
    "        raise ValueError(f\"Mismatch: {len(data)} JSON records vs {len(embeddings)} embeddings\")\n",
    "    \n",
    "    # Extract labels from model_scores\n",
    "    if not data:\n",
    "        raise ValueError(\"Empty JSON file\")\n",
    "    \n",
    "    all_model_names = list(data[0]['model_scores'].keys())\n",
    "    print(f\"Found models in data: {all_model_names}\")\n",
    "    \n",
    "    # Filter to selected models if specified\n",
    "    if selected_models is not None:\n",
    "        model_names = [m for m in all_model_names if m in selected_models]\n",
    "        if len(model_names) != len(selected_models):\n",
    "            missing = set(selected_models) - set(model_names)\n",
    "            print(f\"Warning: The following selected models were not found in data: {missing}\")\n",
    "        print(f\"Using selected models: {model_names}\")\n",
    "    else:\n",
    "        model_names = all_model_names\n",
    "    \n",
    "    # Build label matrix\n",
    "    labels = []\n",
    "    prompts = []\n",
    "    for record in data:\n",
    "        prompts.append(record['prompt'])\n",
    "        label_row = [record['model_scores'].get(model, 0) for model in model_names]\n",
    "        labels.append(label_row)\n",
    "    \n",
    "    Y = np.array(labels, dtype=np.float32)\n",
    "    X = embeddings.astype(np.float32)\n",
    "    \n",
    "    return X, Y, model_names, prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cdf166",
   "metadata": {},
   "source": [
    "### 4.4 Training Utilities\n",
    "\n",
    "Functions for class weight computation and model training with early stopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y_train, model_names, method='inverse'):\n",
    "    \"\"\"\n",
    "    Compute class weights to handle imbalanced data.\n",
    "    \n",
    "    Args:\n",
    "        y_train: Training labels (n_samples, n_models)\n",
    "        model_names: List of model names\n",
    "        method: 'inverse' or 'balanced'\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_models,) with weights for each model\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Computing class weights to handle data imbalance:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        pos_rate = y_train[:, i].mean()\n",
    "        \n",
    "        if method == 'inverse':\n",
    "            weight = 1.0 / pos_rate if pos_rate > 0 else 1.0\n",
    "        elif method == 'balanced':\n",
    "            n_pos = y_train[:, i].sum()\n",
    "            weight = len(y_train) / (2 * n_pos) if n_pos > 0 else 1.0\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        \n",
    "        weights.append(weight)\n",
    "        print(f\"  {model_name:40s}: pos_rate={pos_rate:.4f}, weight={weight:.4f}\")\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = np.array(weights)\n",
    "    weights = weights * len(weights) / weights.sum()\n",
    "    \n",
    "    print(f\"\\nNormalized weights (sum={weights.sum():.2f}):\")\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        print(f\"  {model_name:40s}: {weights[i]:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config, model_names, class_weights=None):\n",
    "    \"\"\"\n",
    "    Train the neural network with early stopping.\n",
    "    \"\"\"\n",
    "    # Use weighted BCE loss if class weights are provided\n",
    "    if class_weights is not None:\n",
    "        def weighted_bce_loss(outputs, targets):\n",
    "            bce = -(targets * torch.log(outputs + 1e-8) + (1 - targets) * torch.log(1 - outputs + 1e-8))\n",
    "            weighted = bce * class_weights.to(outputs.device)\n",
    "            return weighted.mean()\n",
    "        criterion = weighted_bce_loss\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"    Epoch {epoch+1}/{config['epochs']}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config['patience']:\n",
    "                print(f\"    Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46afb6",
   "metadata": {},
   "source": [
    "### 4.5 Evaluation Functions\n",
    "\n",
    "Functions to evaluate router performance on test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_router(model, X_test, y_test, model_names, model_thresholds=None):\n",
    "    \"\"\"Evaluate the router on test set with optional thresholds.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test).to(DEVICE)\n",
    "        proba_test = model(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "    # Evaluate per-model metrics\n",
    "    metrics = {}\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        y_true = y_test[:, i]\n",
    "        y_score = proba_test[:, i]\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "        except ValueError:\n",
    "            auc = float(\"nan\")\n",
    "        preds = (y_score >= 0.5).astype(int)\n",
    "        acc = accuracy_score(y_true, preds)\n",
    "        f1 = f1_score(y_true, preds, zero_division=0)\n",
    "        metrics[model_name] = {\"auc\": auc, \"accuracy\": acc, \"f1\": f1, \"positive_rate\": y_true.mean()}\n",
    "    \n",
    "    # Router evaluation (system accuracy)\n",
    "    chosen_idx = np.argmax(proba_test, axis=1)\n",
    "    \n",
    "    # Apply model thresholds if specified\n",
    "    if model_thresholds is not None and len(model_thresholds) > 0:\n",
    "        threshold_map = {model_names.index(m): t for m, t in model_thresholds.items() if m in model_names}\n",
    "        for i in range(len(chosen_idx)):\n",
    "            chosen_model_idx = chosen_idx[i]\n",
    "            if chosen_model_idx in threshold_map:\n",
    "                if proba_test[i, chosen_model_idx] < threshold_map[chosen_model_idx]:\n",
    "                    sorted_indices = np.argsort(proba_test[i])[::-1]\n",
    "                    for candidate_idx in sorted_indices:\n",
    "                        if candidate_idx == chosen_model_idx:\n",
    "                            continue\n",
    "                        if candidate_idx in threshold_map:\n",
    "                            if proba_test[i, candidate_idx] >= threshold_map[candidate_idx]:\n",
    "                                chosen_idx[i] = candidate_idx\n",
    "                                break\n",
    "                        else:\n",
    "                            chosen_idx[i] = candidate_idx\n",
    "                            break\n",
    "    \n",
    "    chosen_label = np.array([y_test[i, chosen_idx[i]] for i in range(len(chosen_idx))])\n",
    "    system_accuracy = chosen_label.mean()\n",
    "    \n",
    "    # Model selection counts\n",
    "    model_selection_counts = {}\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        count = np.sum(chosen_idx == i)\n",
    "        model_selection_counts[model_name] = {\n",
    "            'count': int(count),\n",
    "            'percentage': count / len(chosen_idx) * 100,\n",
    "            'accuracy_when_chosen': chosen_label[chosen_idx == i].mean() if count > 0 else 0.0\n",
    "        }\n",
    "    \n",
    "    any_correct = (y_test.sum(axis=1) >= 1).mean()\n",
    "    always_acc = {model_names[i]: y_test[:, i].mean() for i in range(len(model_names))}\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics, 'system_accuracy': system_accuracy,\n",
    "        'model_selection_counts': model_selection_counts, 'any_correct': any_correct,\n",
    "        'always_acc': always_acc, 'proba_test': proba_test\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba8fd6",
   "metadata": {},
   "source": [
    "### 4.6 Hyperparameter Tuning\n",
    "\n",
    "Random search over hyperparameter space to find optimal configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X_train, y_train, X_val, y_val, model_names, class_weights=None, n_trials=10):\n",
    "    \"\"\"Perform random search over hyperparameters.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPERPARAMETER TUNING (Neural Network)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Running {n_trials} random search trials...\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_config = None\n",
    "    best_model = None\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = y_train.shape[1]\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        config = {\n",
    "            'hidden_dims': random.choice(param_grid['hidden_dims']),\n",
    "            'dropout': float(np.random.choice(param_grid['dropout'])),\n",
    "            'learning_rate': float(np.random.choice(param_grid['learning_rate'])),\n",
    "            'batch_size': int(np.random.choice(param_grid['batch_size'])),\n",
    "            'weight_decay': float(np.random.choice(param_grid['weight_decay'])),\n",
    "            'epochs': default_config['epochs'],\n",
    "            'patience': default_config['patience'],\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTrial {trial+1}/{n_trials}\")\n",
    "        print(f\"  Config: hidden={config['hidden_dims']}, dropout={config['dropout']:.2f}, \"\n",
    "              f\"lr={config['learning_rate']:.4f}, bs={config['batch_size']}\")\n",
    "        \n",
    "        model = RouterNetwork(input_dim, output_dim, config['hidden_dims'], config['dropout']).to(DEVICE)\n",
    "        \n",
    "        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "        \n",
    "        model, val_loss = train_model(model, train_loader, val_loader, config, model_names, class_weights)\n",
    "        \n",
    "        print(f\"  Final validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_config = config.copy()\n",
    "            best_model = model\n",
    "            print(f\"  *** New best model! ***\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(\"=\"*80)\n",
    "    for key, value in best_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return best_model, best_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d44cae",
   "metadata": {},
   "source": [
    "### 4.7 Load Training Data\n",
    "\n",
    "Load embeddings and labels for router training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfbb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train on\n",
    "selected_models = [\n",
    "    'gpt-5-chat',\n",
    "    'nvidia/nvidia-nemotron-nano-9b-v2',\n",
    "    'Qwen/Qwen3-VL-8B-Instruct'\n",
    "]\n",
    "\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "X_train_full, y_train_full, router_model_names, train_prompts = load_data(\n",
    "    TRAIN_JSON, TRAIN_EMBEDDINGS, selected_models=selected_models\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "print(\"\\nLoading test data...\")\n",
    "X_test, y_test, _, test_prompts = load_data(\n",
    "    TEST_JSON, TEST_EMBEDDINGS, selected_models=selected_models\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training set: {X_train_full.shape[0]} samples, {X_train_full.shape[1]} features\")\n",
    "print(f\"✓ Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"✓ Models: {router_model_names}\")\n",
    "print(f\"\\nLabel distribution (train):\")\n",
    "for i, model in enumerate(router_model_names):\n",
    "    pos_rate = y_train_full[:, i].mean()\n",
    "    print(f\"  {model}: {pos_rate:.3f} positive rate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74189193",
   "metadata": {},
   "source": [
    "### 4.8 Train Router Model\n",
    "\n",
    "Split data into train/validation sets, compute class weights, and train the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ca780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train/val for hyperparameter tuning\n",
    "val_size = int(0.15 * len(X_train_full))\n",
    "indices = np.random.permutation(len(X_train_full))\n",
    "val_indices = indices[:val_size]\n",
    "train_indices = indices[val_size:]\n",
    "\n",
    "X_train = X_train_full[train_indices]\n",
    "y_train = y_train_full[train_indices]\n",
    "X_val = X_train_full[val_indices]\n",
    "y_val = y_train_full[val_indices]\n",
    "\n",
    "print(f\"Split: {len(X_train)} train, {len(X_val)} validation\")\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    class_weights = compute_class_weights(y_train, router_model_names, method='balanced')\n",
    "\n",
    "# Train model\n",
    "if TUNE_HYPERPARAMETERS:\n",
    "    router_model, best_config = tune_hyperparameters(\n",
    "        X_train, y_train, X_val, y_val, router_model_names, class_weights, n_trials=10\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nTraining with default parameters...\")\n",
    "    config = default_config.copy()\n",
    "    router_model = RouterNetwork(input_dim, output_dim, config['hidden_dims'], config['dropout']).to(DEVICE)\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    router_model, val_loss = train_model(router_model, train_loader, val_loader, config, router_model_names, class_weights)\n",
    "    best_config = config\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3c936",
   "metadata": {},
   "source": [
    "### 4.9 Evaluate Router\n",
    "\n",
    "Evaluate the trained router on the test set and display metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATION ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = evaluate_router(router_model, X_test, y_test, router_model_names)\n",
    "\n",
    "# Print per-model metrics\n",
    "print(\"\\nPer-model metrics (on test set):\")\n",
    "print(\"=\"*80)\n",
    "for model_name, m in results['metrics'].items():\n",
    "    print(f\"  {model_name:40s}: AUC={m['auc']:.3f}, acc={m['accuracy']:.3f}, \"\n",
    "          f\"f1={m['f1']:.3f}, pos_rate={m['positive_rate']:.3f}\")\n",
    "\n",
    "# Print baselines\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Baseline (always choose single model) accuracies:\")\n",
    "print(\"=\"*80)\n",
    "for model_name, acc in results['always_acc'].items():\n",
    "    print(f\"  {model_name:40s}: {acc:.3f}\")\n",
    "\n",
    "# Print router selection distribution\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Router model selection distribution:\")\n",
    "print(\"=\"*80)\n",
    "for model_name, stats in results['model_selection_counts'].items():\n",
    "    print(f\"  {model_name:40s}: selected {stats['count']:4d} times \"\n",
    "          f\"({stats['percentage']:5.1f}%) - accuracy when chosen: {stats['accuracy_when_chosen']:.3f}\")\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Router system accuracy: {results['system_accuracy']:.3f}\")\n",
    "print(f\"Oracle (best possible):  {results['any_correct']:.3f}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f022e4b",
   "metadata": {},
   "source": [
    "### 4.10 Save Trained Model\n",
    "\n",
    "Save the trained router model for later use in inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d879bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and config\n",
    "out_dir = Path(\"multimodal_router/src/nat_sfc_router/training/router_artifacts\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = out_dir / \"nn_router.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': router_model.state_dict(),\n",
    "    'config': best_config,\n",
    "    'model_names': router_model_names,\n",
    "    'input_dim': input_dim,\n",
    "    'output_dim': output_dim,\n",
    "}, model_path)\n",
    "\n",
    "print(f\"✓ Saved trained router model to: {model_path}\")\n",
    "print(f\"\\nModel configuration:\")\n",
    "for key, value in best_config.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c114e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
