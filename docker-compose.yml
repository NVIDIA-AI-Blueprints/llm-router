version: '3.8'

services:
  # Router Backend - handles routing logic and endpoints
  router-backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: router-backend
    ports:
      - "8001:8001"
    volumes:
      # Mount configs for dynamic configuration without rebuilding
      - ./src/nat_sfc_router/configs/config.yml:/opt/dgxcbot/router/config.yml
    environment:
      - CUDA_VISIBLE_DEVICES=0
      # Intent-based router configuration
      - ROUTER_MODEL_URL=http://qwen-router:8000
      - ROUTER_MODEL_NAME=Qwen/Qwen3-1.7B
      # Neural network router configuration
      - CLIP_SERVER=grpc://clip-server:51000
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ipc: host
    depends_on:
      qwen-router:
        condition: service_healthy
        required: false
      clip-server:
        condition: service_healthy
        required: false
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - router-network

  # Qwen3 1.7B used for intent-based routing
  qwen-router:
    image: vllm/vllm-openai:latest
    container_name: qwen-router
    profiles:
      - intent  # Use 'intent' profile for intent-based routing
    ports:
      - "8011:8000"
    volumes:
      # Cache HuggingFace models locally to avoid re-downloading
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./qwen3_nonthinking.jinja:/app/qwen3_nonthinking.jinja
    environment:
      - CUDA_VISIBLE_DEVICES=0
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ipc: host
    command: ["--model", "Qwen/Qwen3-1.7B", "--chat-template", "/app/qwen3_nonthinking.jinja"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 120s
    networks:
      - router-network

  # CLIP Embedding Server used for neural network routing
  clip-server:
    image: jinaai/clip-as-service:latest
    container_name: clip-server
    profiles:
      - nn  # Use 'nn' profile for neural network routing
    ports:
      - "51000:51000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "exit 0"]
      interval: 10s
      timeout: 5s
      retries: 1
      start_period: 60s
    networks:
      - router-network

  # Demo Web App - Gradio UI for interacting with the router
  demo-app:
    build:
      context: ./demo
      dockerfile: Dockerfile
    container_name: router-demo
    ports:
      - "7860:7860"
    environment:
      # Router endpoint (uses internal docker network)
      - ROUTER_ENDPOINT=http://router-backend:8001/sfc_router/chat/completions
      # API Keys (set these via .env file or environment variables)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - NVIDIA_API_KEY=${NVIDIA_API_KEY}
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
    depends_on:
      router-backend:
        condition: service_healthy
    networks:
      - router-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

networks:
  router-network:
    driver: bridge

