{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e60cb36-f153-4991-a31f-702f11144446",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f2acb-52d9-49dd-9676-2d58715786f5",
   "metadata": {},
   "source": [
    "\n",
    "Ever struggled to decide which LLM to use for a specific task? In an ideal world the most accurate model would also be the cheapest and fastest, but in practice modern agentic AI systems have to make trade-offs between accuracy, speed, and cost.\n",
    "\n",
    "This blueprint provides a router that automates these tradeoffs by routing user prompts between different LLMs. Given a user prompt, the router:\n",
    "\n",
    "- applies a policy (eg task classification or intent classification)\n",
    "- uses a router trained for that policy to map the prompt to an appropriate LLM\n",
    "- proxies the prompt to the identified fit-for-purpose LLM\n",
    "\n",
    "For example, using a task classification policy, the following user prompts can be classified into tasks and directed to the appropriate model.\n",
    "\n",
    "| User Prompt | Task Classification | Route To |\n",
    "|---|---|---|\n",
    "| \"Help me write a python function to load salesforce data into my warehouse.\" | Code Generation | deepseek |\n",
    "| \"Tell me about your return policy \" | Open QA | llama 70B | \n",
    "| \"Rewrite the user prompt to be better for an LLM agent. User prompt: what is the best coffee recipe\" | Rewrite | llama 8B |\n",
    "\n",
    "The key features of the LLM Router are:\n",
    "\n",
    "- OpenAI API compliant: use the LLM Router as a drop-in replacement in code regardless of which LLM framework you use.\n",
    "- Flexible: use the default policy and router, or create your own policy and fine tune a router. We expect additional trained routers to be available from the community as well.\n",
    "- Configurable: easily configure which backend models are available. \n",
    "- Performant: LLM Router uses Rust and NVIDIA Triton Inference Server to add minimal latency compared to routing requests directly to a model.\n",
    "\n",
    "This notebook will walk you through deploying the router and making sample requests. We will also outline common trouble shooting steps and metrics that are available to monitor the router. The blueprint also includes an example client application, an example loadtest, and details on how to customize the router policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c7758-746f-4e00-8a28-cbdc463a7925",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    ">[Prerequisites](#Prerequisites)  \n",
    ">[Understand the blueprint](#Understand-the-blueprint)  \n",
    ">[Spin up blueprint](#Spin-up-the-blueprint)    \n",
    ">[Test the LLM Router](#Test-the-LLM-Router)  \n",
    ">[Use the example app](#Use-the-example-app)\n",
    ">[View metrics](#View-metrics)\n",
    ">[Next Steps](#Next-Steps)  \n",
    ">[Shutting down blueprint](#Stopping-Services-and-Cleaning-Up)  \n",
    "________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fcc47-fb41-4e54-9d30-4d17bc483779",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09678c9-8fbe-41d7-84ad-ce624bec582c",
   "metadata": {},
   "source": [
    "### Clone repository and install software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2b89e-f8c3-458e-88a4-ecfac63a9916",
   "metadata": {},
   "source": [
    "1. Install **[Docker](https://docs.docker.com/engine/install/ubuntu/)**\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip:</b> Ensure the Docker Compose plugin version is 2.29.1 or higher.  Run docker compose version to confirm. Refer to Install the Compose plugin Docker documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8d198-db10-4529-b76b-14f0c5cf216d",
   "metadata": {},
   "source": [
    "2. Install **[NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-the-nvidia-container-toolkit)** to configure Docker for GPU-accelerated containers.\n",
    " If you are using a system deployed with Brev you can skip this step since Brev systems come with NVIDIA Container Toolkit preinstalled. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b33bb-eade-43ec-806a-1d1dcdc7e773",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> After installing the toolkit, follow the instructions in the Configure Docker section in the NVIDIA Container Toolkit documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b335e1",
   "metadata": {},
   "source": [
    "3. Make sure necessary python packages mentioned in requirements.txt are installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb0839",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26d7bf-0044-45c4-bace-18b63c41dd04",
   "metadata": {},
   "source": [
    "### Get API Keys\n",
    "\n",
    "#### NVIDIA NGC Catalog key\n",
    " \n",
    "The NVIDIA NGC API Key is used in this blueprint in order to pull the default router models from NGC as well as the NVIDIA Triton Inference Server docker image which is used to run those models. Refer to [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key) in the NVIDIA NGC User Guide for more information.\n",
    "\n",
    "\n",
    "Authenticate with the NVIDIA Container Registry with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe4bca-f542-4610-8424-0b97a57237db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker login nvcr.io --username $oauthtoken --password \"ENTER YOUR NGC API KEY HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1441d659-be45-422e-a732-530750286da7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Use oauthtoken as the username and your API key as the password. The $oauthtoken username is a special name that indicates that you will authenticate with an API key and not a user name and password. After installing the toolkit, follow the instructions in the Configure Docker section in the NVIDIA Container Toolkit documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec102b-21d3-441c-a1a8-e72dbfc6c6fd",
   "metadata": {},
   "source": [
    "#### NVIDIA API Catalog key\n",
    "\n",
    "The LLM router is responsible for routing requests between foundational LLM models. In this example, we will use NVIDIA NIMs. In order to access these LLMs we will use a NVIDIA API Catalog key, which is separate from the NVIDIA NGC API key used above. You can alternatively use other LLM models or even on-premise models. To do so, you would update the router configuration file which will be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47fa8f-7b46-4188-aa77-73d57438058c",
   "metadata": {},
   "source": [
    "1. Navigate to **[NVIDIA API Catalog](https://build.nvidia.com/explore/discover)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b707189-4848-4ebc-ab4e-d55d7084bff5",
   "metadata": {},
   "source": [
    "2. Select a model, such as llama3-8b-instruct.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4217edc7-4689-4c91-8f32-59fe32d4240b",
   "metadata": {},
   "source": [
    "3. Select an **Input** option. The following example is of a model that offers a Docker option. Not all of the models offer this option, but all include a “Get API Key” link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff83e0-cfee-4c77-910b-317f087df453",
   "metadata": {},
   "source": [
    "<img src=\"https://docscontent.nvidia.com/dims4/default/d6307a8/2147483647/strip/true/crop/1920x919+0+0/resize/2880x1378!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fsphinx%2F00000192-bfa6-da2c-a1f2-ffbf41aa0000%2Fnim%2Flarge-language-models%2Flatest%2F_images%2Fbuild_docker_tab.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1931c98c-4419-44e9-85d1-edb98c40d655",
   "metadata": {},
   "source": [
    "3. Click **Get API Key**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed353f53-1998-413c-9a64-1bcaad83913c",
   "metadata": {},
   "source": [
    "<img src=\"https://docscontent.nvidia.com/dims4/default/c6e2096/2147483647/strip/true/crop/1920x919+0+0/resize/2880x1378!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fsphinx%2F00000192-bfa6-da2c-a1f2-ffbf41aa0000%2Fnim%2Flarge-language-models%2Flatest%2F_images%2Fbuild_get_api_key.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764837f-217c-43db-b48b-30ce34b6daf4",
   "metadata": {},
   "source": [
    "4. Select **\"Generate Key\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5065b-0472-4947-a4b1-be13cace84d1",
   "metadata": {},
   "source": [
    "<img src=\"https://docscontent.nvidia.com/dims4/default/e7c4057/2147483647/strip/true/crop/1920x919+0+0/resize/2880x1378!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fsphinx%2F00000192-bfa6-da2c-a1f2-ffbf41aa0000%2Fnim%2Flarge-language-models%2Flatest%2F_images%2Fbuild_generate_key.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b06b0-c691-4c0e-b288-19d3fa7ca109",
   "metadata": {},
   "source": [
    "5. **Copy your key** and store it in a secure place. Do not share it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e6fdc-e035-44b9-87c8-3564f00b01ad",
   "metadata": {},
   "source": [
    "<img src=\"https://docscontent.nvidia.com/dims4/default/4b0710a/2147483647/strip/true/crop/1920x919+0+0/resize/2880x1378!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fsphinx%2F00000192-bfa6-da2c-a1f2-ffbf41aa0000%2Fnim%2Flarge-language-models%2Flatest%2F_images%2Fbuild_copy_key.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f2e66",
   "metadata": {},
   "source": [
    "## Understand the blueprint\n",
    "\n",
    "The LLM Router is composed of three components: \n",
    "\n",
    "- <b>Router Controller</b> - is a service similar to a proxy that routes OpenAI compatible requests.\n",
    "- <b>Router Server</b> - is a service that classifies the user's prompt according to a routing strategy and policy. The classification is made using a pre-trained model. \n",
    "- <b>Downstream LLMs</b> - are the LLMs the prompt will be routed to, typically foundational LLMs. \n",
    "\n",
    "These three components are all managed in the LLM Router configuration file which is located at `src/router-controller/config.yml`. \n",
    "\n",
    "```yaml \n",
    "policies:\n",
    "  - name: \"task_router\"\n",
    "    url: http://router-server:8000/v2/models/task_router_ensemble/infer\n",
    "    llms:\n",
    "      - name: Brainstorming\n",
    "        api_base: https://integrate.api.nvidia.com\n",
    "        api_key: \n",
    "        model: meta/llama-3.1-70b-instruct\n",
    "      - name: Chatbot\n",
    "        api_base: https://integrate.api.nvidia.com\n",
    "        api_key: \n",
    "        model: mistralai/mixtral-8x22b-instruct-v0.1\n",
    "    ...    \n",
    "  - name: \"complexity_router\"\n",
    "    url: http://router-server:8000/v2/models/complexity_router_ensemble/infer\n",
    "    llms:\n",
    "      - name: Creativity\n",
    "        api_base: https://integrate.api.nvidia.com\n",
    "        api_key: \n",
    "        model: meta/llama-3.1-70b-instruct\n",
    "      - name: Reasoning\n",
    "        api_base: https://integrate.api.nvidia.com\n",
    "        api_key: \n",
    "        model: deepseek-ai/deepseek-r1\n",
    "    ...\n",
    "```\n",
    "\n",
    "### Policies\n",
    "\n",
    "The configuration file specifies the routing policies. In the default configuration, a prompt can either be classified using a `task_router` policy or a `complexity_router` policy. \n",
    "\n",
    "The `task_router` uses a pre-trained model that will be deployed at `http://router-server:8000/v2/models/task_router_ensemble/infer`. The model classifies prompts into categories based on the task of the prompt:\n",
    "  - Brainstorming\n",
    "  - Chatbot\n",
    "  - Classification\n",
    "  - Closed QA\n",
    "  - Code Generation\n",
    "  - Extraction\n",
    "  - Open QA\n",
    "  - Other\n",
    "  - Rewrite\n",
    "  - Summarization\n",
    "  - Text Generation\n",
    "  - Unknown\n",
    "\n",
    "For example, the prompt `Help me write a python function to load salesforce data into my warehouse` would be classified as a `Code Generation` task.\n",
    "\n",
    "The `complexity_router` uses a different pre-trained model that will be deployed at `http://router-server:8000/v2/models/complexity_router_ensemble/infer`. This model classifies prompts into categories based on the complexity of the prompt:\n",
    "  - Creativity: Prompts that require create knowledge, eg \"write me a science fiction story\".\n",
    "  - Reasoning: Prompts that require reasoning, eg solving a riddle.\n",
    "  - Contextual-Knowledge: Prompts that require background information, eg asking for technical help with a specific product.\n",
    "  - Few-Shot: Prompts that include example questions and answers.\n",
    "  - Domain-Knowledge: Prompts that require broad domain knowledge, such as asking for an explanation of a historical event.\n",
    "  - No-Label-Reason: Prompts that are not classified into one of the other categories.\n",
    "  - Constraint: Prompts that include specific constraints, eg requesting an answer in a haiku format.\n",
    "\n",
    "The `customize/README.md` describes how to create your own policy and classification model, providing an example showing a policy for classifying user interactions with a bank support chatbot. \n",
    "\n",
    "### LLMs \n",
    "\n",
    "The `llms` portion of the configuration file specifies where the classified prompts should be routed. For example, in the default configuration file: \n",
    "\n",
    "```yaml\n",
    "policies:\n",
    "  - name: \"task_router\"\n",
    "    url: http://router-server:8000/v2/models/task_router_ensemble/infer\n",
    "    llms:\n",
    "      - name: Brainstorming\n",
    "        api_base: https://integrate.api.nvidia.com\n",
    "        api_key: \n",
    "        model: meta/llama-3.1-70b-instruct\n",
    "      - name: Chatbot\n",
    "        api_base: https://integrate.api.nvidia.com\n",
    "        api_key: \n",
    "        model: mistralai/mixtral-8x22b-instruct-v0.1\n",
    "```\n",
    "\n",
    "A prompt sent to the `task_router` policy classified as a Brainstorming task would be proxied to the NVIDIA NIM `meta/llama-3.1-70b-instruct` whereas a prompt classified as a Chatbot task would be sent to `mistralai/mixtral-8x22b-instruct-v0.1`. \n",
    "\n",
    "### Using the router\n",
    "\n",
    "The LLM Router is compatible with OpenAI API requests. This means that any applications or code that normally use an OpenAI API client (such as LangChain) can use LLM Router with minimal modification. For example, this RESTful API request to LLM Router follows the OpenAI specification with a few modifications:\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'http://0.0.0.0:8084/v1/chat/completions' \\   # the URL to the deployed LLM router\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "    \"model\": \"\",                                # the model field is left blank, as the LLM router will add this based on the prompt classification\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Hello! How are you?\"\n",
    "      },\n",
    "      {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"Hi! I am quite well, how can I help you today?\"\n",
    "      },\n",
    "      {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Can you write me a song? Use as many emojis as possible.\"\n",
    "      }\n",
    "    ],\n",
    "    \"max_tokens\": 64,\n",
    "    \"stream\": true,\n",
    "    \"nim-llm-router\": {\"policy\": \"task_router\",\n",
    "                       \"routing_strategy\": \"triton\",\n",
    "                       \"model\": \"\"}\n",
    "  }'\n",
    "```\n",
    "\n",
    "The primary modification is the inclusion of the `nim-llm-router` metadata in the body of the request. In most python clients this metadata would be added as `extra_body`, see `src/test_router.py` for an example. The required metadata is:\n",
    "\n",
    "- policy: the policy to use for classification, by default either `task_router`  or `complexity_router`.\n",
    "- routing_strategy: either `triton` which means the prompt is sent to a model for classification or `manual` which means that the classification is skipped - use this if the client needs to make a manual over-ride\n",
    "- model: if the `routing_strategy` is `triton` leave this blank, if the routing strategy is `manual` specify the model over-ride\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7f3c4-d8c3-4b96-bb43-5c15c4c4918b",
   "metadata": {},
   "source": [
    "## Spin up the blueprint\n",
    "Docker compose scripts are provided which spin up the LLM Router microservices on a single node. Make commands are provided for convenience that use the docker compose scripts.\n",
    "\n",
    "### Download the artifacts\n",
    "\n",
    "Start by adding your NGC API key to the commmand below and then running the `make download` command. This command will configure the NGC CLI and then download the pre-trained models for the task and complexity router policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export NGC_CLI_API_KEY=\"ENTER YOUR NGC API KEY HERE\" && export NGC_CLI_ORG=\"ENTER YOUR NGC ORG HERE\" && cd .. && make download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca1484",
   "metadata": {},
   "source": [
    "To confirm the command ran successfully, inspect the `routers` directory. The original folder was empty, but it should now include folders for each router policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && ls ./routers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c8c9d",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "```\n",
    "complexity_router  complexity_router_ensemble  postprocessing_complexity_router  postprocessing_task_router  preprocessing_complexity_router  preprocessing_task_router  readme.md  task_router  task_router_ensemble\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a72c7",
   "metadata": {},
   "source": [
    "### Update the configuration\n",
    "\n",
    "The default LLM Router configuration uses NVIDIA NIMs as the foundational LLMs used to respond to user prompts. In order to access these LLMs we need to add the NVIDIA API key to the router configuration:\n",
    "\n",
    "1. Open the router controller configuration file located in `src/router-controller/config.yaml` \n",
    "2. Add your NVIDIA API key to each line that has an empty `api_key` field\n",
    "\n",
    "Note that you can use other LLMs as well, including locally hosted NVIDIA NIMs or 3rd party LLMs that are OpenAI API compatible.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip:</b> The NVIDIA API key to enter in this file DOES begin with the letters nvapi-."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7869c30b",
   "metadata": {},
   "source": [
    "### Start the services\n",
    "\n",
    "Next, run the `make up` command. This command will run docker compose to start the router controller and router server. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b> The first time you run this command the docker images needs to be pulled and built. This step could take up to 20 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c6e5d-0804-45be-97cc-2c7b45bbb903",
   "metadata": {},
   "source": [
    "To validate the deployment of the blueprint, execute the following command to ensure the container are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf647f-0f0b-45e2-959b-d96b013169a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps --format \"table {{{{.ID}}}}\\t{{{{.Names}}}}\\t{{{{.Status}}}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90c358-f0e9-4607-8b88-32a44ffce74e",
   "metadata": {},
   "source": [
    "This command should produce similiar output in the following format:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f71f3651-f6d1-461b-bd3b-c370339e8ed3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "CONTAINER ID   NAMES                            STATUS\n",
    "5498c569db5e   llm-router-router-controller-1   Up 6 minutes\n",
    "526f6e76dbe0   llm-router-router-server-1       Up 6 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942805f-f317-48f8-9d74-51e125ba50b4",
   "metadata": {},
   "source": [
    "## Test the LLM Router\n",
    "\n",
    "Now that the services are running, we can use Python to create prompts and pass them to the router. The following code will send the prompt to the task router policy. The classified prompt will then be forwarded to the appropriate LLM, and the response will be sent back.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip:</b> If the code fails, see the troubleshooting section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from typing import Literal \n",
    "\n",
    "def make_request_to_router(PROMPT: str, policy: Literal['task_router', 'complexity_router']): \n",
    "    \"\"\" make a request to a LLM through the LLM router \"\"\"\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key=\"open-ai-placeholder\", # can leave this place holder as-is, the API key for the proxied LLM is pulled from the router configuration file\n",
    "        max_retries=0,\n",
    "        base_url=\"http://127.0.0.1:8084/v1/\" # the url for the router-controller running locally\n",
    "    )\n",
    "\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": PROMPT\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    extra_body = {\n",
    "        \"nim-llm-router\": {\n",
    "            \"policy\": policy,     \n",
    "            \"routing_strategy\": \"triton\", \n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"\",\n",
    "        extra_body=extra_body\n",
    "    )\n",
    "    return response \n",
    "\n",
    "PROMPT = \"Write a python function that implements the two sum algorithm.\"\n",
    "chat_completion = make_request_to_router(PROMPT, \"task_router\")\n",
    "\n",
    "print(f\"Prompt was handled by model: {chat_completion.model}\")\n",
    "print(chat_completion.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced1e5cf",
   "metadata": {},
   "source": [
    "The example above requested code generation and was classified accordingly, being routed to a the rather expensive R1 model. A simpler rewrite prompt is classified and routed to a cheaper model. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Rewrite this prompt to be better suited for a LLM. Prompt: Tell me about polar bears\"\n",
    "chat_completion = make_request_to_router(PROMPT, \"task_router\")\n",
    "\n",
    "print(f\"Prompt was handled by model: {chat_completion.model}\")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3567e",
   "metadata": {},
   "source": [
    "You can also make requests to the router directly through a curl request. For an example look at `src/test_router.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ab222",
   "metadata": {},
   "source": [
    "### Understanding classifications \n",
    "\n",
    "Invoking the router leads to a two step process:\n",
    "\n",
    "1. The user's prompt is classified based on the router controller policy\n",
    "\n",
    "2. Based on the classification, the request is forwarded to the downstream LLM\n",
    "\n",
    "By default, the router proxies back a response that is identical to an OpenAI API. If you want to see details of the router's classification system you can bypass the proxy and directly interact with the router policy model. The code below shows how a prompt would be classified with the task router policy.\n",
    "\n",
    "*Note*: You can also see how a prompt is routed by viewing the router logs during a request. Run `docker compose logs router-controller -f` while making a request to see the details. You can also inspect the headers from the request to see information about the prompt classification if you use a client that returns the header values. Finally, aggregate metrics about prompt classification and routing are visible through a metrics service as described in the [Metrics](#view-metrics) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c5de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "from transformers import AutoConfig\n",
    "import warnings\n",
    "import yaml\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PROMPT = \"Can you write me a song?  Use as many emojis as possible.\"\n",
    "\n",
    "def send_request(triton_client, text):\n",
    "    input_text = np.array([[text]], dtype=object)\n",
    "    inputs = [httpclient.InferInput(\"INPUT\", input_text.shape, \"BYTES\")]\n",
    "    inputs[0].set_data_from_numpy(input_text)\n",
    "\n",
    "    outputs = [httpclient.InferRequestedOutput(\"OUTPUT\")]\n",
    "\n",
    "    response = triton_client.infer(model_name=\"task_router_ensemble\", inputs=inputs, outputs=outputs)\n",
    "    return response\n",
    "\n",
    "# Load the config\n",
    "config = AutoConfig.from_pretrained(\"nvidia/prompt-task-and-complexity-classifier\")\n",
    "\n",
    "with open(\"../src/router-controller/config.yaml\", 'r') as file:\n",
    "    router_config = yaml.safe_load(file) \n",
    "\n",
    "models = [llm['model'] for llm in router_config['policies'][0]['llms']]\n",
    "\n",
    "# Make the request\n",
    "triton_client = httpclient.InferenceServerClient(url=\"127.0.0.1:8000\")\n",
    "result = send_request(triton_client, PROMPT)\n",
    "\n",
    "# Format the output\n",
    "output_data = result.as_numpy(\"OUTPUT\")\n",
    "predicted_task_index = np.argmax(output_data)\n",
    "\n",
    "task_types = list(config.task_type_map.values())\n",
    "predicted_task = task_types[predicted_task_index]\n",
    "predicted_model = models[predicted_task_index]\n",
    "\n",
    "print(f\"Using policy: task_router\")\n",
    "print(f\"Input prompt: {PROMPT}\")\n",
    "print(f\"Predicted class: {predicted_task}\")\n",
    "print(f\"Predicted model: {predicted_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3b01c",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "If the code above failed there are a few debugging steps to consider:\n",
    "\n",
    "- Start by running `docker compose logs router-controller`. This command will show the logs from the router controller microservice. The log details the steps the request takes including prompt classification and the proxied request to the LLM.\n",
    "- The router-controller logs may indicate that the request failed to be classified, typically with an error message related to the Triton service. In this case, run `docker compose logs router-server`. You may need to try re-downloading the router models, or double checking that the GPU hardware requirements for the router server are satisfied. \n",
    "- The router-controller logs may indicate that the request was correctly classified, but then failed during the proxied LLM call. Typical indications would be a 403, 404, or 500 error message. In this case, double check that the router controller configuration file has appropriate URLs for the LLM including a valid api key and valid model names. If you encounter a 500 error message, re-trying the code may be sufficient.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b> If you make changes to the configuration file or router models, be sure to restart the services. You can do this by running <code>make down</code> followed by <code>make up</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fc4b9",
   "metadata": {},
   "source": [
    "### Use a different policy\n",
    "\n",
    "The first test used the task router policy. You can also send requests that use the complexity router policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "PROMPT = \"Can you write me a song?  Use as many emojis as possible.\"\n",
    "\n",
    "# The following prompts showcase complexity routing, uncomment as desired\n",
    "\n",
    "## Reasoning\n",
    "# PROMPT = \"\"\"\n",
    "# A farmer needs to transport a wolf, a goat, and a cabbage across a river. \n",
    "# He has a boat that can only carry himself and one item at a time. \n",
    "# If left alone together, the wolf will eat the goat, and the goat will eat the cabbage. \n",
    "# How can the farmer safely transport all three items across the river?\n",
    "# \"\"\"\n",
    "\n",
    "## Domain Knowledge \n",
    "# PROMPT = \"\"\"\n",
    "# Re-solve the classic farmer transporting a wolf, goat, and cabbage riddle using graph theory. \n",
    "# Define nodes as valid states (e.g., 'FWGC-left') and edges as permissible boat movements. Formalize the solution as a shortest-path algorithm.\n",
    "# \"\"\"\n",
    "\n",
    "## Creativity\n",
    "# PROMPT = \"\"\"\n",
    "# Write a scientific fiction story about the classic farmer transporting a wolf, goat, and cabbage riddle\"\n",
    "# \"\"\"\n",
    "\n",
    "chat_completion = make_request_to_router(PROMPT, \"complexity_router\")\n",
    "\n",
    "print(f\"Prompt was handled by model: {chat_completion.model}\")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e54f5",
   "metadata": {},
   "source": [
    "## Use the example app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e949b2a",
   "metadata": {},
   "source": [
    "The blueprint also includes an example client application that is useful for testing different prompts and routing policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c806465",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86636a8d",
   "metadata": {},
   "source": [
    "The app will be accessible at `http://localhost:8008`. \n",
    "\n",
    "<img src=\"../assets/app_screenshot.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54248436",
   "metadata": {},
   "source": [
    "## View metrics\n",
    "\n",
    "The LLM Router automatically emits metrics to help you track behavior. To see these metrics, run `make metrics` from the repository root, or `cd .. && make metrics` from the launchable notebook directory. This make command will launch Prometheus and Grafana. \n",
    "- Open Grafana by navigating to `http://localhost:3000`\n",
    "- Login using the default credentials, username: `admin` and passwords: `secret`. \n",
    "- Select \"Data source\" and pick \"Prometheus\". Enter `http://prometheus:9090` as the Prometheus URL.\n",
    "- Use the Grafana \"Explore\" query interfaces to look at available metrics.\n",
    "\n",
    "<img src=\"../assets/grafana_metric_example2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24f9a7",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The LLM Router is running and you know how to adapt your OpenAI API code to use it. Further explore:\n",
    "\n",
    "- Customization: The LLM Router directs requests to backend models based on the config file `src/router-controller/config.yml`. Update the model endpoints in this file to change the backend routing behaviors. You may also wish to customize the router, see `customize/README.md` for details.\n",
    "\n",
    "- Performance: This blueprint includes a sample load test that can be run against the router. Metrics are automatically created and stored in a Grafana dashboard. To get started, run `make loadtest`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23cdd4d-2f22-46c1-ab94-99847c1c7dbb",
   "metadata": {},
   "source": [
    "## Stopping Services and Cleaning Up\n",
    "\n",
    "To shut down the microservices, run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b77f00-0fb3-4287-9503-9435f3da4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make down"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
