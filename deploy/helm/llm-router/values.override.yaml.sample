# LLM Router Helm deployment sample configuration
# Copy this file to values.override.yaml and customize for your environment
# Prerequisites: Complete README Steps 1-3 (Router Controller Configuration) before deployment
global:
  storageClass: "YOUR_STORAGE_CLASS"  # e.g., "standard", "microk8s-hostpath"
  imageRegistry: "YOUR_REGISTRY/"  # e.g., "nvcr.io/yourorg/", "docker.io/youruser/"
  imagePullSecrets:
    - name: nvcr-secret  # Change name to match your registry secret

ingress:
  enabled: false  # Enable for external access via domain names
  className: ""  # Use default ingress class for microk8s
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  hosts:
    - host: llm-router.local
      paths:
        - path: /app(/|$)(.*)
          pathType: ImplementationSpecific
          service: app
        - path: /router-controller(/|$)(.*)
          pathType: ImplementationSpecific
          service: router-controller

# Router Server
routerServer:
  enabled: true
  replicas: 1  # Single replica for simpler deployment
  image:
    repository: llm-router-server
    tag: latest
    pullPolicy: IfNotPresent
  env:
    - name: HF_HOME
      value: "/tmp/huggingface"
    - name: TRANSFORMERS_CACHE
      value: "/tmp/huggingface/transformers"
    - name: HF_HUB_CACHE
      value: "/tmp/huggingface/hub"
  resources:
    limits:
      nvidia.com/gpu: 1
      memory: "8Gi"
    requests:
      nvidia.com/gpu: 1
      memory: "8Gi"
  modelRepository:
    path: "/model_repository/routers"
  volumes:
    modelRepository:
      enabled: true
      mountPath: "/model_repository"
      storage:
        persistentVolumeClaim:
          enabled: true
          existingClaim: "router-models-pvc"
  service:
    type: ClusterIP
  shm_size: "8G"

# Router Controller
routerController:
  enabled: true
  replicas: 1  # Single replica for simpler deployment
  image:
    repository: llm-router-controller
    tag: latest
    pullPolicy: IfNotPresent
  
  # Configuration Examples (uncomment and customize as needed)
  
  # Option 1: Default NVIDIA Cloud API (no changes needed)
  # Uses llm-api-keys secret with nvidia_api_key (created in Step 1)
  
  # Option 2: Custom secret configuration
  # config:
  #   apiKeySecret: "my-custom-secret"
  #   apiKeySecretKey: "my_api_key"
  
  # Option 3: Custom models (while keeping NVIDIA Cloud API)
  # config:
  #   models:
  #     brainstorming: "meta/llama-3.1-405b-instruct"
  #     creativity: "meta/llama-3.1-405b-instruct"
  
  # Option 4: Custom environment variables for local LLMs or multiple providers
  # env:
  #   - name: DYNAMO_API_KEY
  #     valueFrom:
  #       secretKeyRef:
  #         name: dynamo-api-secret
  #         key: DYNAMO_API_KEY
  #   - name: OPENAI_API_KEY
  #     valueFrom:
  #       secretKeyRef:
  #         name: multi-provider-keys
  #         key: openai_key
  #   - name: ROUTER_TIMEOUT
  #     value: "30s"
  
  service:
    type: ClusterIP

app:
  enabled: false  # Enable for demo web interface
  replicas: 1  # Single replica for simpler deployment
  image:
    repository: llm-router-app
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP  