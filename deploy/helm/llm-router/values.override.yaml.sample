# LLM Router Helm deployment sample configuration
# Copy this file to values.override.yaml and customize for your environment
# Prerequisites: Follow README Quick Start steps 1-4 before deployment
global:
  storageClass: "YOUR_STORAGE_CLASS"  # e.g., "standard", "microk8s-hostpath"
  imageRegistry: "YOUR_REGISTRY/"  # e.g., "nvcr.io/yourorg/", "docker.io/youruser/"
  imagePullSecrets:
    - name: nvcr-secret  # Change name to match your registry secret

ingress:
  enabled: false  # Enable for external access via domain names
  # hosts:
  #   - host: llm-router.local

# Router Server
routerServer:
  enabled: true
  image:
    repository: llm-router-server
    tag: latest
    pullPolicy: IfNotPresent
  resources:
    limits:
      nvidia.com/gpu: 1
      memory: "8Gi"
    requests:
      nvidia.com/gpu: 1
      memory: "8Gi"
  modelRepository:
    path: "/model_repository/routers"
  volumes:
    modelRepository:
      enabled: true
      mountPath: "/model_repository"
      storage:
        persistentVolumeClaim:
          enabled: true
          existingClaim: "router-models-pvc"
  service:
    type: ClusterIP
  shm_size: "8G"

# Router Controller
routerController:
  enabled: true
  replicas: 1  # Single replica for simpler deployment
  image:
    repository: llm-router-controller
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP

app:
  enabled: false  # Enable for demo web interface
  replicas: 1  # Single replica for simpler deployment
  image:
    repository: llm-router-app
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP  