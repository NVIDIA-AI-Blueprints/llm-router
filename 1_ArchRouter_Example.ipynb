{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# ArchRouter Multimodal Routing Example\n",
    "\n",
    "This notebook demonstrates how to use [Arch-Router-1.5B](https://huggingface.co/katanemo/Arch-Router-1.5B) for intelligent request routing in a multimodal LLM system.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Arch-Router** is a lightweight model designed to classify user intents and route requests to the most appropriate backend model. It supports:\n",
    "\n",
    "- **Hard questions** â†’ Route to powerful reasoning models (e.g., GPT-5)\n",
    "- **Chit-chat** â†’ Route to efficient conversational models (e.g., Nemotron)\n",
    "- **Image understanding** â†’ Route to vision-language models (e.g., Nemotron-VL)\n",
    "- **Retry requests** â†’ Handle cases where users indicate previous answers were incorrect\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "User Request â†’ Arch-Router â†’ Intent Classification â†’ Model Selection â†’ Response\n",
    "                    â†“\n",
    "            [hard_question, chit_chat, image_understanding, image_question, try_again, other]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereq-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "python = sys.executable\n",
    "\n",
    "!{python} -m ensurepip --upgrade\n",
    "!{python} -m pip install --upgrade pip setuptools wheel\n",
    "!{python} -m pip install --upgrade --force-reinstall python-dotenv\n",
    "%pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f571d9",
   "metadata": {},
   "source": [
    "Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1374c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ccd899",
   "metadata": {},
   "source": [
    "Clone the repository, or skip this line if you are using the Brev launchable which already has the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA-AI-Blueprints/llm-router.git\n",
    "!cd llm-router && git checkout experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../llm-router')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84eebb",
   "metadata": {},
   "source": [
    "Before running this notebook, you need to deploy the Arch-Router model using vLLM. Run the following Docker command on a Linux machine with GPU support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prereq-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# PREREQUISITE: Deploy Arch-Router with Docker (Linux)\n",
    "# =====================================================\n",
    "# Run this command in your terminal BEFORE running this notebook.\n",
    "# The router model must be running on a GPU server.\n",
    "\n",
    "!docker run -d --rm --runtime nvidia --gpus \"device=0\" \\\n",
    "    --name arch_router \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    -p 8011:8000 \\\n",
    "    --ipc=host \\\n",
    "    vllm/vllm-openai:latest \\\n",
    "    --model katanemo/Arch-Router-1.5B\n",
    "\n",
    "# After starting, verify it's running:\n",
    "# curl http://localhost:8011/health\n",
    "\n",
    "print(\"âœ“ Make sure the Arch-Router Docker container is running before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6488af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d5b6f-a063-4094-8936-0fdc4905474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8ded7",
   "metadata": {},
   "source": [
    "First, we set up the Python path and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from functools import lru_cache\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "from pydantic import Field\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# NAT Framework imports\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "# Configure logging for the notebook\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ“ Standard imports loaded successfully\")\n",
    "print(\"âœ“ NAT Framework imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-md",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up the remote model endpoint and model name. The router uses a vLLM server hosting the Arch-Router model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Remote Model Configuration\n",
    "# =====================================================\n",
    "\n",
    "# Set your Arch-Router endpoint (vLLM server URL)\n",
    "# Default: http://localhost:8011 if running locally via Docker\n",
    "REMOTE_MODEL_URL = os.getenv(\"ARCH_ROUTER_ENDPOINT\", \"http://localhost:8011\")\n",
    "MODEL_NAME = \"katanemo/Arch-Router-1.5B\"\n",
    "\n",
    "# Global tokenizer (lazy loaded)\n",
    "tokenizer = None\n",
    "\n",
    "print(f\"âœ“ Configuration set:\")\n",
    "print(f\"  - Remote Model URL: {REMOTE_MODEL_URL}\")\n",
    "print(f\"  - Model Name: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-md",
   "metadata": {},
   "source": [
    "## 3. Tokenizer and Health Check Functions\n",
    "\n",
    "Utility functions to load the tokenizer and verify the remote model is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_tokenizer():\n",
    "    \"\"\"Lazy load the tokenizer on first use.\"\"\"\n",
    "    global tokenizer\n",
    "    if tokenizer is None:\n",
    "        logger.info(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "            logger.info(\"Tokenizer loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load tokenizer: {e}\")\n",
    "            raise\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def _check_remote_model():\n",
    "    \"\"\"Check if remote model is available.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{REMOTE_MODEL_URL}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            logger.info(f\"Remote model at {REMOTE_MODEL_URL} is available\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Remote model at {REMOTE_MODEL_URL} is not available: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"âœ“ Tokenizer and health check functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompts-md",
   "metadata": {},
   "source": [
    "## 4. Prompt Templates\n",
    "\n",
    "The Arch-Router model uses specific prompt templates for optimal performance. These templates structure the conversation and route definitions for the model to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompts-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Prompt Templates (Use as provided for best performance)\n",
    "# =====================================================\n",
    "\n",
    "TASK_INSTRUCTION = \"\"\"\n",
    "You are a helpful assistant designed to find the best suited route.\n",
    "You are provided with route description within <routes></routes> XML tags:\n",
    "<routes>\n",
    "\n",
    "{routes}\n",
    "\n",
    "</routes>\n",
    "\n",
    "<conversation>\n",
    "\n",
    "{conversation}\n",
    "\n",
    "</conversation>\n",
    "\"\"\"\n",
    "\n",
    "FORMAT_PROMPT = \"\"\"\n",
    "Your task is to decide which route is best suit with user intent on the conversation in <conversation></conversation> XML tags.  Follow the instruction:\n",
    "1. If the latest intent from user is irrelevant or user intent is full filled, response with other route {\"route\": \"other\"}.\n",
    "2. You must analyze the route descriptions and find the best match route for user latest intent. \n",
    "3. You only response the name of the route that best matches the user's request, use the exact name in the <routes></routes>.\n",
    "\n",
    "Based on your analysis, provide your response in the following JSON formats if you decide to match any route:\n",
    "{\"route\": \"route_name\"} \n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ“ Prompt templates defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoder-md",
   "metadata": {},
   "source": [
    "## 5. JSON Encoder Utility\n",
    "\n",
    "Custom JSON encoder that handles Pydantic models and other non-serializable objects commonly used in the routing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encoder-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PydanticEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom JSON encoder for Pydantic models and non-serializable objects.\"\"\"\n",
    "    \n",
    "    def default(self, obj):\n",
    "        # Handle Pydantic models\n",
    "        if hasattr(obj, 'model_dump'):\n",
    "            return obj.model_dump()\n",
    "        # Handle dict-like objects\n",
    "        if hasattr(obj, '__dict__'):\n",
    "            return obj.__dict__\n",
    "        # Handle iterables (except strings)\n",
    "        if hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes)):\n",
    "            try:\n",
    "                return list(obj)\n",
    "            except TypeError:\n",
    "                pass\n",
    "        return super().default(obj)\n",
    "\n",
    "print(\"âœ“ PydanticEncoder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "routes-md",
   "metadata": {},
   "source": [
    "## 6. Route Configuration\n",
    "\n",
    "Define the available routes and their descriptions. Each route maps to a specific intent type that the router will classify incoming requests into.\n",
    "\n",
    "The `MAP_INTENT_TO_PIPELINE` dictionary maps classified intents to the appropriate backend model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "routes-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Route Configuration\n",
    "# =====================================================\n",
    "\n",
    "# Define available routes with their descriptions\n",
    "route_config = [\n",
    "    {\n",
    "        \"name\": \"hard_question\",\n",
    "        \"description\": \"A question that requires deep reasoning, or complex problem solving, or if the user asks for careful thinking or careful consideration\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"chit_chat\",\n",
    "        \"description\": \"Any social chit chat, small talk, or casual conversation.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"try_again\",\n",
    "        \"description\": \"Only if the user explicitly says the previous answer was incorrect or incomplete.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"image_understanding\",\n",
    "        \"description\": \"A question that requires understanding an image.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"image_question\",\n",
    "        \"description\": \"A question that requires the assistant to see the user eg a question about their appearance, environment, scene or surroundings.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Pre-compute routes JSON once to avoid repeated serialization\n",
    "_ROUTES_JSON_CACHED = json.dumps(route_config, cls=PydanticEncoder)\n",
    "\n",
    "# Map classified intents to backend models\n",
    "MAP_INTENT_TO_PIPELINE = {\n",
    "    \"other\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
    "    \"chit_chat\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
    "    \"hard_question\": \"gpt-5-chat\",\n",
    "    \"image_understanding\": \"nvidia/nemotron-nano-12b-v2-vl\",\n",
    "    \"image_question\": \"nvidia/nemotron-nano-12b-v2-vl\",\n",
    "    \"try_again\": \"gpt-5-chat\",\n",
    "}\n",
    "\n",
    "print(\"âœ“ Route configuration defined:\")\n",
    "for route in route_config:\n",
    "    print(f\"  - {route['name']}: {route['description'][:50]}...\")\n",
    "print(f\"\\nâœ“ Intent-to-model mapping:\")\n",
    "for intent, model in MAP_INTENT_TO_PIPELINE.items():\n",
    "    print(f\"  - {intent} â†’ {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers-md",
   "metadata": {},
   "source": [
    "## 7. Helper Functions\n",
    "\n",
    "These helper functions handle:\n",
    "- **Image redaction**: Removes image data from multimodal conversations (router is text-only but can detect image-related intent from text)\n",
    "- **Prompt formatting**: Constructs the prompt for the Arch-Router model\n",
    "- **Response parsing**: Extracts the route decision from model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpers-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_images_from_conversation(conversation: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Remove image data from conversation while preserving text context.\n",
    "    \n",
    "    The router model doesn't support images directly, but can still determine\n",
    "    if the text intent requires image understanding capabilities.\n",
    "    \"\"\"\n",
    "    redacted = []\n",
    "    for i, msg in enumerate(conversation):\n",
    "        msg_copy = msg.copy()\n",
    "        content = msg_copy.get(\"content\")\n",
    "        \n",
    "        # If content is a list (multimodal format), process it\n",
    "        if isinstance(content, list):\n",
    "            text_parts = []\n",
    "            \n",
    "            for item in content:\n",
    "                logger.info(f\"  Item: {type(item)}, {item if not isinstance(item, dict) else list(item.keys())}\")\n",
    "                if isinstance(item, dict):\n",
    "                    if item.get(\"type\") == \"text\":\n",
    "                        item_text = item.get(\"text\", \"\")\n",
    "                        text = f\"<new msg>{item_text} </msg>\"\n",
    "                        text_parts.append(text)\n",
    "                    elif item.get(\"type\") == \"image_url\":\n",
    "                        # Skip image content\n",
    "                        continue\n",
    "            \n",
    "            # Combine text parts\n",
    "            combined_text = \" \".join(text_parts)\n",
    "            msg_copy[\"content\"] = combined_text\n",
    "        \n",
    "        redacted.append(msg_copy)\n",
    "    \n",
    "    return redacted\n",
    "\n",
    "\n",
    "def format_prompt(conversation: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Create the system prompt for the router model.\n",
    "    \n",
    "    Uses pre-computed routes JSON for efficiency.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        TASK_INSTRUCTION.format(\n",
    "            routes=_ROUTES_JSON_CACHED,\n",
    "            conversation=json.dumps(conversation, cls=PydanticEncoder)\n",
    "        )\n",
    "        + FORMAT_PROMPT\n",
    "    )\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def _parse_route_response(response: str) -> str:\n",
    "    \"\"\"Parse and cache route responses to avoid repeated JSON parsing.\"\"\"\n",
    "    try:\n",
    "        return json.loads(response)[\"route\"]\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle single quote format\n",
    "        import ast\n",
    "        return ast.literal_eval(response)[\"route\"]\n",
    "\n",
    "\n",
    "def materialize_iterator(obj):\n",
    "    \"\"\"Recursively convert ValidatorIterator and other iterables to lists.\"\"\"\n",
    "    if hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, dict)):\n",
    "        try:\n",
    "            return [materialize_iterator(item) for item in obj]\n",
    "        except TypeError:\n",
    "            pass\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: materialize_iterator(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "routing-md",
   "metadata": {},
   "source": [
    "## 8. Core Routing Function\n",
    "\n",
    "The main routing function that:\n",
    "1. Takes a conversation (list of messages)\n",
    "2. Redacts any images from multimodal content\n",
    "3. Formats the prompt for the Arch-Router model\n",
    "4. Calls the remote vLLM API\n",
    "5. Parses the response to determine the best route\n",
    "6. Returns the appropriate backend model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "routing-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_route_from_conversation(conversation: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Determine the best route for the conversation using the remote Arch-Router model.\n",
    "    \n",
    "    Args:\n",
    "        conversation: List of message dictionaries with 'role' and 'content' keys\n",
    "        \n",
    "    Returns:\n",
    "        str: The name of the selected route (e.g., 'hard_question', 'chit_chat', etc.)\n",
    "    \"\"\"\n",
    "    inference_start = time.perf_counter()\n",
    "    \n",
    "    # Redact images from messages because the router does not support them\n",
    "    # But it can still determine if the text intent requires image understanding\n",
    "    redacted_conversation = redact_images_from_conversation(conversation)\n",
    "    \n",
    "    # ===== FORMAT PROMPT =====\n",
    "    prompt_start = time.perf_counter()\n",
    "    route_prompt = format_prompt(redacted_conversation)\n",
    "    prompt_time = time.perf_counter() - prompt_start\n",
    "    \n",
    "    # ===== CONSTRUCT MESSAGES =====\n",
    "    construct_start = time.perf_counter()\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": route_prompt},\n",
    "    ]\n",
    "    construct_time = time.perf_counter() - construct_start\n",
    "\n",
    "    # ===== ENCODE (TOKENIZE) =====\n",
    "    # Not needed for remote API, but keeping for timing consistency\n",
    "    encode_start = time.perf_counter()\n",
    "    encode_time = time.perf_counter() - encode_start\n",
    "\n",
    "    # ===== GENERATION (REMOTE API CALL) =====\n",
    "    generation_start = time.perf_counter()\n",
    "    try:\n",
    "        # Call remote vLLM OpenAI-compatible API\n",
    "        response = requests.post(\n",
    "            f\"{REMOTE_MODEL_URL}/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"messages\": messages,\n",
    "                \"max_tokens\": 32,\n",
    "                \"temperature\": 0.3,\n",
    "                \"top_p\": 0.9,\n",
    "            },\n",
    "            timeout=30,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        response_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to call remote model: {e}\")\n",
    "        raise\n",
    "    \n",
    "    generation_time = time.perf_counter() - generation_start\n",
    "\n",
    "    # ===== DECODING =====\n",
    "    decode_start = time.perf_counter()\n",
    "    # Response is already decoded text from remote API\n",
    "    decode_time = time.perf_counter() - decode_start\n",
    "    \n",
    "    # Use cached parser\n",
    "    route = _parse_route_response(response_text)\n",
    "    \n",
    "    total_time = time.perf_counter() - inference_start\n",
    "    \n",
    "    # Log timing breakdown\n",
    "    logger.info(\n",
    "        f\"Route inference timing breakdown | \"\n",
    "        f\"Format: {prompt_time*1000:.2f}ms | \"\n",
    "        f\"Construct: {construct_time*1000:.2f}ms | \"\n",
    "        f\"Encode: {encode_time*1000:.2f}ms | \"\n",
    "        f\"Generate: {generation_time*1000:.2f}ms | \"\n",
    "        f\"Decode: {decode_time*1000:.2f}ms | \"\n",
    "        f\"Total: {total_time*1000:.2f}ms\"\n",
    "    )\n",
    "    logger.debug(f\"Route: {route}, Response: {response_text[:100]}\")\n",
    "    \n",
    "    return route\n",
    "\n",
    "\n",
    "def route_request(conversation: List[Dict[str, Any]]) -> Tuple[str, str]:\n",
    "    \"\"\"Route a conversation to the appropriate backend model.\n",
    "    \n",
    "    This is a simplified synchronous version for demonstration purposes.\n",
    "    \n",
    "    Args:\n",
    "        conversation: List of message dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model_name, intent_name)\n",
    "    \"\"\"\n",
    "    response_start = time.perf_counter()\n",
    "    \n",
    "    # Get the route from the conversation\n",
    "    user_intent = get_route_from_conversation(conversation)\n",
    "    \n",
    "    total_response_time = time.perf_counter() - response_start\n",
    "    \n",
    "    logger.info(f\"User intent: {user_intent} (total response time: {total_response_time*1000:.2f}ms)\")\n",
    "    return MAP_INTENT_TO_PIPELINE[user_intent], user_intent\n",
    "\n",
    "print(\"âœ“ Core routing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Example Usage\n",
    "\n",
    "Let's test the router with different types of user messages to see how it classifies intents and routes to appropriate models.\n",
    "\n",
    "**Note**: These examples require the Arch-Router Docker container to be running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check if the remote model is available\n",
    "print(\"Checking remote model availability...\")\n",
    "is_available = _check_remote_model()\n",
    "\n",
    "if is_available:\n",
    "    print(\"\\nâœ“ Remote model is available! You can run the examples below.\")\n",
    "else:\n",
    "    print(\"\\nâœ— Remote model is not available.\")\n",
    "    print(\"  Please start the Arch-Router Docker container first.\")\n",
    "    print(f\"  Expected endpoint: {REMOTE_MODEL_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Test Cases for Different Intent Types\n",
    "# =====================================================\n",
    "\n",
    "test_conversations = [\n",
    "    # Test 1: Chit-chat\n",
    "    {\n",
    "        \"name\": \"Chit-chat\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hey! How's it going? Nice weather today, isn't it?\"}]\n",
    "    },\n",
    "    \n",
    "    # Test 2: Hard question (requires reasoning)\n",
    "    {\n",
    "        \"name\": \"Hard Question\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Can you carefully think through this problem: If a train leaves station A at 3pm traveling at 60mph, and another train leaves station B at 4pm traveling at 80mph towards station A, and the stations are 200 miles apart, when and where will they meet?\"}]\n",
    "    },\n",
    "    \n",
    "    # Test 3: Image understanding request\n",
    "    {\n",
    "        \"name\": \"Image Understanding\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Can you analyze this image and tell me what objects are in it?\"}]\n",
    "    },\n",
    "    \n",
    "    # Test 4: Image question (about user's surroundings)\n",
    "    {\n",
    "        \"name\": \"Image Question (Surroundings)\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What can you see in my room? Can you describe my surroundings?\"}]\n",
    "    },\n",
    "    \n",
    "    # Test 5: Try again request\n",
    "    {\n",
    "        \"name\": \"Try Again\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"2+2 equals 5.\"},\n",
    "            {\"role\": \"user\", \"content\": \"That's wrong! Please try again with the correct answer.\"}\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # Test 6: General question (should route to 'other')\n",
    "    {\n",
    "        \"name\": \"General Question\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Test conversations defined. Run the next cell to test routing.\")\n",
    "print(f\"\\nTotal test cases: {len(test_conversations)}\")\n",
    "for i, test in enumerate(test_conversations, 1):\n",
    "    print(f\"  {i}. {test['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Run Routing Tests\n",
    "# =====================================================\n",
    "# Note: This requires the Arch-Router Docker container to be running\n",
    "\n",
    "print(\"Running routing tests...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "for test in test_conversations:\n",
    "    print(f\"\\nTest: {test['name']}\")\n",
    "    print(f\"Input: {test['messages'][-1]['content'][:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        model, intent = route_request(test['messages'])\n",
    "        results.append({\n",
    "            \"test\": test['name'],\n",
    "            \"intent\": intent,\n",
    "            \"model\": model,\n",
    "            \"status\": \"success\"\n",
    "        })\n",
    "        print(f\"â†’ Intent: {intent}\")\n",
    "        print(f\"â†’ Routed to: {model}\")\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"test\": test['name'],\n",
    "            \"intent\": \"error\",\n",
    "            \"model\": \"N/A\",\n",
    "            \"status\": str(e)\n",
    "        })\n",
    "        print(f\"â†’ Error: {e}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "for r in results:\n",
    "    status = \"âœ“\" if r['status'] == 'success' else \"âœ—\"\n",
    "    print(f\"{status} {r['test']}: {r['intent']} â†’ {r['model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multimodal-md",
   "metadata": {},
   "source": [
    "## 10. Multimodal Example\n",
    "\n",
    "The router can also handle multimodal messages (with images). The images are redacted for the router, but it can still determine intent from the text context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multimodal-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Multimodal Message Example\n",
    "# =====================================================\n",
    "\n",
    "# Example multimodal message with text and image\n",
    "multimodal_conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's in this image? Can you describe it in detail?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,/9j/4AAQSk...\"}}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing multimodal message...\")\n",
    "print(f\"Original message has {len(multimodal_conversation[0]['content'])} parts\")\n",
    "\n",
    "# Show the redaction process\n",
    "redacted = redact_images_from_conversation(multimodal_conversation)\n",
    "print(f\"\\nRedacted message: {redacted[0]['content']}\")\n",
    "\n",
    "# Route the request (requires remote model)\n",
    "try:\n",
    "    model, intent = route_request(multimodal_conversation)\n",
    "    print(f\"\\nâ†’ Intent: {intent}\")\n",
    "    print(f\"â†’ Routed to: {model}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâ†’ Error (remote model may not be available): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nat-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. NAT Framework Integration\n",
    "\n",
    "This section registers the router as an objective function with the NAT framework for production deployment.\n",
    "\n",
    "The `hf_intent_objective_fn` is registered as a function that can be used with the `sfc_router` component to handle incoming chat requests and route them to the appropriate backend model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nat-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# NAT Framework Integration\n",
    "# =====================================================\n",
    "\n",
    "from nat_sfc_router.schema.openai_chat_request import OpenAIChatRequest\n",
    "\n",
    "\n",
    "class HFIntentObjectiveConfig(FunctionBaseConfig, name=\"hf_intent_objective_fn\"):\n",
    "    \"\"\"HF intent objective function for best route.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@register_function(config_type=HFIntentObjectiveConfig)\n",
    "async def hf_intent_objective_fn(config: HFIntentObjectiveConfig,\n",
    "                                 _builder: Builder):\n",
    "    \"\"\"HF intent objective function for best route.\"\"\"\n",
    "\n",
    "    # Check if remote model is available\n",
    "    _check_remote_model()\n",
    "    \n",
    "    # Load tokenizer (model is remote)\n",
    "    loaded_tokenizer = _load_tokenizer()\n",
    "\n",
    "    async def _response_fn(chat_request: OpenAIChatRequest) -> Tuple[str, str]:\n",
    "        \"\"\"HF intent objective function for best route.\"\"\"\n",
    "        response_start = time.perf_counter()\n",
    "\n",
    "        # ===== EXTRACT MESSAGES =====\n",
    "        extract_start = time.perf_counter()\n",
    "        messages = chat_request.messages\n",
    "        extract_time = time.perf_counter() - extract_start\n",
    "\n",
    "        if messages:\n",
    "            # ===== CONVERT TO DICT =====\n",
    "            dict_convert_start = time.perf_counter()\n",
    "            last_msg = messages[-1]\n",
    "            last_msg_dict = last_msg.model_dump() if hasattr(last_msg, 'model_dump') else dict(last_msg)\n",
    "            dict_convert_time = time.perf_counter() - dict_convert_start\n",
    "\n",
    "            # ===== MATERIALIZE ITERATORS =====\n",
    "            materialize_start = time.perf_counter()\n",
    "            last_msg_dict = materialize_iterator(last_msg_dict)\n",
    "            materialize_time = time.perf_counter() - materialize_start\n",
    "\n",
    "            # Assign a list containing only the last message's dictionary\n",
    "            messages_dict = [last_msg_dict]\n",
    "            \n",
    "            logger.debug(\n",
    "                f\"Message preparation timing | \"\n",
    "                f\"Extract: {extract_time*1000:.2f}ms | \"\n",
    "                f\"Dict convert: {dict_convert_time*1000:.2f}ms | \"\n",
    "                f\"Materialize: {materialize_time*1000:.2f}ms\"\n",
    "            )\n",
    "        else:\n",
    "            # Handle the case where the list of messages is empty\n",
    "            messages_dict = []\n",
    "            logger.warning(\"No messages received in chat request\")\n",
    "\n",
    "        # Run model inference (blocking call in event loop)\n",
    "        user_intent = get_route_from_conversation(messages_dict)\n",
    "        \n",
    "        total_response_time = time.perf_counter() - response_start\n",
    "\n",
    "        logger.info(f\"User intent: {user_intent} (total response time: {total_response_time*1000:.2f}ms)\")\n",
    "        return MAP_INTENT_TO_PIPELINE[user_intent], \"\"\n",
    "    \n",
    "\n",
    "    yield FunctionInfo.from_fn(\n",
    "        _response_fn,\n",
    "        description=\"Demonstrative objective function for best model.\")\n",
    "\n",
    "print(\"âœ“ HFIntentObjectiveConfig class defined\")\n",
    "print(\"âœ“ hf_intent_objective_fn registered with NAT framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3306379",
   "metadata": {},
   "source": [
    "## 12. NAT Framework Example Usage\n",
    "\n",
    "Now let's demonstrate how to use the NAT-registered `hf_intent_objective_fn` with sample chat requests. This shows how the function would be invoked in a production NAT pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e28ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# NAT Framework Example Usage\n",
    "# =====================================================\n",
    "# This demonstrates how to use the registered hf_intent_objective_fn\n",
    "# with the NAT framework in a production-like scenario.\n",
    "\n",
    "async def run_nat_framework_example():\n",
    "    \"\"\"Run example routing requests through the NAT framework function.\"\"\"\n",
    "    \n",
    "    # Create the config instance\n",
    "    config = HFIntentObjectiveConfig()\n",
    "    \n",
    "    # In production, this is handled by the NAT Builder\n",
    "    async with hf_intent_objective_fn(config, None) as func_info:\n",
    "        response_fn = func_info.single_fn\n",
    "        \n",
    "        # Define test requests using OpenAI chat format\n",
    "        test_requests = [\n",
    "            {\n",
    "                \"name\": \"Chit-chat\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Hey there! How's your day going?\"}]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Hard Question\", \n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Please think carefully: What are the philosophical implications of GÃ¶del's incompleteness theorems?\"}]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Image Request\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Can you look at this photo and tell me what you see?\"}]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Retry Request\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"What is 15 * 7?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"15 * 7 = 95\"},\n",
    "                    {\"role\": \"user\", \"content\": \"That's incorrect, please try again.\"}\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"NAT FRAMEWORK ROUTING EXAMPLES\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for test in test_requests:\n",
    "            print(f\"\\nðŸ“¨ Test: {test['name']}\")\n",
    "            print(f\"   Input: {test['messages'][-1]['content'][:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                # Create an OpenAIChatRequest-like object\n",
    "                # In production, this comes from the incoming API request\n",
    "                chat_request = OpenAIChatRequest(\n",
    "                    model=\"router\",\n",
    "                    messages=test['messages']\n",
    "                )\n",
    "                \n",
    "                # Call the response function (this is what NAT does internally)\n",
    "                model, _ = await response_fn(chat_request)\n",
    "                \n",
    "                print(f\"   âœ“ Routed to: {model}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âœ— Error: {e}\")\n",
    "            \n",
    "            print(\"-\" * 70)\n",
    "        \n",
    "        print(\"\\nâœ“ NAT Framework example completed!\")\n",
    "\n",
    "# Run the async example\n",
    "print(\"Running NAT Framework example...\")\n",
    "print(\"Note: This requires the Arch-Router Docker container to be running.\\n\")\n",
    "\n",
    "try:\n",
    "    # Use asyncio.run() or await depending on environment\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(run_nat_framework_example())\n",
    "except ImportError:\n",
    "    # nest_asyncio not available, try direct run\n",
    "    try:\n",
    "        asyncio.run(run_nat_framework_example())\n",
    "    except RuntimeError:\n",
    "        # Already in an async context (e.g., Jupyter)\n",
    "        await run_nat_framework_example()\n",
    "except Exception as e:\n",
    "    print(f\"Error running example: {e}\")\n",
    "    print(\"\\nMake sure:\")\n",
    "    print(\"  1. The Arch-Router Docker container is running\")\n",
    "    print(\"  2. All previous cells have been executed\")\n",
    "    print(f\"  3. The endpoint {REMOTE_MODEL_URL} is accessible\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0bf9d-f415-41bb-a686-87f1817e6246",
   "metadata": {},
   "source": [
    "Once you're done, let's spin down the ArchRouter and free up space for the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957fa9d-9822-4228-b28a-da75e1d3f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker kill arch_router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to use the Arch-Router model for intelligent request routing:\n",
    "\n",
    "1. **Setup**: Deploy the model using vLLM with Docker\n",
    "2. **Configuration**: Define routes and their descriptions\n",
    "3. **Routing**: Send user messages to the router to classify intent\n",
    "4. **Model Selection**: Route to the appropriate backend model based on intent\n",
    "5. **NAT Integration**: Register the router as a NAT framework objective function\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Cost Efficiency**: Route simple queries to smaller, cheaper models\n",
    "- **Quality**: Route complex queries to more capable models\n",
    "- **Multimodal Support**: Automatically detect when vision capabilities are needed\n",
    "- **Low Latency**: The 1.5B router model is fast and lightweight\n",
    "- **Production Ready**: NAT framework integration for seamless deployment\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Customize the `route_config` for your specific use cases\n",
    "- Integrate with your backend model serving infrastructure\n",
    "- See `2_Embedding_NN_Training.ipynb` for training your own router model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
